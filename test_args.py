#!/usr/bin/env python
"""
æµ‹è¯•å‚æ•°è§£ææ˜¯å¦æ­£ç¡®

ç”¨æ³•:
    python test_args.py
    python test_args.py --no-distributed
    torchrun --nproc_per_node=2 test_args.py
"""

import os
import argparse

def test_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--batch_size', type=int, default=256)
    parser.add_argument('--no-distributed', dest='distributed', 
                       action='store_false', default=True, 
                       help="ç¦ç”¨åˆ†å¸ƒå¼è®­ç»ƒ")
    
    args = parser.parse_args()
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    rank = os.environ.get('RANK', 'Not set')
    local_rank = os.environ.get('LOCAL_RANK', 'Not set')
    world_size = os.environ.get('WORLD_SIZE', 'Not set')
    
    print("=" * 60)
    print("å‚æ•°æµ‹è¯•")
    print("=" * 60)
    print(f"args.distributed: {args.distributed}")
    print(f"args.batch_size: {args.batch_size}")
    print()
    print("ç¯å¢ƒå˜é‡:")
    print(f"  RANK: {rank}")
    print(f"  LOCAL_RANK: {local_rank}")
    print(f"  WORLD_SIZE: {world_size}")
    print()
    
    # æ¨¡æ‹Ÿ init_distributed é€»è¾‘
    ddp = int(os.environ.get('RANK', -1)) != -1
    
    if not (ddp and args.distributed):
        print("ç»“æœ: å•å¡æ¨¡å¼")
        print("  åŸå› :", end=" ")
        if not ddp:
            print("æœªæ£€æµ‹åˆ° RANK ç¯å¢ƒå˜é‡ï¼ˆé torchrun å¯åŠ¨ï¼‰")
        elif not args.distributed:
            print("--no-distributed å‚æ•°ç¦ç”¨äº†åˆ†å¸ƒå¼")
    else:
        print(f"ç»“æœ: åˆ†å¸ƒå¼æ¨¡å¼ (Rank {rank})")
    
    print("=" * 60)
    print()
    
    # æ˜¾ç¤ºä½¿ç”¨å»ºè®®
    if not ddp:
        print("ğŸ’¡ æç¤º:")
        print("  å½“å‰æ˜¯å•è¿›ç¨‹æ¨¡å¼")
        print("  å¦‚éœ€æµ‹è¯•åˆ†å¸ƒå¼ï¼Œä½¿ç”¨:")
        print("    torchrun --nproc_per_node=2 test_args.py")
    elif not args.distributed:
        print("âš ï¸  æ³¨æ„:")
        print("  è™½ç„¶åœ¨ torchrun ç¯å¢ƒä¸‹ï¼Œä½† --no-distributed ç¦ç”¨äº†åˆ†å¸ƒå¼")
    else:
        print("âœ… åˆ†å¸ƒå¼é…ç½®æ­£ç¡®!")

if __name__ == "__main__":
    test_args()









