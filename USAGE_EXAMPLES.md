# ä½¿ç”¨ç¤ºä¾‹

## âœ… æ­£ç¡®ç”¨æ³•

### 1. å¤šå¡åˆ†å¸ƒå¼è®­ç»ƒï¼ˆæ¨èï¼‰

```bash
# ä½¿ç”¨ 4 å¼  GPUï¼Œä¸éœ€è¦ --distributed å‚æ•°ï¼ˆé»˜è®¤å¯ç”¨ï¼‰
torchrun --nproc_per_node=4 simsiam.py \
    --batch_size 256 \
    --train_dir /path/to/train \
    --test_dir /path/to/test \
    --epochs 100

# ä½¿ç”¨ 2 å¼  GPU
torchrun --nproc_per_node=2 simsiam.py \
    --batch_size 128 \
    --train_dir /path/to/train \
    --test_dir /path/to/test
```

### 2. å•å¡è®­ç»ƒ

```bash
# æ–¹å¼ 1: ç›´æ¥è¿è¡Œï¼ˆä¼šè‡ªåŠ¨æ£€æµ‹åˆ°æ²¡æœ‰ RANK ç¯å¢ƒå˜é‡ï¼Œä½¿ç”¨å•å¡ï¼‰
python simsiam.py \
    --batch_size 64 \
    --train_dir /path/to/train \
    --test_dir /path/to/test

# æ–¹å¼ 2: æ˜ç¡®ç¦ç”¨åˆ†å¸ƒå¼
python simsiam.py \
    --no-distributed \
    --batch_size 64 \
    --train_dir /path/to/train \
    --test_dir /path/to/test

# æ–¹å¼ 3: æŒ‡å®šä½¿ç”¨å“ªå¼  GPU
CUDA_VISIBLE_DEVICES=0 python simsiam.py \
    --batch_size 64 \
    --train_dir /path/to/train \
    --test_dir /path/to/test
```

### 3. ä½¿ç”¨éƒ¨åˆ† GPU

```bash
# åªä½¿ç”¨ GPU 0 å’Œ 2
CUDA_VISIBLE_DEVICES=0,2 torchrun --nproc_per_node=2 simsiam.py \
    --batch_size 128 \
    --train_dir /path/to/train \
    --test_dir /path/to/test
```

### 4. åå°è¿è¡Œ

```bash
# ä½¿ç”¨ nohup
nohup torchrun --nproc_per_node=4 simsiam.py \
    --batch_size 256 \
    --train_dir /path/to/train \
    --test_dir /path/to/test \
    > training.log 2>&1 &

# æŸ¥çœ‹æ—¥å¿—
tail -f training.log
```

### 5. å®Œæ•´å‚æ•°ç¤ºä¾‹

```bash
torchrun --nproc_per_node=4 simsiam.py \
    --batch_size 256 \
    --epochs 100 \
    --workers 16 \
    --train_dir /datasets/train \
    --test_dir /datasets/test \
    --save_dir ./checkpoints \
    --vocab_path config/encryptd_vocab.txt \
    --hidden_size 768 \
    --num_layers 12 \
    --heads_num 12 \
    --feedforward_size 3072 \
    --max_seq_length 4096 \
    --proj_arch "2048-2048" \
    --pred_arch "512" \
    --base_lr 0.05 \
    --momentum 0.9 \
    --wd 1e-4 \
    --fix_pred_lr \
    --weak_sample_rate 0.5 \
    --knn_n 200 \
    --knn_t 0.2 \
    --log_freq 10 \
    --resume
```

## âŒ é”™è¯¯ç”¨æ³•ï¼ˆä¼šå¯¼è‡´é”™è¯¯ï¼‰

### é”™è¯¯ 1: åœ¨ torchrun å‘½ä»¤ä¸­ä½¿ç”¨ --distributed

```bash
# âŒ é”™è¯¯ï¼torchrun ä¸è®¤è¯† --distributed
torchrun --nproc_per_node=4 simsiam.py --distributed --batch_size 256

# âœ… æ­£ç¡®ï¼ä¸éœ€è¦ --distributedï¼ˆé»˜è®¤å¯ç”¨ï¼‰
torchrun --nproc_per_node=4 simsiam.py --batch_size 256
```

### é”™è¯¯ 2: torchrun å‚æ•°ä½ç½®é”™è¯¯

```bash
# âŒ é”™è¯¯ï¼--nproc_per_node å¿…é¡»åœ¨è„šæœ¬åä¹‹å‰
torchrun simsiam.py --nproc_per_node=4 --batch_size 256

# âœ… æ­£ç¡®ï¼torchrun çš„å‚æ•°åœ¨è„šæœ¬åä¹‹å‰
torchrun --nproc_per_node=4 simsiam.py --batch_size 256
```

## ğŸ” éªŒè¯æ˜¯å¦ä½¿ç”¨åˆ†å¸ƒå¼

è¿è¡Œååº”è¯¥çœ‹åˆ°ç±»ä¼¼è¾“å‡ºï¼š

### å¤šå¡åˆ†å¸ƒå¼ï¼ˆæ­£ç¡®ï¼‰
```
| distributed init (rank 0), gpu 0
| distributed init (rank 1), gpu 1
| distributed init (rank 2), gpu 2
| distributed init (rank 3), gpu 3
```

### å•å¡æ¨¡å¼
```
Not using distributed mode!
```

## ğŸ“Š ç›‘æ§å‘½ä»¤

```bash
# å®æ—¶æŸ¥çœ‹ GPU ä½¿ç”¨
watch -n 1 nvidia-smi

# æŸ¥çœ‹è¿›ç¨‹
ps aux | grep simsiam

# æŸ¥çœ‹ç«¯å£å ç”¨
lsof -i :29500

# æ€æ­»è®­ç»ƒè¿›ç¨‹
pkill -9 -f simsiam
```

## ğŸ¯ å¿«é€Ÿæµ‹è¯•

```bash
# 1. æµ‹è¯•å•å¡
python simsiam.py --batch_size 8 --epochs 1

# 2. æµ‹è¯• 2 å¡
torchrun --nproc_per_node=2 simsiam.py --batch_size 16 --epochs 1

# 3. æµ‹è¯• 4 å¡
torchrun --nproc_per_node=4 simsiam.py --batch_size 32 --epochs 1
```

## ğŸ’¡ å¸¸ç”¨ç»„åˆ

### å¼€å‘/è°ƒè¯•ï¼ˆå¿«é€Ÿè¿­ä»£ï¼‰
```bash
python simsiam.py \
    --batch_size 32 \
    --epochs 5 \
    --log_freq 1
```

### æ­£å¼è®­ç»ƒï¼ˆ4 å¡ï¼‰
```bash
torchrun --nproc_per_node=4 simsiam.py \
    --batch_size 256 \
    --epochs 100 \
    --workers 16 \
    --resume
```

### æ¢å¤è®­ç»ƒ
```bash
torchrun --nproc_per_node=4 simsiam.py \
    --batch_size 256 \
    --resume  # è‡ªåŠ¨ä» checkpoint.pt æ¢å¤
```

## ğŸ”§ ç¯å¢ƒå˜é‡

æœ‰æ—¶éœ€è¦è®¾ç½®è¿™äº›ç¯å¢ƒå˜é‡ï¼š

```bash
# NCCL è°ƒè¯•
export NCCL_DEBUG=INFO

# ç¦ç”¨ P2Pï¼ˆå¦‚æœé‡åˆ°é€šä¿¡é”™è¯¯ï¼‰
export NCCL_P2P_DISABLE=1

# æŒ‡å®šç½‘ç»œæ¥å£
export NCCL_SOCKET_IFNAME=eth0

# ç„¶åè¿è¡Œè®­ç»ƒ
torchrun --nproc_per_node=4 simsiam.py --batch_size 256
```

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **`--distributed` å‚æ•°å·²ç§»é™¤**ï¼Œæ”¹ä¸º `--no-distributed`
   - é»˜è®¤ï¼šå¯ç”¨åˆ†å¸ƒå¼ï¼ˆå¦‚æœæœ‰ RANK ç¯å¢ƒå˜é‡ï¼‰
   - å•å¡ï¼šè‡ªåŠ¨æ£€æµ‹æˆ–ä½¿ç”¨ `--no-distributed`

2. **Batch Size è§„åˆ™**ï¼š
   - torchrun å¤šå¡ï¼š`--batch_size` æ˜¯æ€» batchï¼ˆä¼šè‡ªåŠ¨åˆ†é…åˆ°å„å¡ï¼‰
   - å•å¡ï¼š`--batch_size` å°±æ˜¯å®é™… batch size

3. **å­¦ä¹ ç‡è‡ªåŠ¨ç¼©æ”¾**ï¼š
   - ä»£ç ä¼šæ ¹æ® batch size è‡ªåŠ¨è°ƒæ•´ï¼š`lr = base_lr * batch_size / 256`

4. **Workers æ•°é‡**ï¼š
   - æ¨èï¼š`GPUæ•°é‡ Ã— 4`
   - ä¾‹å¦‚ 4 GPUï¼š`--workers 16`









