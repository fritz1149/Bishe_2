"""
å‘é‡ç´¢å¼•å·¥å…·æ¨¡å—

æä¾› FAISS å‘é‡ç´¢å¼•æ„å»ºçš„é€šç”¨åŠŸèƒ½ï¼Œä¾› corpus_generate.py å’Œ Dense.py å¤ç”¨ã€‚
"""

import os
import json
import numpy as np
import faiss
from typing import List, Dict, Optional


def build_faiss_index(
    embeddings: np.ndarray,
    doc_ids: List[str],
    doc_contents: Optional[List[str]],
    index_dir: str,
    index_type: str = 'flat',
    verbose: bool = True
) -> None:
    """
    æ„å»º FAISS å‘é‡ç´¢å¼•ï¼ˆä½¿ç”¨å†…ç§¯ = ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
    
    Args:
        embeddings: å‘é‡æ•°ç»„ï¼Œå½¢çŠ¶ä¸º (num_docs, dim)
        doc_ids: æ–‡æ¡£ ID åˆ—è¡¨
        doc_contents: æ–‡æ¡£å†…å®¹åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
        index_dir: ç´¢å¼•ä¿å­˜ç›®å½•
        index_type: ç´¢å¼•ç±»å‹ ('flat' æˆ– 'ivf')
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    """
    if len(embeddings) != len(doc_ids):
        raise ValueError(f"å‘é‡æ•°é‡ ({len(embeddings)}) ä¸æ–‡æ¡£ ID æ•°é‡ ({len(doc_ids)}) ä¸åŒ¹é…")
    
    if doc_contents and len(doc_contents) != len(doc_ids):
        raise ValueError(f"æ–‡æ¡£å†…å®¹æ•°é‡ ({len(doc_contents)}) ä¸æ–‡æ¡£ ID æ•°é‡ ({len(doc_ids)}) ä¸åŒ¹é…")
    
    dim = embeddings.shape[1]
    
    if verbose:
        print(f"ğŸ“„ æ„å»ºç´¢å¼•: {len(doc_ids)} ä¸ªæ–‡æ¡£ï¼Œç»´åº¦ {dim}")
    
    # æ„å»º FAISS ç´¢å¼•ï¼ˆä½¿ç”¨å†…ç§¯ = ä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå‰ææ˜¯å‘é‡å·²å½’ä¸€åŒ–ï¼‰
    if index_type == 'flat':
        index = faiss.IndexFlatIP(dim)  # å†…ç§¯
    elif index_type == 'ivf':
        nlist = min(100, len(doc_ids) // 10 + 1)
        quantizer = faiss.IndexFlatIP(dim)
        index = faiss.IndexIVFFlat(quantizer, dim, nlist, faiss.METRIC_INNER_PRODUCT)
        index.train(embeddings)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç´¢å¼•ç±»å‹: {index_type}")
    
    index.add(embeddings)
    
    # ä¿å­˜ç´¢å¼•å’Œå…ƒæ•°æ®
    os.makedirs(index_dir, exist_ok=True)
    faiss.write_index(index, os.path.join(index_dir, 'index.faiss'))
    
    metadata = {
        'doc_ids': doc_ids,
        'doc_contents': doc_contents if doc_contents else [],
        'index_type': index_type,
        'dim': dim,
        'num_docs': len(doc_ids)
    }
    
    with open(os.path.join(index_dir, 'metadata.json'), 'w', encoding='utf-8') as f:
        json.dump(metadata, f, ensure_ascii=False, indent=2)
    
    if verbose:
        print(f"âœ… ç´¢å¼•æ„å»ºå®Œæˆï¼")
        print(f"   - ç´¢å¼•ç›®å½•: {index_dir}")
        print(f"   - æ–‡æ¡£æ•°é‡: {len(doc_ids)}")
        print(f"   - å‘é‡ç»´åº¦: {dim}")
        print(f"   - ç´¢å¼•ç±»å‹: {index_type}")


def load_faiss_index(index_dir: str) -> tuple:
    """
    åŠ è½½ FAISS ç´¢å¼•å’Œå…ƒæ•°æ®
    
    Args:
        index_dir: ç´¢å¼•ç›®å½•è·¯å¾„
    
    Returns:
        (index, metadata) å…ƒç»„
    """
    if not os.path.exists(index_dir):
        raise FileNotFoundError(f"ç´¢å¼•ç›®å½•ä¸å­˜åœ¨: {index_dir}")
    
    index = faiss.read_index(os.path.join(index_dir, 'index.faiss'))
    
    with open(os.path.join(index_dir, 'metadata.json'), 'r', encoding='utf-8') as f:
        metadata = json.load(f)
    
    return index, metadata


def search_faiss_index(
    query_embedding: np.ndarray,
    index_dir: str,
    k: int = 10,
    return_contents: bool = True
) -> List[tuple]:
    """
    åœ¨ FAISS ç´¢å¼•ä¸­æ£€ç´¢
    
    Args:
        query_embedding: æŸ¥è¯¢å‘é‡ï¼Œå½¢çŠ¶ä¸º (1, dim) æˆ– (dim,)
        index_dir: ç´¢å¼•ç›®å½•è·¯å¾„
        k: è¿”å› top-k ç»“æœ
        return_contents: æ˜¯å¦è¿”å›æ–‡æ¡£å†…å®¹
    
    Returns:
        [(doc_id, score, content), ...] åˆ—è¡¨
    """
    index, metadata = load_faiss_index(index_dir)
    
    # ç¡®ä¿æŸ¥è¯¢å‘é‡æ˜¯ 2D
    assert query_embedding.ndim == 2, "æŸ¥è¯¢å‘é‡ç»´åº¦å¿…é¡»æ˜¯2"
    
    # æ£€ç´¢
    scores, indices = index.search(query_embedding, k)
    
    doc_ids = metadata['doc_ids']
    doc_contents = metadata.get('doc_contents', [])
    
    results = []
    for idx, score in zip(indices[0], scores[0]):
        if idx == -1:  # FAISS è¿”å› -1 è¡¨ç¤ºæ²¡æœ‰æ›´å¤šç»“æœ
            break
        
        doc_id = doc_ids[idx]
        content = doc_contents[idx] if return_contents and doc_contents else None
        results.append((doc_id, float(score), content))
    
    return results
