import json
import os
import subprocess
import sys
from typing import List, Dict, Optional, Tuple, Literal
from pyserini.index.lucene import LuceneIndexer
from pyserini.search.lucene import LuceneSearcher
from pyserini.analysis import get_lucene_analyzer
from z2.RAG.utils import save_corpus
import jieba


def tokenize_text(text: str, language: Literal['zh', 'en'] = 'zh') -> str:
    """
    å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯
    
    Args:
        text: è¾“å…¥æ–‡æœ¬
        language: è¯­è¨€é€‰é¡¹ï¼Œ'zh' ä¸ºä¸­æ–‡ï¼ˆä½¿ç”¨ jiebaï¼‰ï¼Œ'en' ä¸ºè‹±æ–‡ï¼ˆç©ºæ ¼åˆ†å‰²ï¼‰
    
    Returns:
        åˆ†è¯åçš„æ–‡æœ¬ï¼Œç”¨ç©ºæ ¼åˆ†éš”
    """
    if language == 'zh':
        # ä¸­æ–‡ä½¿ç”¨ jieba åˆ†è¯
        tokens = list(jieba.cut(text))
        # è¿‡æ»¤ç©ºç™½å­—ç¬¦
        tokens = [t.strip() for t in tokens if t.strip()]
        return ' '.join(tokens)
    else:
        # è‹±æ–‡ç›´æ¥è¿”å›ï¼ˆå‡è®¾å·²ç”¨ç©ºæ ¼åˆ†éš”ï¼‰
        return text


def build_index(
    corpus_path: str,
    index_dir: str = 'index_dir',
    analyzer_name: str = 'whitespace',
    language: Literal['zh', 'en'] = 'zh',
    verbose: bool = True
) -> None:
    """
    ä» corpus_path ç›®å½•ä¸‹çš„æ‰€æœ‰ jsonl æ–‡ä»¶æ„å»º BM25 ç´¢å¼•ã€‚
    
    Args:
        corpus_path: è¯­æ–™ç›®å½•è·¯å¾„ï¼ˆåŒ…å«å¤šä¸ª .jsonl æ–‡ä»¶ï¼‰
        index_dir: ç´¢å¼•ä¿å­˜ç›®å½•
        analyzer_name: Lucene åˆ†æå™¨åç§°ï¼Œé»˜è®¤ 'whitespace'
                      å¸¸ç”¨é€‰é¡¹: 'whitespace', 'standard', 'english'
        language: è¯­è¨€é€‰é¡¹ï¼Œ'zh' ä¸ºä¸­æ–‡ï¼ˆä½¿ç”¨ jieba åˆ†è¯ï¼‰ï¼Œ'en' ä¸ºè‹±æ–‡
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Example:
        >>> build_index('corpus_dir/', index_dir='my_index', language='zh')
    """
    if verbose:
        print(f"ğŸ”§ æ„å»º BM25 ç´¢å¼•")
        print(f"   - è¯­æ–™ç›®å½•: {corpus_path}")
        print(f"   - ç´¢å¼•ç›®å½•: {index_dir}")
        print(f"   - åˆ†æå™¨: {analyzer_name}")
        print(f"   - è¯­è¨€: {language}")
    import sys
    sys.stdout.flush()
    
    if not os.path.exists(corpus_path):
        raise FileNotFoundError(f"è¯­æ–™ç›®å½•ä¸å­˜åœ¨: {corpus_path}")
    
    if os.path.isdir(corpus_path):
        jsonl_files = [f for f in os.listdir(corpus_path) if f.endswith('.jsonl')]
        if not jsonl_files:
            raise FileNotFoundError(f"ç›®å½• {corpus_path} ä¸­æ²¡æœ‰æ‰¾åˆ° .jsonl æ–‡ä»¶")
    elif os.path.isfile(corpus_path):
        if not corpus_path.endswith('.jsonl'):
            raise ValueError(f"è¯­æ–™æ–‡ä»¶å¿…é¡»æ˜¯ .jsonl æ ¼å¼: {corpus_path}")
    else:
        raise FileNotFoundError(f"è¯­æ–™è·¯å¾„ä¸å­˜åœ¨: {corpus_path}")

    os.makedirs(index_dir, exist_ok=True)
    args = [
        sys.executable,
        '-m',
        'pyserini.index.lucene',
        '--collection', 'JsonCollection',
        '--input', corpus_path,
        '--index', index_dir,
        '--generator', 'DefaultLuceneDocumentGenerator',
        '--threads', '1',
        '--storePositions',
        '--storeDocvectors',
        '--storeRaw',
    ]
    if language:
        args += ['--language', language]

    if verbose:
        print("ğŸš€ è°ƒç”¨ Pyserini ä¸€æ¬¡æ€§æ„å»ºç´¢å¼•")
        print("   " + " ".join(args))

    subprocess.run(args, check=True)

    if verbose:
        print(f"\nâœ… ç´¢å¼•æ„å»ºå®Œæˆï¼")
        print(f"   - ç´¢å¼•ç›®å½•: {index_dir}")


def search(
    query: str,
    index_dir: str = 'index_dir',
    k: int = 10,
    language: Literal['zh', 'en'] = 'zh',
    return_contents: bool = True,
    verbose: bool = False
) -> List[Tuple[str, float, Optional[str]]]:
    """
    ä½¿ç”¨ BM25 ç®—æ³•è¿›è¡Œ top-k æ£€ç´¢ã€‚
    
    Args:
        query: æŸ¥è¯¢æ–‡æœ¬
        index_dir: ç´¢å¼•ç›®å½•è·¯å¾„
        k: è¿”å› top-k ç»“æœæ•°é‡
        language: è¯­è¨€é€‰é¡¹ï¼Œ'zh' ä¸ºä¸­æ–‡ï¼ˆä½¿ç”¨ jieba åˆ†è¯ï¼‰ï¼Œ'en' ä¸ºè‹±æ–‡
        return_contents: æ˜¯å¦è¿”å›æ–‡æ¡£å†…å®¹ï¼ˆé»˜è®¤ Trueï¼‰
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
        List[Tuple[str, float, Optional[str]]]: 
        è¿”å›åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯ (doc_id, score, contents)
        å¦‚æœ return_contents=Falseï¼Œåˆ™ contents ä¸º None
    
    Example:
        >>> results = search('æœºå™¨å­¦ä¹ æ·±åº¦å­¦ä¹ ', k=5, language='zh')
        >>> for doc_id, score, content in results:
        ...     print(f"ID: {doc_id}, Score: {score:.4f}")
        ...     print(f"Content: {content[:100]}...\n")
    """
    if not os.path.exists(index_dir):
        raise FileNotFoundError(f"ç´¢å¼•ç›®å½•ä¸å­˜åœ¨: {index_dir}")
    
    searcher = LuceneSearcher(index_dir)
    
    # å¯¹æŸ¥è¯¢è¿›è¡Œåˆ†è¯
    tokenized_query = tokenize_text(query, language)
    
    hits = searcher.search(tokenized_query, k=k)
    
    results = []
    for hit in hits:
        doc_id = hit.docid
        score = hit.score
        contents = hit.lucene_document.get('raw') if return_contents and hit.lucene_document else None
        
        if return_contents and contents:
            try:
                doc_dict = json.loads(contents)
                contents = doc_dict.get('contents', contents)
            except json.JSONDecodeError:
                pass
        
        results.append((doc_id, score, contents))
        
        if verbose:
            print(f"\næ’å {len(results)}: {doc_id} (åˆ†æ•°: {score:.4f})")
            if contents:
                print(f"å†…å®¹: {contents[:200]}..." if len(contents) > 200 else f"å†…å®¹: {contents}")
    
    if verbose:
        print(f"\nâœ… æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(results)} ä¸ªç»“æœ")
    
    return results

if __name__ == '__main__':
    import fire
    fire.Fire()