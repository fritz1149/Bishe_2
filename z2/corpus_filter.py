"""
æ–‡æœ¬è¯­æ–™ç­›é€‰æ¨¡å—

ä¸»è¦åŠŸèƒ½ï¼š
ä½¿ç”¨ AutoModelForCausalLM (Qwen3) å¯¹ç”Ÿæˆçš„æ–‡æœ¬è¯­æ–™è¿›è¡Œè´¨é‡ç­›é€‰
ç­›é€‰æ ‡å‡†ï¼šè¯­è¨€è‡ªç„¶åº¦ã€æ¡ç†æ€§ã€é—®ç­”å¯¹åº”åº¦
"""

import json
import os
import re
import torch
from collections import defaultdict
from typing import List, Dict, Tuple, Optional
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer


class CorpusFilter:
    """
    æ–‡æœ¬è¯­æ–™ç­›é€‰å™¨ - ä½¿ç”¨ LLM å¯¹è¯­æ–™è¿›è¡Œè´¨é‡è¯„ä¼°å’Œç­›é€‰
    """
    
    def __init__(
        self,
        model_path: str = "Qwen3-1.7B",
        device: str = None,
        torch_dtype: torch.dtype = torch.bfloat16
    ):
        """
        åˆå§‹åŒ–è¯­æ–™ç­›é€‰å™¨
        
        Args:
            model_path: Qwen3 æ¨¡å‹è·¯å¾„
            device: è®¾å¤‡ ('cuda' æˆ– 'cpu')ï¼ŒNone åˆ™è‡ªåŠ¨é€‰æ‹©
            torch_dtype: æ¨¡å‹æ•°æ®ç±»å‹
        """
        self.model_path = model_path
        self.device = device or ('cuda:0' if torch.cuda.is_available() else 'cpu')
        self.torch_dtype = torch_dtype
        self.model = None
        self.tokenizer = None
    
    def _load_model(self):
        """å»¶è¿ŸåŠ è½½æ¨¡å‹"""
        if self.model is None:
            print(f"ğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_path}")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                torch_dtype=self.torch_dtype,
                device_map=self.device
            )
            self.model.eval()
            print(f"âœ… æ¨¡å‹å·²åŠ è½½ (è®¾å¤‡: {self.device})")
    
    def _build_scoring_prompt(self, data: List) -> str:
        """
        æ„å»ºè¯„åˆ† prompt
        
        Args:
            data: åŒä¸€ id ä¸‹çš„æ‰€æœ‰è¯­æ–™å†…å®¹åˆ—è¡¨
            
        Returns:
            è¯„åˆ† prompt å­—ç¬¦ä¸²
        """
        contents_text = "\n\n".join([
            f"ã€è¯­æ–™ {i+1}ã€‘\n{content}" for i, content in enumerate(data['contents'])
        ])
        
        prompt = f"""<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡æœ¬è´¨é‡è¯„ä¼°ä¸“å®¶ï¼Œæ“…é•¿è¯„ä¼°æ–‡æœ¬çš„è¯­è¨€è´¨é‡å’Œå†…å®¹è´¨é‡ã€‚<|im_end|>
<|im_start|>user
è¯·ä»”ç»†é˜…è¯»æ¥ä¸‹æ¥ç»™å‡ºçš„é’ˆå¯¹å¯¹ç½‘ç»œæµé‡çš„é—®é¢˜å’Œå›ç­”ï¼Œç„¶åä»ä»¥ä¸‹ä¸‰ä¸ªæ–¹é¢ä¸ºå›ç­”éƒ¨åˆ†æ‰“åˆ†ï¼š

1. è¯­è¨€è‡ªç„¶åº¦ï¼ˆ1-10åˆ†ï¼‰ï¼šè¯„ä¼°æ–‡æœ¬æ˜¯å¦æµç•…è‡ªç„¶ï¼Œåƒæ¯è¯­è€…æ’°å†™çš„ä¸­æ–‡ä¸€æ ·ã€‚è€ƒè™‘è¯­æ³•æ­£ç¡®æ€§ã€è¯æ±‡ä½¿ç”¨æ˜¯å¦åœ°é“ã€å¥å­æ˜¯å¦è¿è´¯ã€æ— å°´å°¬è¡¨è¾¾ã€‚
2. æ¡ç†æ€§ï¼ˆ1-10åˆ†ï¼‰ï¼šè¯„ä¼°æ–‡æœ¬çš„é€»è¾‘ç»“æ„æ˜¯å¦æ¸…æ™°ã€æ¡ç†åˆ†æ˜ã€‚è€ƒè™‘å†…å®¹æ˜¯å¦å±‚å±‚é€’è¿›ã€æ— è·³è·ƒã€è¦ç‚¹æ˜¯å¦ç»„ç»‡è‰¯å¥½ã€æ˜¯å¦æœ‰æ¸…æ™°çš„å¼€å¤´/ç»“å°¾ã€‚
3. é—®ç­”å¯¹åº”åº¦ï¼ˆ1-10åˆ†ï¼‰ï¼šè¯„ä¼°ç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å¦ç›´æ¥ã€å®Œæ•´åœ°å¯¹åº”å¹¶å›åº”äº†ç”¨æˆ·çš„é—®é¢˜ã€‚è€ƒè™‘æ˜¯å¦åˆ‡ä¸­é—®é¢˜æ ¸å¿ƒã€æ˜¯å¦å®Œæ•´è¦†ç›–é—®é¢˜è¦ç‚¹ã€æ˜¯å¦æœ‰ç­”éæ‰€é—®ã€è·‘é¢˜ã€é—æ¼å…³é”®éƒ¨åˆ†æˆ–åŒ…å«å¤§é‡æ— å…³å†—ä½™å†…å®¹ã€‚

ä»¥ä¸‹æ˜¯æ³¨æ„äº‹é¡¹ï¼š

1. ç½‘ç»œæµé‡æœ¬èº«çš„ä¿¡æ¯å°†ä¸ä¼šè¢«ç»™å‡ºï¼Œè¯·ä»…æ ¹æ®é—®ç­”è¿›è¡Œæ‰“åˆ†ã€‚
2. ä¸è¦é‡å¤ç»™å®šçš„é—®é¢˜å’Œå›ç­”ã€‚
3. åªè¾“å‡ºè¯„åˆ†ç»“æœï¼Œä¸è¦è¾“å‡ºå…¶ä»–è§£é‡Šã€‚
4. åˆ†æ•°å¿…é¡»æ˜¯1-10ä¹‹é—´çš„æ•´æ•°ã€‚
5. å¿…é¡»å¯¹æ‰€æœ‰è¯­æ–™éƒ½è¿›è¡Œè¯„åˆ†ã€‚
6. è¯­è¨€è‡ªç„¶åº¦ä½äº5åˆ†çš„è¯­æ–™å°†è¢«æ’é™¤ï¼Œå…¶ä»–è¯­æ–™åˆ™æŒ‰è¯­è¨€è‡ªç„¶åº¦ã€æ¡ç†æ€§ã€é—®ç­”å¯¹åº”åº¦çš„é¡ºåºæ’åºï¼Œå–æ’åç¬¬ä¸€çš„è¯­æ–™ä½œä¸ºè¯¥ id çš„å”¯ä¸€ç•™å­˜è¯­æ–™ã€‚

é—®é¢˜ï¼š
{data['question']}
å¾…è¯„åˆ†è¯­æ–™ï¼š
{contents_text}

...

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºæ¯æ¡è¯­æ–™çš„è¯„åˆ†ï¼Œæ¯è¡Œä¸€æ¡ï¼š
è¯­æ–™1: è¯­è¨€è‡ªç„¶åº¦=X, æ¡ç†æ€§=Y, é—®ç­”å¯¹åº”åº¦=Zã€‚
è¯­æ–™2: è¯­è¨€è‡ªç„¶åº¦=X, æ¡ç†æ€§=Y, é—®ç­”å¯¹åº”åº¦=Zã€‚
/no_think
<|im_end|>
<|im_start|>assistant
"""
        return prompt
    
    def _parse_scores(self, score_text: str, num_contents: int) -> List[Dict[str, int]]:
        """
        è§£æ LLM è¾“å‡ºçš„è¯„åˆ†ç»“æœ
        
        Args:
            score_text: LLM è¾“å‡ºçš„è¯„åˆ†æ–‡æœ¬
            num_contents: è¯­æ–™æ•°é‡
            
        Returns:
            è¯„åˆ†åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯åŒ…å«ä¸‰ä¸ªè¯„åˆ†çš„å­—å…¸
            
        Raises:
            ValueError: è§£æå¤±è´¥
        """
        scores = []
        
        # åŒ¹é… "è¯­æ–™X: è¯­è¨€è‡ªç„¶åº¦=A, æ¡ç†æ€§=B, é—®ç­”å¯¹åº”åº¦=C" æ ¼å¼
        pattern = r'è¯­æ–™\s*(\d+)\s*[:ï¼š]\s*è¯­è¨€è‡ªç„¶åº¦\s*[=ï¼]\s*(\d+)\s*[,ï¼Œ]\s*æ¡ç†æ€§\s*[=ï¼]\s*(\d+)\s*[,ï¼Œ]\s*é—®ç­”å¯¹åº”åº¦\s*[=ï¼]\s*(\d+)'
        matches = re.findall(pattern, score_text)
        
        if len(matches) < num_contents:
            raise ValueError(f"è¯„åˆ†è§£æå¤±è´¥: æœŸæœ› {num_contents} æ¡è¯„åˆ†ï¼Œå®é™…è§£æåˆ° {len(matches)} æ¡")
        
        # æŒ‰è¯­æ–™ç¼–å·æ’åº
        matches_sorted = sorted(matches, key=lambda x: int(x[0]))
        
        for match in matches_sorted[:num_contents]:
            idx, naturalness, coherence, relevance = match
            scores.append({
                'naturalness': int(naturalness),
                'coherence': int(coherence),
                'relevance': int(relevance)
            })
        
        return scores
    
    @torch.no_grad()
    def filter_corpus(
        self,
        input_dir: str,
        output_path: str,
        naturalness_threshold: int = 5,
        max_new_tokens: int = 256,
        verbose: bool = True
    ) -> Dict[str, float]:
        """
        å¯¹è¯­æ–™è¿›è¡Œç­›é€‰
        
        ç­›é€‰é€»è¾‘ï¼š
        1. è¯»å–æ¯ä¸ª id å¯¹åº”çš„æ‰€æœ‰è¯­æ–™
        2. ä½¿ç”¨ LLM å¯¹æ¯æ¡è¯­æ–™è¯„åˆ†ï¼ˆè¯­è¨€è‡ªç„¶åº¦ã€æ¡ç†æ€§ã€é—®ç­”å¯¹åº”åº¦ï¼‰
        3. æ’é™¤è¯­è¨€è‡ªç„¶åº¦ä½äºé˜ˆå€¼çš„è¯­æ–™
        4. å¯¹å‰©ä½™è¯­æ–™æŒ‰ä¼˜å…ˆçº§æ’åºï¼ˆè¯­è¨€è‡ªç„¶åº¦ > æ¡ç†æ€§ > é—®ç­”å¯¹åº”åº¦ï¼‰
        5. å–ç¬¬ä¸€ä½ä½œä¸ºè¯¥ id çš„å”¯ä¸€ç•™å­˜è¯­æ–™
        
        Args:
            input_dir: è¾“å…¥è¯­æ–™ç›®å½•ï¼ˆåŒ…å« .jsonl æ–‡ä»¶ï¼‰
            output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆ.jsonlï¼‰
            naturalness_threshold: è¯­è¨€è‡ªç„¶åº¦é˜ˆå€¼ï¼Œä½äºæ­¤å€¼çš„è¯­æ–™è¢«æ’é™¤
            max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
            
        Returns:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        self._load_model()
        
        # è¯»å–æ‰€æœ‰è¯­æ–™ï¼ˆæ¯æ¡è®°å½•åŒ…å« id, question, contentsï¼‰
        print(f"ğŸ“‚ æ­£åœ¨è¯»å–è¯­æ–™ç›®å½•: {input_dir}")
        corpus_data = []
        
        jsonl_files = sorted([f for f in os.listdir(input_dir) if f.endswith('.jsonl')])
        if not jsonl_files:
            raise FileNotFoundError(f"ç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ° .jsonl æ–‡ä»¶: {input_dir}")
        
        for jsonl_file in jsonl_files:
            file_path = os.path.join(input_dir, jsonl_file)
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    entry = json.loads(line.strip())
                    # ç¡®ä¿ contents æ˜¯åˆ—è¡¨
                    if isinstance(entry['contents'], str):
                        entry['contents'] = [entry['contents']]
                    corpus_data.append(entry)
        
        total_ids = len(corpus_data)
        print(f"   - æ€» ID æ•°é‡: {total_ids}")
        print(f"   - è¯­è¨€è‡ªç„¶åº¦é˜ˆå€¼: {naturalness_threshold}")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else '.', exist_ok=True)
        
        # ç­›é€‰ç»Ÿè®¡
        retained_count = 0
        discarded_count = 0
        error_count = 0
        retained_corpus = []
        
        # é€æ¡è¿›è¡Œç­›é€‰
        for data in tqdm(corpus_data, desc="ç­›é€‰è¯­æ–™"):
            sample_id = data['id']
            contents = data['contents']
            
            if len(contents) == 0:
                discarded_count += 1
                continue
            
            try:
                # æ„å»ºè¯„åˆ† prompt
                prompt = self._build_scoring_prompt(data)
                inputs = self.tokenizer(prompt, return_tensors='pt', add_special_tokens=False)
                inputs = {k: v.to(self.device) for k, v in inputs.items()}
                
                # ç”Ÿæˆè¯„åˆ†
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_new_tokens,
                    do_sample=True,
                    temperature=0.7,
                    top_p=0.8,
                    top_k=20,
                    min_p=0,
                    pad_token_id=self.tokenizer.pad_token_id
                )
                
                score_text = self.tokenizer.decode(
                    outputs[0][inputs['input_ids'].shape[1]:],
                    skip_special_tokens=True
                )
                if verbose:
                    origin_text = self.tokenizer.decode(
                        outputs[0],
                        skip_special_tokens=True
                    )
                    print(f"ğŸ“ ID {sample_id} è¯„åˆ†ç»“æœ: {origin_text}...")
                
                # è§£æè¯„åˆ†
                scores = self._parse_scores(score_text, len(contents))
                
                # ç­›é€‰ï¼šæ’é™¤è¯­è¨€è‡ªç„¶åº¦ä½äºé˜ˆå€¼çš„
                valid_indices = [
                    i for i, s in enumerate(scores) 
                    if s['naturalness'] >= naturalness_threshold
                ]
                
                if not valid_indices:
                    # æ‰€æœ‰è¯­æ–™éƒ½è¢«æ’é™¤
                    discarded_count += 1
                    continue
                
                # æ’åºï¼šæŒ‰ (è¯­è¨€è‡ªç„¶åº¦, æ¡ç†æ€§, é—®ç­”å¯¹åº”åº¦) é™åº
                valid_indices.sort(
                    key=lambda i: (
                        scores[i]['naturalness'],
                        scores[i]['coherence'],
                        scores[i]['relevance']
                    ),
                    reverse=True
                )
                
                # å–ç¬¬ä¸€ä½
                best_idx = valid_indices[0]
                retained_corpus.append({
                    'id': sample_id,
                    'question': data.get('question', ''),
                    'contents': f'é—®é¢˜ï¼š{data.get("question", "")}\nå›ç­”ï¼š{contents[best_idx]}',
                    'scores': scores[best_idx]
                })
                retained_count += 1
                
            except Exception as e:
                if verbose:
                    print(f"âš ï¸ ID {sample_id} å¤„ç†å¤±è´¥: {str(e)}")
                error_count += 1
                discarded_count += 1
        
        # ä¿å­˜ç»“æœ
        filtered_output_path = os.path.join(output_path, 'filtered.jsonl')
        os.makedirs(output_path, exist_ok=True)
        with open(filtered_output_path, 'w', encoding='utf-8') as f:
            for entry in retained_corpus:
                f.write(json.dumps(entry, ensure_ascii=False) + '\n')
        
        # ç»Ÿè®¡ä¿¡æ¯
        retention_rate = retained_count / total_ids if total_ids > 0 else 0
        
        print(f"\nâœ… è¯­æ–™ç­›é€‰å®Œæˆï¼")
        print(f"   - æ€» ID æ•°é‡: {total_ids}")
        print(f"   - æœ‰ç•™å­˜è¯­æ–™çš„ ID: {retained_count}")
        print(f"   - æ— ç•™å­˜è¯­æ–™çš„ ID: {discarded_count}")
        print(f"   - å¤„ç†é”™è¯¯æ•°é‡: {error_count}")
        print(f"   - ç•™å­˜æ¯”ä¾‹: {retention_rate:.2%}")
        print(f"   - è¾“å‡ºæ–‡ä»¶: {output_path}")
        
        return {
            'total_ids': total_ids,
            'retained_count': retained_count,
            'discarded_count': discarded_count,
            'error_count': error_count,
            'retention_rate': retention_rate
        }


def run_corpus_filter_pipeline(
    # è¾“å…¥è¾“å‡ºè·¯å¾„
    input_dir: str,
    output_path: str,
    # æ¨¡å‹å‚æ•°
    model_path: str = "Qwen3-1.7B",
    device: str = None,
    # ç­›é€‰å‚æ•°
    naturalness_threshold: int = 5,
    max_new_tokens: int = 256,
    verbose: bool = True
):
    """
    æ–‡æœ¬è¯­æ–™ç­›é€‰æµæ°´çº¿
    
    Args:
        input_dir: è¾“å…¥è¯­æ–™ç›®å½•ï¼ˆåŒ…å« skip_clustering=True ç”Ÿæˆçš„ .jsonl æ–‡ä»¶ï¼‰
        output_path: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆ.jsonlï¼‰
        model_path: Qwen3 æ¨¡å‹è·¯å¾„
        device: è®¾å¤‡
        naturalness_threshold: è¯­è¨€è‡ªç„¶åº¦é˜ˆå€¼ï¼ˆ1-10ï¼‰ï¼Œä½äºæ­¤å€¼çš„è¯­æ–™è¢«æ’é™¤
        max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    """
    print("=" * 60)
    print("æ–‡æœ¬è¯­æ–™ç­›é€‰æµæ°´çº¿")
    print("=" * 60)
    
    # åˆ›å»ºç­›é€‰å™¨
    corpus_filter = CorpusFilter(
        model_path=model_path,
        device=device
    )
    
    # æ‰§è¡Œç­›é€‰
    stats = corpus_filter.filter_corpus(
        input_dir=input_dir,
        output_path=output_path,
        naturalness_threshold=naturalness_threshold,
        max_new_tokens=max_new_tokens,
        verbose=verbose
    )
    
    print("\n" + "=" * 60)
    print("âœ… æ–‡æœ¬è¯­æ–™ç­›é€‰æµæ°´çº¿å®Œæˆï¼")
    print("=" * 60)
    
    return stats


if __name__ == "__main__":
    import fire
    fire.Fire()
