"""
è¯­æ–™ç”Ÿæˆä¸å­˜å‚¨æ¨¡å—

ä¸»è¦åŠŸèƒ½ï¼š
1. TrafficEmbeddingGenerator: ä½¿ç”¨ TrafficEmbedder ç”Ÿæˆæµé‡å‘é‡
2. TextCorpusGenerator: ä½¿ç”¨ ProposeModel ç”Ÿæˆæ–‡æœ¬è¯­æ–™
3. å‘é‡åº“å’Œ BM25 è¯­æ–™åº“çš„æ„å»º
"""

import json
import os
import pickle
import numpy as np
import torch
from typing import List, Dict, Optional, Tuple
from tqdm import tqdm
from z2.RAG.utils import save_corpus

class TrafficEmbeddingGenerator:
    """
    æµé‡åµŒå…¥ç”Ÿæˆå™¨ - ä»é¢„å¤„ç†çš„æ•°æ®é›†ç”Ÿæˆæµé‡å‘é‡
    
    ä½¿ç”¨ TrafficEmbedder æ¨¡å‹å¯¹æµé‡è¿›è¡Œç¼–ç ï¼Œç”Ÿæˆå‘é‡å¹¶å­˜å‚¨ã€‚
    """
    
    def __init__(self, args):
        """
        åˆå§‹åŒ–æµé‡åµŒå…¥ç”Ÿæˆå™¨
        
        Args:
            args: æ¨¡å‹å‚æ•°ï¼Œéœ€åŒ…å« TrafficEmbedder æ‰€éœ€çš„é…ç½®
            device: è®¾å¤‡ ('cuda' æˆ– 'cpu')ï¼ŒNone åˆ™è‡ªåŠ¨é€‰æ‹©
        """
        self.args = args
        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
        self.model = None
        self.processor = None
    
    def _load_model(self):
        """å»¶è¿ŸåŠ è½½æ¨¡å‹"""
        if self.model is None:
            from z2.model import TrafficEmbedder
            from transformers import AutoProcessor
            
            self.model = TrafficEmbedder(self.args)
            self.model.to(self.device)
            # self.model.dispatch()
            self.model.resume(self.args)
            self.model.eval()
            
            self.processor = AutoProcessor.from_pretrained(self.args.llm)
            
            print(f"âœ… TrafficEmbedder å·²åŠ è½½ (è®¾å¤‡: {self.device})")
    
    @torch.no_grad()
    def generate_embeddings(
        self,
        dataset_path: str,
        output_dir: str,
        batch_size: int = 8,
        save_threshold: int = 1000,
        normalize: bool = True
    ) -> None:
        """
        ä»æ•°æ®é›†ç”Ÿæˆæµé‡åµŒå…¥å‘é‡ï¼Œä½¿ç”¨ CustomDataset å’Œç´¯ç§¯é˜ˆå€¼å­˜å‚¨
        
        Args:
            dataset_path: æ•°æ®é›†ç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆæ¯æ‰¹ä¼šä¿å­˜åˆ°å•ç‹¬çš„æ–‡ä»¶ï¼‰
            batch_size: æ•°æ®å¤„ç†æ‰¹å¤„ç†å¤§å°
            save_threshold: ç´¯ç§¯å¤šå°‘æ ·æœ¬åä¿å­˜ä¸€æ¬¡
            normalize: æ˜¯å¦å½’ä¸€åŒ–åµŒå…¥å‘é‡
        """
        from torch.utils.data import DataLoader
        from dataset import CustomDataset, collate_TrafficEmbeddingDataset
        
        self._load_model()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # åŠ è½½æ•°æ®é›†
        print(f"ğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®é›†: {dataset_path}")
        dataset = CustomDataset(dataset_path)
        
        # åˆ›å»º DataLoader
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=False,
            collate_fn=collate_TrafficEmbeddingDataset,
            num_workers=0
        )
        
        print(f"ğŸ”„ å¼€å§‹ç”ŸæˆåµŒå…¥å‘é‡...")
        print(f"   - å¤„ç†æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   - å­˜å‚¨é˜ˆå€¼: {save_threshold}")
        
        total_samples = 0
        save_batch_idx = 0
        
        # ç´¯ç§¯çš„embeddingså’Œids
        accumulated_embeddings = []
        accumulated_ids = []
        
        # åˆ†æ‰¹å¤„ç†å’Œå­˜å‚¨
        for batch_data, txt_filenames in tqdm(dataloader, desc="ç”ŸæˆåµŒå…¥"):
            # batch_data æ˜¯å­—å…¸: {input_ids, payloads, position_ids, attention_mask}
            # labels æ˜¯åˆ—è¡¨ï¼ŒåŒ…å« txt_filename (id)
            
            # ç”ŸæˆåµŒå…¥å¹¶ç›´æ¥è½¬åˆ°cpu
            embeddings = self.model(**batch_data, normalize=normalize).cpu()
            embeddings_np = embeddings.numpy()
            
            # ä» labels ä¸­ç›´æ¥è·å– id
            accumulated_ids.extend(txt_filenames)
            
            # ç´¯ç§¯embeddings
            accumulated_embeddings.append(embeddings_np)
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°å­˜å‚¨é˜ˆå€¼
            if len(accumulated_ids) >= save_threshold:
                # åˆå¹¶å¹¶ä¿å­˜
                merged_embeddings = np.vstack(accumulated_embeddings)
                
                batch_output = {
                    'embeddings': merged_embeddings,
                    'ids': accumulated_ids,
                    'dim': merged_embeddings.shape[1],
                    'num_samples': len(accumulated_ids)
                }
                
                batch_file = os.path.join(output_dir, f'embeddings_batch_{save_batch_idx:05d}.pkl')
                with open(batch_file, 'wb') as f:
                    pickle.dump(batch_output, f)
                
                total_samples += len(accumulated_ids)
                save_batch_idx += 1
                
                # é‡ç½®ç´¯ç§¯æ•°æ®
                accumulated_embeddings = []
                accumulated_ids = []
        
        # ä¿å­˜å‰©ä½™çš„æ•°æ®
        if len(accumulated_ids) > 0:
            merged_embeddings = np.vstack(accumulated_embeddings)
            
            batch_output = {
                'embeddings': merged_embeddings,
                'ids': accumulated_ids,
                'dim': merged_embeddings.shape[1],
                'num_samples': len(accumulated_ids)
            }
            
            batch_file = os.path.join(output_dir, f'embeddings_batch_{save_batch_idx:05d}.pkl')
            with open(batch_file, 'wb') as f:
                pickle.dump(batch_output, f)
            
            total_samples += len(accumulated_ids)
            save_batch_idx += 1
        
        print(f"\nâœ… åµŒå…¥ç”Ÿæˆå®Œæˆï¼")
        print(f"   - æ ·æœ¬æ€»æ•°: {total_samples}")
        print(f"   - ä¿å­˜æ‰¹æ¬¡æ•°é‡: {save_batch_idx}")
        print(f"   - è¾“å‡ºç›®å½•: {output_dir}")
    
    #TODOï¼šåˆ†æ‰¹æ„å»ºç´¢å¼•
    def build_vector_index(
        self,
        embeddings_path: str,
        index_dir: str,
        index_type: str = 'flat',
        verbose: bool = True
    ) -> None:
        """
        ä»åµŒå…¥æ–‡ä»¶æ„å»º FAISS å‘é‡ç´¢å¼•ï¼ˆä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        
        Args:
            embeddings_path: åµŒå…¥æ–‡ä»¶è·¯å¾„ (.pkl)
            index_dir: ç´¢å¼•ä¿å­˜ç›®å½•
            index_type: ç´¢å¼•ç±»å‹ ('flat' æˆ– 'ivf')
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        """
        from z2.RAG.vector_utils import build_faiss_index
        
        # éå†è¾“å…¥ç›®å½•ä¸‹çš„æ‰€æœ‰ pkl æ–‡ä»¶
        embeddings_dir = embeddings_path
        pkl_files = sorted([f for f in os.listdir(embeddings_dir) if f.endswith('.pkl')])
        
        if not pkl_files:
            raise FileNotFoundError(f"ç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ° .pkl æ–‡ä»¶: {embeddings_dir}")
        
        all_embeddings = []
        all_ids = []
        
        for pkl_file in pkl_files:
            pkl_path = os.path.join(embeddings_dir, pkl_file)
            with open(pkl_path, 'rb') as f:
                data = pickle.load(f)
            all_embeddings.append(data['embeddings'])
            all_ids.extend(data['ids'])
        
        embeddings = np.vstack(all_embeddings)
        
        # ä½¿ç”¨å…±äº«çš„ç´¢å¼•æ„å»ºå‡½æ•°
        build_faiss_index(
            embeddings=embeddings,
            doc_ids=all_ids,
            doc_contents=None,
            index_dir=index_dir,
            index_type=index_type,
            verbose=verbose
        )

def run_traffic_embedding_pipeline(
    # æ•°æ®é›†è·¯å¾„
    dataset_path: str,
    # è¾“å‡ºè·¯å¾„
    embeddings_output_dir: str,
    index_output_dir: str,
    # æ¨¡å‹å‚æ•°
    llm: str = 'Qwen3-VL-Embedding-2B',
    projector: str = 'linear',
    linear_output_dim: int = 2048,
    # ç”Ÿæˆå‚æ•°
    batch_size: int = 8,
    save_threshold: int = 1000,
    normalize: bool = True,
    # ç´¢å¼•å‚æ•°
    index_type: str = 'flat',
    verbose: bool = True,
    # åŠ è½½å‚æ•°
    resume_log: bool = True,
    resume_encoder: str = None,
    resume_linear: str = None,
    resume_lora0: str = None
):
    """
    æµé‡åµŒå…¥ç”Ÿæˆæµæ°´çº¿ï¼šç”ŸæˆåµŒå…¥å‘é‡ + æ„å»ºå‘é‡ç´¢å¼•
    
    Args:
        dataset_path: æ•°æ®é›†ç›®å½•è·¯å¾„
        embeddings_output_dir: åµŒå…¥å‘é‡è¾“å‡ºç›®å½•
        index_output_dir: å‘é‡ç´¢å¼•è¾“å‡ºç›®å½•
        llm: LLM æ¨¡å‹è·¯å¾„
        train_mode: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
        eval_mode: æ˜¯å¦ä¸ºè¯„ä¼°æ¨¡å¼
        projector: æŠ•å½±å™¨ç±»å‹
        projector_arch: æŠ•å½±å™¨æ¶æ„
        adapter_path: é€‚é…å™¨è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        batch_size: æ‰¹å¤„ç†å¤§å°
        save_threshold: å­˜å‚¨é˜ˆå€¼
        normalize: æ˜¯å¦å½’ä¸€åŒ–
        index_type: ç´¢å¼•ç±»å‹ ('flat' æˆ– 'ivf')
        device: è®¾å¤‡
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    """
    from types import SimpleNamespace
    
    print("=" * 60)
    print("æµé‡åµŒå…¥ç”Ÿæˆæµæ°´çº¿")
    print("=" * 60)
    
    # æ„å»º args
    args = SimpleNamespace(
        llm=llm,
        projector=projector,
        linear_output_dim=linear_output_dim,
        resume_log=resume_log,
        resume_encoder=resume_encoder,
        resume_linear=resume_linear,
        resume_lora0=resume_lora0,
        eval_mode=True,
        train_mode=False
    )
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = TrafficEmbeddingGenerator(args)
    
    # æ­¥éª¤ 1: ç”ŸæˆåµŒå…¥å‘é‡
    print("\nã€æ­¥éª¤ 1/2ã€‘ç”Ÿæˆæµé‡åµŒå…¥å‘é‡")
    print("-" * 60)
    generator.generate_embeddings(
        dataset_path=dataset_path,
        output_dir=embeddings_output_dir,
        batch_size=batch_size,
        save_threshold=save_threshold,
        normalize=normalize
    )
    
    # æ­¥éª¤ 2: æ„å»ºå‘é‡ç´¢å¼•
    print("\nã€æ­¥éª¤ 2/2ã€‘æ„å»ºå‘é‡ç´¢å¼•")
    print("-" * 60)
    generator.build_vector_index(
        embeddings_path=embeddings_output_dir,
        index_dir=index_output_dir,
        index_type=index_type,
        verbose=verbose
    )
    
    print("\n" + "=" * 60)
    print("âœ… æµé‡åµŒå…¥ç”Ÿæˆæµæ°´çº¿å®Œæˆï¼")
    print("=" * 60)

class TextCorpusGenerator:
    """
    æ–‡æœ¬è¯­æ–™ç”Ÿæˆå™¨ - ä»é¢„å¤„ç†çš„æ•°æ®é›†ä½¿ç”¨ LLM ç”Ÿæˆæ–‡æœ¬è¯­æ–™
    
    ä½¿ç”¨ ProposeModel å¯¹æµé‡ç†è§£é—®é¢˜è¿›è¡Œå›ç­”ï¼Œç”Ÿæˆæ–‡æœ¬è¯­æ–™å¹¶å­˜å‚¨ã€‚
    """
    
    def __init__(self, args, device: str = None):
        """
        åˆå§‹åŒ–æ–‡æœ¬è¯­æ–™ç”Ÿæˆå™¨
        
        Args:
            args: æ¨¡å‹å‚æ•°ï¼Œéœ€åŒ…å« ProposeModel æ‰€éœ€çš„é…ç½®
            device: è®¾å¤‡ ('cuda' æˆ– 'cpu')ï¼ŒNone åˆ™è‡ªåŠ¨é€‰æ‹©
        """
        self.args = args
        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
        self.model = None
        self.tokenizer = None
    
    def _load_model(self):
        """å»¶è¿ŸåŠ è½½æ¨¡å‹"""
        if self.model is None:
            from z1.model import ProposeModel
            from transformers import AutoTokenizer
            
            self.model = ProposeModel(self.args)
            self.model.eval()
            # self.model.to(self.device)
            self.model.dispatch()
            self.model.resume(self.args)
            
            self.tokenizer = AutoTokenizer.from_pretrained(self.args.llm)
            
            print(f"âœ… ProposeModel å·²åŠ è½½ (è®¾å¤‡: {self.device})")
    
    @torch.no_grad()
    def generate_corpus(
        self,
        dataset_path: str,
        output_dir: str,
        num_generations: int = 5,
        entropy_threshold: float = 1.5,
        save_threshold: int = 1000,
        max_new_tokens: int = 512,
        min_new_tokens: int = 64,
        temperature: float = 0.7,
        repetition_penalty: float = 1.25,
        generation_mode: str = "batch",
        early_stop_batch: int = None
    ) -> None:
        """
        ä»æ•°æ®é›†ç”Ÿæˆæ–‡æœ¬è¯­æ–™ï¼Œä½¿ç”¨å¤šç»“æœç”Ÿæˆã€LLMèšç±»å’Œç†µç­›é€‰
        
        å¯¹æ¯ä¸ªæ ·æœ¬ï¼š
        1. ç”Ÿæˆ num_generations æ¡ç»“æœ
        2. å°†ç»“æœæ‹¼æ¥åè¾“å…¥ LLM è¿›è¡Œè¯­ä¹‰èšç±»
        3. æ ¹æ®èšç±»ç»“æœè®¡ç®—ç†µ
        4. è‹¥ç†µè¶…è¿‡é˜ˆå€¼åˆ™æŠ›å¼ƒæœ¬è½®è¾“å‡ºï¼Œå¦åˆ™ä»ç»“æœæ•°é‡æœ€å¤šçš„èšç±»ä¸­éšæœºé€‰æ‹©ä¸€æ¡
        
        Args:
            dataset_path: æ•°æ®é›†ç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆæ¯æ‰¹ä¼šä¿å­˜åˆ°å•ç‹¬çš„æ–‡ä»¶ï¼‰
            num_generations: æ¯ä¸ªæ ·æœ¬ç”Ÿæˆçš„ç»“æœæ•°é‡
            entropy_threshold: ç†µé˜ˆå€¼ï¼Œè¶…è¿‡åˆ™æŠ›å¼ƒæœ¬è½®è¾“å‡º
            save_threshold: ç´¯ç§¯å¤šå°‘æ ·æœ¬åä¿å­˜ä¸€æ¬¡
            max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            generation_mode: ç”Ÿæˆæ¨¡å¼ï¼ˆ"batch" ä¸€æ¬¡æ€§ç”Ÿæˆ / "loop" å¾ªç¯é€æ¡ç”Ÿæˆï¼‰
        """
        import math
        import random
        import re
        from torch.utils.data import DataLoader
        from dataset import CustomDataset, collate_LLMDataset_leftpadding
        
        self._load_model()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # åŠ è½½æ•°æ®é›†
        print(f"ğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®é›†: {dataset_path}")
        dataset = CustomDataset(dataset_path)
        
        # åˆ›å»º DataLoaderï¼Œbatch_size å›ºå®šä¸º 1
        dataloader = DataLoader(
            dataset,
            batch_size=1,
            shuffle=True,
            collate_fn=lambda batch: collate_LLMDataset_leftpadding(batch, keep_labels=False),
            num_workers=0
        )

        generation_mode = generation_mode.lower()
        if generation_mode not in {"batch", "loop"}:
            raise ValueError(f"generation_mode å¿…é¡»æ˜¯ 'batch' æˆ– 'loop'ï¼Œå½“å‰: {generation_mode}")
        
        print(f"ğŸ”„ å¼€å§‹ç”Ÿæˆæ–‡æœ¬è¯­æ–™...")
        print(f"   - æ¯æ ·æœ¬ç”Ÿæˆæ•°é‡: {num_generations}")
        print(f"   - ç”Ÿæˆæ¨¡å¼: {generation_mode}")
        print(f"   - ç†µé˜ˆå€¼: {entropy_threshold}")
        print(f"   - å­˜å‚¨é˜ˆå€¼: {save_threshold}")
        print(f"   - æœ€å¤§ç”Ÿæˆtokenæ•°: {max_new_tokens}")
        print(f"   - æœ€å°ç”Ÿæˆtokenæ•°: {min_new_tokens}")
        print(f"   - æ¸©åº¦: {temperature}")
        print(f"   - é‡å¤æƒ©ç½š: {repetition_penalty}")
        if early_stop_batch is not None:
            print(f"   - æå‰åœæ­¢æ‰¹æ¬¡: {early_stop_batch}")

        
        total_samples = 0
        discarded_samples = 0
        save_batch_idx = 0
        
        # ç´¯ç§¯çš„è¯­æ–™å’Œids
        accumulated_corpus = []
        accumulated_ids = []
        
        def compute_entropy(cluster_sizes: list) -> float:
            """è®¡ç®—èšç±»ç»“æœçš„ç†µ"""
            total = sum(cluster_sizes)
            if total == 0:
                return 0.0
            entropy = 0.0
            for size in cluster_sizes:
                if size > 0:
                    p = size / total
                    entropy -= p * math.log2(p)
            return entropy
        
        def parse_clusters(cluster_text: str, num_results: int) -> dict:
            """è§£æ LLM è¾“å‡ºçš„èšç±»ç»“æœ
            
            æœŸæœ›æ ¼å¼: 
            èšç±»1: 1, 3, 5
            èšç±»2: 2, 4
            
            Raises:
                ValueError: è§£æå¤±è´¥æˆ–èšç±»ç»“æœæœªè¦†ç›–æ‰€æœ‰ç¼–å·æˆ–æœ‰é‡å¤ç¼–å·
            """
            clusters = {}
            # è¦†ç›–çŠ¶æ€æ•°ç»„ï¼ŒFalseè¡¨ç¤ºæœªè¦†ç›–ï¼ŒTrueè¡¨ç¤ºå·²è¦†ç›–
            covered = [False] * (num_results + 1)  # ç´¢å¼•0ä¸ä½¿ç”¨ï¼Œ1~num_resultså¯¹åº”ç¼–å·
            uncovered_count = num_results  # å‰©ä½™æœªè¢«è¦†ç›–çš„ç¼–å·æ•°é‡
            
            # åŒ¹é… "èšç±»X: 1, 2, 3" æˆ– "ç±»åˆ«X: 1, 2, 3" æ ¼å¼
            pattern = r'(?:èšç±»|ç±»åˆ«|Cluster)\s*(\d+)\s*[:ï¼š]\s*([\d,ï¼Œ\s]+)'
            matches = re.findall(pattern, cluster_text, re.IGNORECASE)
            
            for cluster_id, members_str in matches:
                # è§£ææˆå‘˜ç¼–å·
                members_str = members_str.replace('ï¼Œ', ',')
                members = []
                for m in members_str.split(','):
                    m = m.strip()
                    if m.isdigit():
                        idx = int(m)
                        if 1 <= idx <= num_results:
                            if not covered[idx]:
                                covered[idx] = True
                                uncovered_count -= 1
                                members.append(idx)
                            else:
                                raise ValueError(f"èšç±»è§£æå¤±è´¥: ç¼–å· {idx} é‡å¤å‡ºç°")
                if members:
                    clusters[int(cluster_id)] = members
            
            # å¦‚æœè§£æå¤±è´¥ï¼ŒæŠ›å‡ºé”™è¯¯
            if not clusters:
                raise ValueError(f"èšç±»è§£æå¤±è´¥: æœªèƒ½ä»è¾“å‡ºä¸­æå–æœ‰æ•ˆèšç±»")
            
            # æ£€æŸ¥æ˜¯å¦è¦†ç›–äº†æ‰€æœ‰ç¼–å·
            if uncovered_count != 0:
                missing = [i for i in range(1, num_results + 1) if not covered[i]]
                raise ValueError(f"èšç±»ç»“æœä¸å®Œæ•´: ç¼ºå°‘ç¼–å· {missing}")
            
            return clusters
        
        def build_clustering_prompt(results: list) -> str:
            """æ„å»ºèšç±» prompt"""
            results_text = "\n".join([
                f"[{i+1}] {r}" for i, r in enumerate(results)
            ])
            prompt = f"""<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬èšç±»åŠ©æ‰‹ï¼Œæ“…é•¿ä»è¯­ä¹‰è§’åº¦å¯¹æ–‡æœ¬è¿›è¡Œèšç±»ã€‚<|im_end|>
<|im_start|>user
ä»¥ä¸‹æ˜¯é’ˆå¯¹åŒä¸€ç½‘ç»œæµé‡çš„å¤šæ¡åˆ†æç»“æœï¼Œè¯·ä»è¯­ä¹‰è§’åº¦å°†å®ƒä»¬èšç±»ï¼Œç›´æ¥è¾“å‡ºèšç±»ç»“æœä¸éœ€è¦è¾“å‡ºè§£é‡Šã€‚

åˆ†æç»“æœï¼š
{results_text}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºèšç±»ç»“æœï¼Œæ¯è¡Œä¸€ä¸ªèšç±»ï¼ŒåŒ…å«å±äºè¯¥èšç±»çš„ç»“æœç¼–å·ï¼š
èšç±»1: 1, 3, 5
èšç±»2: 2, 4
...<|im_end|>
<|im_start|>assistant
"""
            return prompt
        
        # åˆ†æ‰¹å¤„ç†å’Œå­˜å‚¨
        i = 0
        for batch_data, txt_filenames in tqdm(dataloader, desc="ç”Ÿæˆè¯­æ–™"):
            i += 1
            if early_stop_batch is not None and i > early_stop_batch:
                break
            try:
                # batch_size=1ï¼Œæ‰€ä»¥ txt_filenames åªæœ‰ä¸€ä¸ªå…ƒç´ 
                sample_id = txt_filenames[0]
                input_length = batch_data['input_ids'].shape[1]
                
                if generation_mode == "batch":
                    # 1. å¯¹åŒä¸€æ ·æœ¬ç”Ÿæˆå¤šæ¡ç»“æœï¼ˆé€šè¿‡å¤åˆ¶ batch_data å®ç°æ‰¹é‡ç”Ÿæˆï¼‰
                    expanded_batch = {}
                    for key, value in batch_data.items():
                        if key == 'labels':
                            continue
                        if key == 'payloads':
                            # payloads æ˜¯åˆ—è¡¨ï¼Œéœ€è¦å¤åˆ¶æ¯ä¸ªå…ƒç´ 
                            assert len(value) == 1 and isinstance(value[0], tuple) and len(value[0]) == 3
                            expanded_batch[key] = [value[0] for _ in range(num_generations)]
                        elif key == 'position_ids':
                            # position_ids åœ¨ç¬¬äºŒä¸ªç»´åº¦å¤åˆ¶ï¼Œå½¢çŠ¶ä» [3, 1, seq_len] å˜æˆ [3, num_generations, seq_len]
                            expanded_batch[key] = value.repeat(1, num_generations, 1)
                        else:
                            # å…¶ä»– tensor æ•°æ®ï¼Œåœ¨ç¬¬ä¸€ä¸ªç»´åº¦å¤åˆ¶
                            expanded_batch[key] = value.repeat(num_generations, 1)

                    # expanded_batch = {
                    #     k: (
                    #         [x.to(self.device) if torch.is_tensor(x) else x for x in v]
                    #         if isinstance(v, list)
                    #         else (v.to(self.device) if torch.is_tensor(v) else v)
                    #     )
                    #     for k, v in expanded_batch.items()
                    # }
                    
                    # æ‰¹é‡ç”Ÿæˆ
                    outputs = self.model.generate(
                        **expanded_batch,
                        max_new_tokens=max_new_tokens,
                        # min_new_tokens=min_new_tokens,
                        repetition_penalty=repetition_penalty,
                        do_sample=True,
                        temperature=temperature,
                        top_p=0.9
                    ).cpu()
                else:
                    # é€æ¡ç”Ÿæˆï¼ˆå¾ªç¯ num_generations æ¬¡ï¼Œæ¯æ¬¡åªå¤„ç†ä¸€æ¡æ•°æ®ï¼‰
                    outputs_list = []
                    for _ in range(num_generations):
                        output = self.model.generate(
                            **batch_data,
                            max_new_tokens=max_new_tokens,
                            min_new_tokens=min_new_tokens,
                            repetition_penalty=repetition_penalty,
                            do_sample=True,
                            temperature=temperature,
                            top_p=0.9,
                        ).cpu()
                        outputs_list.append(output[0])

                    pad_token_id = (
                        self.tokenizer.pad_token_id
                        if self.tokenizer.pad_token_id is not None
                        else 0
                    )
                    outputs = torch.nn.utils.rnn.pad_sequence(
                        outputs_list,
                        batch_first=True,
                        padding_value=pad_token_id
                    )
                
                if early_stop_batch is not None:
                    accumulated_corpus.append({
                        'id': sample_id,
                        'payloads_len': batch_data['payloads'][0][0].shape[0],
                        'position_ids_shape': list(batch_data['position_ids'].shape),
                        'input_ids_shape': list(batch_data['input_ids'].shape),
                        'attention_mask_shape': list(batch_data['attention_mask'].shape),
                        'input_ids_decoded': self.tokenizer.decode(batch_data['input_ids'][0], skip_special_tokens=False),
                        'attention_mask': ' '.join([str(x.item()) for x in batch_data['attention_mask'][0]]),
                        'outputs_shape': list(outputs.shape),
                        'outputs_decoded': [self.tokenizer.decode(outputs[j], skip_special_tokens=False) for j in range(outputs.shape[0])]
                    })
                    continue
                # è§£ç æ‰€æœ‰ç”Ÿæˆçš„æ–‡æœ¬
                generated_results = []
                for i in range(num_generations):
                    generated_text = self.tokenizer.decode(
                        outputs[i][input_length:],
                        skip_special_tokens=True
                    )
                    generated_results.append(generated_text)
                
                #TODOï¼šåˆ†ç±»ä»»åŠ¡ä½œä¸ºé—®é¢˜æ—¶ï¼Œå¯ä»¥ç›´æ¥åˆ†ç±»ï¼Œä¸ç”¨LLMèšç±»
                # 2. æ„å»ºèšç±» prompt å¹¶è®© LLM è¿›è¡Œèšç±»
                clustering_prompt = build_clustering_prompt(generated_results)
                clustering_input = self.tokenizer(
                    clustering_prompt, 
                    return_tensors='pt', 
                    add_special_tokens=False
                )
                clustering_input['position_ids'] = torch.arange(
                    clustering_input['input_ids'].shape[1]
                ).unsqueeze(0).expand(3, -1, -1)
                clustering_input = {k: v.to(self.device) for k, v in clustering_input.items()}
                
                cluster_output = self.model.generate(
                    **clustering_input,
                    max_new_tokens=64,
                    do_sample=False,
                    temperature=0.1
                ).cpu()
                
                cluster_text = self.tokenizer.decode(
                    cluster_output[0][clustering_input['input_ids'].shape[1]:],
                    skip_special_tokens=True
                )
                
                # 3. è§£æèšç±»ç»“æœ
                clusters = parse_clusters(cluster_text, num_generations)
                cluster_sizes = [len(members) for members in clusters.values()]
                
                # 4. è®¡ç®—ç†µ
                entropy = compute_entropy(cluster_sizes)
                
                # 5. æ ¹æ®ç†µå†³å®šæ˜¯å¦ä¿ç•™ç»“æœ
                if entropy > entropy_threshold:
                    # ç†µè¶…è¿‡é˜ˆå€¼ï¼ŒæŠ›å¼ƒæœ¬è½®è¾“å‡º
                    discarded_samples += 1
                    continue
                
                # 6. ä»ç»“æœæ•°é‡æœ€å¤šçš„èšç±»ä¸­éšæœºé€‰æ‹©ä¸€æ¡
                largest_cluster_id = max(clusters.keys(), key=lambda k: len(clusters[k]))
                largest_cluster_members = clusters[largest_cluster_id]
                selected_idx = random.choice(largest_cluster_members) - 1  # è½¬ä¸º 0-indexed
                selected_result = generated_results[selected_idx]
                
                accumulated_corpus.append({
                    'id': sample_id,
                    'contents': selected_result
                })
                accumulated_ids.append(sample_id)
            except Exception as e:
                import traceback
                error_detail = traceback.format_exc()
                print(f"Error processing sample {sample_id}: {str(e)}\nè¯¦ç»†å †æ ˆä¿¡æ¯:\n{error_detail}")
                continue
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°å­˜å‚¨é˜ˆå€¼
            if len(accumulated_corpus) >= save_threshold:
                # ä¿å­˜å½“å‰æ‰¹æ¬¡
                batch_file = os.path.join(output_dir, f'corpus_batch_{save_batch_idx:05d}.jsonl')
                with open(batch_file, 'w', encoding='utf-8') as f:
                    for entry in accumulated_corpus:
                        f.write(json.dumps(entry, ensure_ascii=False) + '\n')
                
                total_samples += len(accumulated_corpus)
                save_batch_idx += 1
                
                # é‡ç½®ç´¯ç§¯æ•°æ®
                accumulated_corpus = []
                accumulated_ids = []
        
        # ä¿å­˜å‰©ä½™çš„æ•°æ®
        if len(accumulated_corpus) > 0:
            batch_file = os.path.join(output_dir, f'corpus_batch_{save_batch_idx:05d}.jsonl')
            with open(batch_file, 'w', encoding='utf-8') as f:
                for entry in accumulated_corpus:
                    f.write(json.dumps(entry, ensure_ascii=False) + '\n')
            
            total_samples += len(accumulated_corpus)
            save_batch_idx += 1
        
        print(f"\nâœ… è¯­æ–™ç”Ÿæˆå®Œæˆï¼")
        print(f"   - æœ‰æ•ˆæ ·æœ¬æ•°: {total_samples}")
        print(f"   - æŠ›å¼ƒæ ·æœ¬æ•°: {discarded_samples} (ç†µè¶…è¿‡é˜ˆå€¼)")
        print(f"   - ä¿å­˜æ‰¹æ¬¡æ•°é‡: {save_batch_idx}")
        print(f"   - è¾“å‡ºç›®å½•: {output_dir}")
    
    def build_bm25_index(
        self,
        corpus_path: str,
        index_dir: str,
        analyzer_name: str = 'whitespace',
        verbose: bool = True
    ) -> None:
        """
        ä»è¯­æ–™æ–‡ä»¶æ„å»º BM25 ç´¢å¼•
        
        Args:
            corpus_path: è¯­æ–™æ–‡ä»¶è·¯å¾„ (.jsonl)
            index_dir: ç´¢å¼•ä¿å­˜ç›®å½•
            analyzer_name: Lucene åˆ†æå™¨åç§°
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        """
        from z2.RAG.retriever.BM25 import build_index
        
        build_index(
            corpus_file=corpus_path,
            index_dir=index_dir,
            analyzer_name=analyzer_name,
            verbose=verbose
        )

def run_text_corpus_pipeline(
    # æ•°æ®é›†è·¯å¾„
    dataset_path: str,
    # è¾“å‡ºè·¯å¾„
    corpus_output_dir: str,
    index_output_dir: str,
    # æ¨¡å‹å‚æ•°
    llm: str = 'Qwen3-VL-8B-Instruct',
    projector: str = 'linear',
    linear_output_dim: int = 4096,
    # ç”Ÿæˆå‚æ•°
    num_generations: int = 5,
    entropy_threshold: float = 1.5,
    save_threshold: int = 1000,
    max_new_tokens: int = 512,
    generation_mode: str = "batch",
    # ç´¢å¼•å‚æ•°
    analyzer_name: str = 'whitespace',
    verbose: bool = True,
    # åŠ è½½å‚æ•°
    resume_log: bool = True,
    resume_encoder: str = None,
    resume_linear: str = None,
    resume_lora0: str = None,
    resume_lora1: str = None,
    # å…¶ä»–å‚æ•°
    early_stop_batch: int = None
):
    """
    æ–‡æœ¬è¯­æ–™ç”Ÿæˆæµæ°´çº¿ï¼šç”Ÿæˆè¯­æ–™ + æ„å»º BM25 ç´¢å¼•
    
    Args:
        dataset_path: æ•°æ®é›†ç›®å½•è·¯å¾„
        corpus_output_dir: è¯­æ–™è¾“å‡ºç›®å½•
        index_output_dir: BM25 ç´¢å¼•è¾“å‡ºç›®å½•
        llm: LLM æ¨¡å‹è·¯å¾„
        train_mode: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
        eval_mode: æ˜¯å¦ä¸ºè¯„ä¼°æ¨¡å¼
        adapter_path: é€‚é…å™¨è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        num_generations: æ¯ä¸ªæ ·æœ¬ç”Ÿæˆçš„ç»“æœæ•°é‡
        entropy_threshold: ç†µé˜ˆå€¼
        save_threshold: å­˜å‚¨é˜ˆå€¼
        max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
        generation_mode: ç”Ÿæˆæ¨¡å¼ï¼ˆ"batch" ä¸€æ¬¡æ€§ç”Ÿæˆ / "loop" å¾ªç¯é€æ¡ç”Ÿæˆï¼‰
        analyzer_name: Lucene åˆ†æå™¨åç§°
        device: è®¾å¤‡
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    """
    from types import SimpleNamespace
    
    print("=" * 60)
    print("æ–‡æœ¬è¯­æ–™ç”Ÿæˆæµæ°´çº¿")
    print("=" * 60)
    
    # æ„å»º args
    args = SimpleNamespace(
        llm=llm,
        linear_output_dim=linear_output_dim,
        resume_log=resume_log,
        resume_encoder=resume_encoder,
        resume_linear=resume_linear,
        resume_lora0=resume_lora0,
        resume_lora1=resume_lora1,
        align1_mode=False, align2_mode=False, test_mode=False, eval_mode=True,
        finetune_mode=False,
        projector=projector
    )
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = TextCorpusGenerator(args)
    
    # æ­¥éª¤ 1: ç”Ÿæˆæ–‡æœ¬è¯­æ–™
    print("\nã€æ­¥éª¤ 1/2ã€‘ç”Ÿæˆæ–‡æœ¬è¯­æ–™")
    print("-" * 60)
    generator.generate_corpus(
        dataset_path=dataset_path,
        output_dir=corpus_output_dir,
        num_generations=num_generations,
        entropy_threshold=entropy_threshold,
        save_threshold=save_threshold,
        max_new_tokens=max_new_tokens,
        generation_mode=generation_mode,
        early_stop_batch=early_stop_batch
    )

    if early_stop_batch is not None:
        return
    
    # æ­¥éª¤ 2: æ„å»º BM25 ç´¢å¼•
    print("\nã€æ­¥éª¤ 2/2ã€‘æ„å»º BM25 ç´¢å¼•")
    print("-" * 60)
    generator.build_bm25_index(
        corpus_path=corpus_output_dir,
        index_dir=index_output_dir,
        analyzer_name=analyzer_name,
        verbose=verbose
    )
    
    print("\n" + "=" * 60)
    print("âœ… æ–‡æœ¬è¯­æ–™ç”Ÿæˆæµæ°´çº¿å®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    import fire
    fire.Fire()
