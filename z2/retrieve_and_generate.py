import torch
import numpy as np
import tempfile
import shutil
import os
from typing import List, Dict, Tuple, Optional, Any
from dataclasses import dataclass
from rank_bm25 import BM25Okapi
import jieba

#TODOï¼šå‚æ•°è°ƒæ•´&å¼€å§‹è·‘

@dataclass
class RAGConfig:
    """RAG ç³»ç»Ÿé…ç½®"""
    # å‘é‡æ£€ç´¢é…ç½®
    vector_index_dir: str  # æµé‡å‘é‡ç´¢å¼•ç›®å½•
    bm25_index_dir: str    # BM25 è¯­æ–™ç´¢å¼•ç›®å½•
    
    # æ£€ç´¢å‚æ•°
    initial_top_k: int = 10        # åˆå§‹æ£€ç´¢ top-k
    iterative_top_k: int = 1       # è¿­ä»£æ£€ç´¢ top-k
    max_iterations: int = 5        # æœ€å¤§è¿­ä»£æ¬¡æ•°
    
    # ç»“æŸæ ‡å¿—
    stop_phrases: List[str] = None  # ç»“æŸæ€§æ–‡æœ¬åˆ—è¡¨
    
    def __post_init__(self):
        if self.stop_phrases is None:
            self.stop_phrases = ["ç»“æœæ˜¯", "æœ€ç»ˆç­”æ¡ˆ", "ç»¼ä¸Šæ‰€è¿°", "å› æ­¤å¯ä»¥åˆ¤æ–­"]

class RAGRetriever:
    """RAG æ£€ç´¢å™¨ï¼Œå°è£…å‘é‡æ£€ç´¢å’Œ BM25 æ£€ç´¢"""
    
    def __init__(
        self,
        embedder_args,
        config: RAGConfig,
        device: str = None
    ):
        """
        åˆå§‹åŒ– RAG æ£€ç´¢å™¨
        
        Args:
            embedder_args: TrafficEmbedder æ¨¡å‹å‚æ•°
            config: RAG é…ç½®
            device: è®¾å¤‡
        """
        self.embedder_args = embedder_args
        self.config = config
        self.device = device or ('cuda' if torch.cuda.is_available() else 'cpu')
        
        self.embedder = None
        self.vector_index = None
        self.vector_metadata = None
    
    def _load_embedder(self):
        """å»¶è¿ŸåŠ è½½ TrafficEmbedder"""
        if self.embedder is None:
            from z2.model import TrafficEmbedder
            print("â³ æ­£åœ¨åŠ è½½ TrafficEmbedder...")
            self.embedder = TrafficEmbedder(self.embedder_args).to(self.device)
            self.embedder.resume(self.embedder_args)
            self.embedder.eval()
            print(f"âœ… TrafficEmbedder å·²åŠ è½½ (è®¾å¤‡: {self.device})")
    
    def _load_vector_index(self):
        """å»¶è¿ŸåŠ è½½å‘é‡ç´¢å¼•"""
        if self.vector_index is None:
            from z2.RAG.vector_utils import load_faiss_index
            print(f"â³ æ­£åœ¨åŠ è½½å‘é‡ç´¢å¼•: {self.config.vector_index_dir}")
            self.vector_index, self.vector_metadata = load_faiss_index(self.config.vector_index_dir)
            print(f"âœ… å‘é‡ç´¢å¼•å·²åŠ è½½ (æ–‡æ¡£æ•°: {self.vector_metadata['num_docs']})")
    
    def unload_embedder(self):
        """å¸è½½ TrafficEmbedder ä»¥é‡Šæ”¾æ˜¾å­˜"""
        if self.embedder is not None:
            del self.embedder
            self.embedder = None
            torch.cuda.empty_cache()
            print("ğŸ—‘ï¸ TrafficEmbedder å·²å¸è½½ï¼Œæ˜¾å­˜å·²é‡Šæ”¾")
    
    def unload_vector_index(self):
        """å¸è½½å‘é‡ç´¢å¼•ä»¥é‡Šæ”¾å†…å­˜"""
        if self.vector_index is not None:
            del self.vector_index
            del self.vector_metadata
            self.vector_index = None
            self.vector_metadata = None
            print("ğŸ—‘ï¸ å‘é‡ç´¢å¼•å·²å¸è½½")
    
    @torch.no_grad()
    def get_traffic_embedding(self, batch_data: Dict) -> np.ndarray:
        """
        è·å–æµé‡æ•°æ®çš„åµŒå…¥å‘é‡
        
        Args:
            batch_data: collate å‡½æ•°è¾“å‡ºçš„æ‰¹æ¬¡æ•°æ®å­—å…¸
        
        Returns:
            numpy æ•°ç»„å½¢å¼çš„åµŒå…¥å‘é‡
        """
        self._load_embedder()
        # ç”ŸæˆåµŒå…¥
        embeddings = self.embedder(**batch_data, normalize=True)
        return embeddings.cpu().numpy()
    
    def search_vector_index(
        self, 
        query_embedding: np.ndarray, 
        k: int = 10
    ) -> List[Tuple[str, float]]:
        """
        åœ¨å‘é‡ç´¢å¼•ä¸­æ£€ç´¢
        
        Args:
            query_embedding: æŸ¥è¯¢å‘é‡
            k: top-k
        
        Returns:
            [(doc_id, score), ...] åˆ—è¡¨
        """
        self._load_vector_index()
        
        assert query_embedding.ndim == 2, f"query_embedding must be 2D, got {query_embedding.ndim}D"
        
        scores, indices = self.vector_index.search(query_embedding, k)
        
        doc_ids = self.vector_metadata['doc_ids']
        results = []
        for idx, score in zip(indices[0], scores[0]):
            if idx == -1:
                break
            results.append((doc_ids[idx], float(score)))
        
        return results
    
    def search_bm25_by_query(
        self, 
        query: str, 
        k: int = 10
    ) -> List[Tuple[str, float, str]]:
        """
        ä½¿ç”¨æ–‡æœ¬æŸ¥è¯¢åœ¨ BM25 ç´¢å¼•ä¸­æ£€ç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: top-k
        
        Returns:
            [(doc_id, score, contents), ...] åˆ—è¡¨
        """
        from z2.RAG.retriever.BM25 import search
        return search(query, self.config.bm25_index_dir, k=k, return_contents=True)
    
    def search_bm25_by_ids(
        self, 
        doc_ids: List[str]
    ) -> List[Dict[str, Any]]:
        """
        æ ¹æ® doc_id åˆ—è¡¨åœ¨ BM25 ç´¢å¼•ä¸­æŸ¥è¯¢å¯¹åº”çš„æ–‡æœ¬è¯­æ–™
        
        æ³¨æ„: pyserini çš„ SimpleSearcher æ”¯æŒé€šè¿‡ doc_id ç›´æ¥è·å–æ–‡æ¡£
        
        Args:
            doc_ids: æ–‡æ¡£ ID åˆ—è¡¨
        
        Returns:
            [{'id': str, 'contents': str, ...}, ...] å­—å…¸åˆ—è¡¨
        """
        import json
        from pyserini.search.lucene import LuceneSearcher
        
        searcher = LuceneSearcher(self.config.bm25_index_dir)
        
        results = []
        for doc_id in doc_ids:
            try:
                doc = searcher.doc(doc_id)
                if doc is not None:
                    raw = doc.lucene_document().get('raw')
                    try:
                        doc_dict = json.loads(raw)
                    except json.JSONDecodeError:
                        print(f"JSON è§£æé”™è¯¯: {raw}")
                        continue
                    results.append(doc_dict)
            except Exception:
                print(f"æ–‡æ¡£ {doc_id} è·å–å¤±è´¥")
                continue
        
        return results


class TempBM25Index:
    """ä¸´æ—¶ BM25 ç´¢å¼•ï¼ˆå†…å­˜ä¸­ï¼‰ï¼Œç”¨äºåœ¨åˆå§‹è¯­æ–™ä¸­æ£€ç´¢"""
    
    def __init__(self, corpus_list: List[Dict]):
        """
        æ„å»ºä¸´æ—¶ BM25 ç´¢å¼•
        
        Args:
            corpus_list: è¯­æ–™åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º {'id': str, 'contents': str, ...}
        """
        self.corpus_list = corpus_list
        self.doc_ids = [c['id'] for c in corpus_list]
        
        # åˆ†è¯ï¼ˆç®€å•æŒ‰ç©ºæ ¼å’Œæ ‡ç‚¹åˆ†å‰²ï¼‰
        self.tokenized_corpus = []
        for c in corpus_list:
            tokens = self._tokenize(c['contents'])
            self.tokenized_corpus.append(tokens)
        
        # æ„å»º BM25 ç´¢å¼•
        if self.tokenized_corpus:
            self.bm25 = BM25Okapi(self.tokenized_corpus)
        else:
            self.bm25 = None
    
    def _tokenize(self, text: str) -> List[str]:
        """ä½¿ç”¨ jieba è¿›è¡Œä¸­æ–‡åˆ†è¯"""
        # ä½¿ç”¨ jieba åˆ†è¯ï¼Œè¿‡æ»¤åœç”¨è¯å’Œç©ºæ ¼
        tokens = list(jieba.cut(text))
        # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²å’Œçº¯ç©ºæ ¼
        tokens = [t.strip() for t in tokens if t.strip()]
        return tokens
    
    def search(self, query: str, k: int = 10) -> List[Dict[str, Any]]:
        """
        åœ¨ä¸´æ—¶ç´¢å¼•ä¸­æ£€ç´¢
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            k: top-k
        
        Returns:
            [{'id': str, 'contents': str, 'score': float, ...}, ...] å­—å…¸åˆ—è¡¨
        """
        if self.bm25 is None or not self.corpus_list:
            return []
        
        query_tokens = self._tokenize(query)
        scores = self.bm25.get_scores(query_tokens)
        
        # è·å– top-k ç´¢å¼•
        top_indices = np.argsort(scores)[::-1][:k]
        
        results = []
        for idx in top_indices:
            if scores[idx] > 0:  # åªè¿”å›æœ‰åˆ†æ•°çš„ç»“æœ
                results.append({
                    'score': float(scores[idx]),
                    **self.corpus_list[idx]
                })
        
        return results


def get_traffic_corelated_corpus(
    retriever: RAGRetriever,
    batch_data: Dict,
    top_k: int = None
) -> List[Dict[str, Any]]:
    """
    åˆå§‹æ£€ç´¢ï¼šè¾“å…¥æµé‡æ•°æ®ï¼Œè·å–ç›¸å…³çš„æ–‡æœ¬è¯­æ–™
    
    æµç¨‹:
    1. ä½¿ç”¨ TrafficEmbedder ç”Ÿæˆæµé‡å‘é‡
    2. åœ¨æµé‡å‘é‡åº“ä¸­æ£€ç´¢ top-k ç›¸å…³å‘é‡
    3. è·å–è¿™äº›å‘é‡çš„ id
    4. åœ¨ BM25 è¯­æ–™åº“ä¸­æŸ¥è¯¢ id å¯¹åº”çš„æ–‡æœ¬è¯­æ–™
    
    Args:
        retriever: RAG æ£€ç´¢å™¨å®ä¾‹
        batch_data: collate å‡½æ•°è¾“å‡ºçš„æ‰¹æ¬¡æ•°æ® (å•ä¸ªæ ·æœ¬æ—¶ batch_size=1)
        top_k: æ£€ç´¢æ•°é‡ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„ initial_top_k
    
    Returns:
        è¯­æ–™åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º {'id': str, 'contents': str, 'score': float}
    """
    if top_k is None:
        top_k = retriever.config.initial_top_k
    
    # 1. ç”Ÿæˆæµé‡åµŒå…¥å‘é‡
    query_embedding = retriever.get_traffic_embedding(batch_data)
    
    # 2. åœ¨å‘é‡åº“ä¸­æ£€ç´¢ top-k
    vector_results = retriever.search_vector_index(query_embedding, k=top_k)
    
    # 3. è·å– id åˆ—è¡¨
    doc_ids = [doc_id for doc_id, _ in vector_results]
    id_to_score = {doc_id: score for doc_id, score in vector_results}
    
    # 4. åœ¨ BM25 è¯­æ–™åº“ä¸­æŸ¥è¯¢å¯¹åº”çš„æ–‡æœ¬
    bm25_results = retriever.search_bm25_by_ids(doc_ids)
    
    # 5. æ·»åŠ  score å­—æ®µ
    for doc_dict in bm25_results:
        doc_dict['score'] = id_to_score.get(doc_dict['id'], 0.0)
    
    return bm25_results

def retrieve_iteratively(
    generator,  # ProposeModel å®ä¾‹
    tokenizer,
    batch_data: Dict,
    initial_corpus: List[Dict[str, Any]],
    config: RAGConfig = None
) -> Dict[str, Any]:
    """
    è¿­ä»£å¼æ£€ç´¢ï¼šäº¤æ›¿è¿›è¡Œæ¨ç†å’Œæ£€ç´¢
    
    è¾“å…¥æ‹¼æ¥é¡ºåºï¼šè¯­æ–™ï¼ˆå‰ï¼‰+ æµé‡ï¼ˆä¸­ï¼‰+ æ¨ç†ï¼ˆåï¼‰
    
    æ¯è½®è¿­ä»£:
    1. æ¨ç†ç¯èŠ‚ï¼šåŸºäºæµé‡ã€é—®é¢˜ã€ä¹‹å‰çš„æ¨ç†å’Œæ£€ç´¢ç»“æœï¼Œç”Ÿæˆä¸€å¥æ¨ç†
    2. æ£€ç´¢ç¯èŠ‚ï¼šä»æ•´ä¸ªè¯­æ–™åº“å’Œä¸´æ—¶è¯­æ–™åº“å„æ£€ç´¢ä¸€æ¡ï¼ˆå…±2æ¡ï¼‰
    3. å°†ç»“æœè¿½åŠ åˆ°å†å²ä¸­
    
    ç»ˆæ­¢æ¡ä»¶:
    - è¿­ä»£æ¬¡æ•°è¶…è¿‡é˜ˆå€¼
    - æ¨ç†ç”Ÿæˆäº†ç»“æŸæ€§æ–‡æœ¬ï¼ˆå¦‚"ç»“æœæ˜¯"ï¼‰
    
    Args:
        retriever: RAG æ£€ç´¢å™¨å®ä¾‹
        generator: ProposeModel å®ä¾‹ï¼ˆç”¨äºç”Ÿæˆæ¨ç†ï¼‰
        tokenizer: tokenizer
        batch_data: collate å‡½æ•°è¾“å‡ºçš„æ‰¹æ¬¡æ•°æ®ï¼ˆbatch_size=1ï¼Œæ— PADå¡«å……ï¼‰
        question: é¢„ç½®çš„é—®é¢˜
        initial_corpus: åˆå§‹æ£€ç´¢å¾—åˆ°çš„è¯­æ–™åˆ—è¡¨
        config: RAG é…ç½®ï¼Œé»˜è®¤ä½¿ç”¨ retriever çš„é…ç½®
    
    Returns:
        {
            'iterations': List[Dict],  # æ¯è½®è¿­ä»£çš„ç»“æœ
            'all_corpus': List[Dict],  # æ‰€æœ‰æ£€ç´¢åˆ°çš„è¯­æ–™ï¼ˆå»é‡ï¼‰
            'reasoning_history': str,  # å®Œæ•´çš„æ¨ç†å†å²
            'stopped_by': str          # ç»ˆæ­¢åŸå› : 'max_iterations' æˆ– 'stop_phrase'
        }
    """
    if config is None:
        raise ValueError("config ä¸èƒ½ä¸º Noneï¼Œè¯·ä¼ å…¥ RAGConfig å®ä¾‹")
    
    iterations = []
    all_corpus = {c['id']: c for c in initial_corpus}  # ç”¨ dict å»é‡
    retrieved_ids = set()  # å·²æ£€ç´¢åˆ°çš„ ID é›†åˆ
    reasoning_history = ""
    stopped_by = "max_iterations"
    
    # æ„å»ºä¸´æ—¶ BM25 ç´¢å¼•ï¼ˆç”¨äºåœ¨åˆå§‹è¯­æ–™ä¸­æ£€ç´¢ï¼‰
    temp_bm25_index = TempBM25Index(initial_corpus)
    
    device = generator.device if hasattr(generator, 'device') and generator.device else 'cuda'
    
    # æ¨æ–­ autocast dtypeï¼ˆå¾ªç¯å¤–åªæ£€æµ‹ä¸€æ¬¡ï¼‰
    model_dtype = next(generator.parameters()).dtype
    use_autocast = model_dtype in (torch.float16, torch.bfloat16)
    
    # è·å–æµé‡æ•°æ®çš„å„éƒ¨åˆ†
    traffic_input_ids = batch_data['input_ids'].to(device)  # (1, traffic_seq_len)
    traffic_attention_mask = batch_data.get('attention_mask')
    traffic_position_ids = batch_data.get('position_ids')
    traffic_payloads = batch_data.get('payloads')
    
    traffic_seq_len = traffic_input_ids.shape[1]
    
    def _get_first_new_result(results: List[Dict], retrieved_ids: set) -> Optional[Dict]:
        """ä»æ£€ç´¢ç»“æœä¸­è·å–ç¬¬ä¸€ä¸ªæœªè¢«æ·»åŠ çš„ç»“æœ"""
        for doc_dict in results:
            if doc_dict['id'] not in retrieved_ids and doc_dict['contents']:
                return doc_dict
        return None
    
    for iteration_idx in range(config.max_iterations):
        # å‡†å¤‡å½“å‰çš„è¯­æ–™æ–‡æœ¬
        corpus_text = "\n".join([
            f"[{i+1}] {c['contents']}" 
            for i, c in enumerate(list(all_corpus.values())[:10])  # é™åˆ¶è¯­æ–™æ•°é‡
        ])
        
        # æ„å»ºå‰ç½® promptï¼ˆè¯­æ–™éƒ¨åˆ†ï¼‰
        system_prompt = """<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œæ“…é•¿é˜…è¯»è¡¨æ ¼å½¢å¼çš„ç½‘ç»œæµé‡å¹¶å¯¹å…¶è¿›è¡Œæ€è€ƒå’Œç†è§£ï¼Œå¹¶èƒ½å¤Ÿå®Œæˆå„ç§é’ˆå¯¹ç½‘ç»œæµé‡çš„é—®é¢˜ã€‚<|im_end|>
<|im_start|>user
"""
        corpus_prompt = f"""æ¥ä¸‹æ¥ä¼šç»™å‡ºä¸€äº›ä¹Ÿæ˜¯é’ˆå¯¹æµé‡ä¿¡æ¯çš„é—®ç­”è¯­æ–™ï¼Œè¿™äº›è¯­æ–™æ‰€åŸºäºçš„æµé‡ä¿¡æ¯å°†ä¸ä¼šè¢«ç»™å‡ºï¼Œä»…æœ‰é—®é¢˜å’Œå›ç­”ä¼šè¢«ç»™å‡ºã€‚å¯ä»¥å‚è€ƒå…¶ä¸­çš„æ¨ç†é€»è¾‘æˆ–æ­¥éª¤ã€‚
ç›¸å…³è¯­æ–™:
{corpus_text}

"""
        # æ„å»ºåç½® promptï¼ˆæ¨ç†éƒ¨åˆ†ï¼‰
        if reasoning_history:
            reasoning_prompt = f"""ä¹‹å‰çš„æ¨ç†:
{reasoning_history}

è¯·ç»§ç»­æ¨ç†ï¼Œè¾“å‡ºä¸‹ä¸€æ­¥çš„åˆ†æï¼ˆä¸€å¥è¯ï¼‰ã€‚å¦‚æœå·²ç»å¯ä»¥å¾—å‡ºç»“è®ºï¼Œè¯·ä»¥"ç»“æœæ˜¯"å¼€å¤´ç»™å‡ºæœ€ç»ˆç­”æ¡ˆã€‚"""
        else:
            reasoning_prompt = """è¯·å¼€å§‹æ¨ç†ï¼Œè¾“å‡ºç¬¬ä¸€æ­¥çš„åˆ†æï¼ˆä¸€å¥è¯ï¼‰ã€‚"""
        
        # æ·»åŠ å¯¹è¯ç»“æŸæ ‡è®°
        reasoning_prompt = reasoning_prompt + "<|im_end|>\n<|im_start|>assistant\n"
        
        # ç¼–ç è¯­æ–™éƒ¨åˆ†å’Œæ¨ç†éƒ¨åˆ†
        # å°† system_prompt å’Œ corpus_prompt åˆå¹¶
        full_corpus_prompt = system_prompt + corpus_prompt
        corpus_encoding = tokenizer(full_corpus_prompt, return_tensors='pt', add_special_tokens=True)
        reasoning_encoding = tokenizer(reasoning_prompt, return_tensors='pt', add_special_tokens=False)
        
        corpus_ids = corpus_encoding['input_ids'].to(device)  # (1, corpus_seq_len)
        reasoning_ids = reasoning_encoding['input_ids'].to(device)  # (1, reasoning_seq_len)
        
        corpus_seq_len = corpus_ids.shape[1]
        reasoning_seq_len = reasoning_ids.shape[1]
        
        # æ‹¼æ¥ input_ids: è¯­æ–™ + æµé‡ + æ¨ç†
        combined_input_ids = torch.cat([corpus_ids, traffic_input_ids, reasoning_ids], dim=1)
        
        # æ„å»º attention_maskï¼ˆå…¨ä¸º1ï¼‰
        total_seq_len = corpus_seq_len + traffic_seq_len + reasoning_seq_len
        combined_attention_mask = torch.ones((1, total_seq_len), dtype=torch.long, device=device)
        
        # æ„å»º position_ids
        # è¯­æ–™éƒ¨åˆ†: 0, 1, 2, ..., corpus_seq_len-1
        # æµé‡éƒ¨åˆ†: åœ¨åŸ position_ids åŸºç¡€ä¸ŠåŠ ä¸Š corpus_seq_len çš„åç§»
        # æ¨ç†éƒ¨åˆ†: corpus_seq_len + traffic_seq_len, ..., total_seq_len-1
        corpus_position_ids = torch.arange(corpus_seq_len, dtype=torch.long, device=device).unsqueeze(0).expand(3, -1, -1)
        adjusted_traffic_position_ids = traffic_position_ids.to(device) + corpus_seq_len
        reasoning_position_ids = torch.arange(
            corpus_seq_len + traffic_seq_len, total_seq_len,
            dtype=torch.long, device=device
        ).unsqueeze(0).expand(3, -1, -1)
        
        combined_position_ids = torch.cat([
            corpus_position_ids, 
            adjusted_traffic_position_ids, 
            reasoning_position_ids
        ], dim=2)
        
        # ç”Ÿæˆæ¨ç†
        with torch.no_grad(), torch.amp.autocast('cuda', enabled=use_autocast, dtype=model_dtype if use_autocast else None):
            output_ids = generator.generate(
                input_ids=combined_input_ids,
                attention_mask=combined_attention_mask,
                position_ids=combined_position_ids,
                payloads=traffic_payloads,
                max_new_tokens=100,
                do_sample=True,
                temperature=0.7,
                top_p=0.9,
                pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id
            )
        
        new_reasoning = tokenizer.decode(
            output_ids[0][combined_input_ids.shape[1]:], 
            skip_special_tokens=True
        ).strip()

        # full_output = tokenizer.decode(
        #     output_ids[0], 
        #     skip_special_tokens=True
        # ).strip()
        # print("Full output:", full_output)
        
        # æ›´æ–°æ¨ç†å†å²
        if reasoning_history:
            reasoning_history += f"\n{new_reasoning}"
        else:
            reasoning_history = new_reasoning
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç»“æŸæ€§æ–‡æœ¬
        should_stop = False
        for stop_phrase in config.stop_phrases:
            if stop_phrase in new_reasoning:
                should_stop = True
                stopped_by = f"stop_phrase: {stop_phrase}"
                break
        
        # 2. æ£€ç´¢ç¯èŠ‚ï¼ˆå¦‚æœæ²¡æœ‰ç»“æŸï¼‰
        # æ¯ä¸ªæ£€ç´¢ç¯èŠ‚åº”å¾—å‡º2æ¡ç»“æœï¼š1æ¡æ¥è‡ªæ•´ä¸ªè¯­æ–™åº“ï¼Œ1æ¡æ¥è‡ªä¸´æ—¶è¯­æ–™åº“
        new_corpus = []
        if not should_stop:
            # 2.1 ä»æ•´ä¸ª BM25 è¯­æ–™åº“æ£€ç´¢
            from z2.RAG.retriever.BM25 import search as bm25_search
            bm25_results = bm25_search(
                new_reasoning, 
                config.bm25_index_dir,
                k=config.iterative_top_k * 5,  # å¤šæ£€ç´¢ä¸€äº›ä»¥ä¾¿æ‰¾åˆ°æœªæ·»åŠ çš„
                return_contents=True
            )
            result_from_bm25 = _get_first_new_result(bm25_results, retrieved_ids)
            if result_from_bm25:
                result_from_bm25['source'] = 'bm25'
                new_corpus.append(result_from_bm25)
                retrieved_ids.add(result_from_bm25['id'])
                all_corpus[result_from_bm25['id']] = result_from_bm25
            
            # 2.2 ä»ä¸´æ—¶è¯­æ–™åº“ï¼ˆåˆå§‹è¯­æ–™ï¼‰æ£€ç´¢
            temp_results = temp_bm25_index.search(
                new_reasoning, 
                k=config.iterative_top_k * 5
            )
            result_from_temp = _get_first_new_result(temp_results, retrieved_ids)
            if result_from_temp:
                result_from_temp['source'] = 'initial'
                new_corpus.append(result_from_temp)
                retrieved_ids.add(result_from_temp['id'])
                all_corpus[result_from_temp['id']] = result_from_temp
        
        # è®°å½•æœ¬è½®è¿­ä»£
        iterations.append({
            'iteration': iteration_idx + 1,
            'reasoning': new_reasoning,
            'new_corpus': new_corpus,
            'corpus_count': len(new_corpus)
        })
        
        if should_stop:
            break
        
        # æ¸…ç†æœ¬è½®è¿­ä»£çš„ä¸­é—´å˜é‡
        del combined_input_ids, combined_attention_mask, combined_position_ids
        del corpus_ids, reasoning_ids, output_ids
        torch.cuda.empty_cache()
    
    return {
        'iterations': iterations,
        'all_corpus': list(all_corpus.values()),
        'reasoning_history': reasoning_history,
        'stopped_by': stopped_by
    }


def generate_response(
    generator,  # ProposeModel å®ä¾‹
    tokenizer,
    batch_data: Dict,
    corpus_list: List[Dict[str, Any]],
    max_new_tokens: int = 512,
    think_first: bool = True,
    have_corpus: bool = True
) -> str:
    """
    æœ€ç»ˆç”Ÿæˆï¼šç»„åˆæµé‡ã€é—®é¢˜ã€æ£€ç´¢ç»“æœï¼Œä½¿ç”¨ ProposeModel ç”Ÿæˆç­”æ¡ˆ
    
    è¾“å…¥æ‹¼æ¥é¡ºåºï¼šè¯­æ–™ï¼ˆå‰ï¼‰+ æµé‡ï¼ˆä¸­ï¼‰+ ç”Ÿæˆæç¤ºï¼ˆåï¼‰
    
    Args:
        generator: ProposeModel å®ä¾‹
        tokenizer: tokenizer
        batch_data: collate å‡½æ•°è¾“å‡ºçš„æ‰¹æ¬¡æ•°æ®ï¼ˆbatch_size=1ï¼Œæ— PADå¡«å……ï¼‰
        question: é¢„ç½®çš„é—®é¢˜
        corpus_list: æ£€ç´¢åˆ°çš„è¯­æ–™åˆ—è¡¨
        max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
    
    Returns:
        ç”Ÿæˆçš„æœ€ç»ˆç­”æ¡ˆæ–‡æœ¬
    """
    device = generator.device if hasattr(generator, 'device') and generator.device else 'cuda'
    
    # æ„å»ºè¯­æ–™æ–‡æœ¬
    corpus_text = "\n".join([
        f"[{i+1}] {c['contents']}" 
        for i, c in enumerate(corpus_list[:10])  # é™åˆ¶è¯­æ–™æ•°é‡
    ])
    
    # æ„å»ºå‰ç½® promptï¼ˆè¯­æ–™éƒ¨åˆ†ï¼‰
    system_prompt = """<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œæ“…é•¿é˜…è¯»è¡¨æ ¼å½¢å¼çš„ç½‘ç»œæµé‡å¹¶å¯¹å…¶è¿›è¡Œæ€è€ƒå’Œç†è§£ï¼Œå¹¶èƒ½å¤Ÿå®Œæˆå„ç§é’ˆå¯¹ç½‘ç»œæµé‡çš„é—®é¢˜ã€‚<|im_end|>
<|im_start|>user
"""
    corpus_prompt = f"""æ¥ä¸‹æ¥ä¼šç»™å‡ºä¸€äº›ä¹Ÿæ˜¯é’ˆå¯¹æµé‡ä¿¡æ¯çš„é—®ç­”è¯­æ–™ï¼Œè¿™äº›è¯­æ–™æ‰€åŸºäºçš„æµé‡ä¿¡æ¯å°†ä¸ä¼šè¢«ç»™å‡ºï¼Œä»…æœ‰é—®é¢˜å’Œå›ç­”ä¼šè¢«ç»™å‡ºã€‚å¯ä»¥å‚è€ƒå…¶ä¸­çš„æ¨ç†é€»è¾‘æˆ–æ­¥éª¤ã€‚
ç›¸å…³è¯­æ–™:
{corpus_text}

"""
    # æ„å»ºåç½® promptï¼ˆç”Ÿæˆæç¤ºéƒ¨åˆ†ï¼‰
    if think_first:
        generation_prompt = """è¯·æ³¨æ„ï¼Œç»™å‡ºçš„æµé‡è¡¨æ ¼ä¸­ï¼Œipå’Œç«¯å£å‡ç»è¿‡äº†éšæœºåŒ–å¤„ç†ï¼Œå› æ­¤è¯·ä¸è¦æ ¹æ®è¿™äº›å­—æ®µçš„å–å€¼èŒƒå›´æ¥åˆ¤æ–­ã€‚
è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºç»“æœï¼š
æ¨ç†ï¼š[æ¨ç†è¿‡ç¨‹ï¼Œä¸è¶…è¿‡300å­—]
ç±»åˆ«ï¼š[åˆ†ç±»æ ‡ç­¾]
<|im_end|>
<|im_start|>assistant
"""
    else:
        generation_prompt = """è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹æ ¼å¼è¾“å‡ºç»“æœï¼š
ç±»åˆ«ï¼š[åˆ†ç±»æ ‡ç­¾]
è§£é‡Šï¼š[è§£é‡Šæ–‡æœ¬ï¼Œä¸è¶…è¿‡300å­—]
<|im_end|>
<|im_start|>assistant
"""

    # è·å–æµé‡æ•°æ®çš„å„éƒ¨åˆ†
    traffic_input_ids = batch_data['input_ids'].to(device)  # (1, traffic_seq_len)
    traffic_attention_mask = batch_data.get('attention_mask')
    traffic_position_ids = batch_data.get('position_ids')
    traffic_payloads = batch_data.get('payloads')
    
    traffic_seq_len = traffic_input_ids.shape[1]
    
    # ç¼–ç è¯­æ–™éƒ¨åˆ†å’Œç”Ÿæˆæç¤ºéƒ¨åˆ†
    # å°† system_prompt å’Œ corpus_prompt åˆå¹¶
    full_corpus_prompt = system_prompt + corpus_prompt if have_corpus else system_prompt
    corpus_encoding = tokenizer(full_corpus_prompt, return_tensors='pt', add_special_tokens=True)
    generation_encoding = tokenizer(generation_prompt, return_tensors='pt', add_special_tokens=False)
    
    corpus_ids = corpus_encoding['input_ids'].to(device)  # (1, corpus_seq_len)
    generation_ids = generation_encoding['input_ids'].to(device)  # (1, generation_seq_len)
    
    corpus_seq_len = corpus_ids.shape[1]
    generation_seq_len = generation_ids.shape[1]
    
    # æ‹¼æ¥ input_ids: è¯­æ–™ + æµé‡ + ç”Ÿæˆæç¤º
    combined_input_ids = torch.cat([corpus_ids, traffic_input_ids, generation_ids], dim=1)
    
    # æ„å»º attention_maskï¼ˆå…¨ä¸º1ï¼‰
    total_seq_len = corpus_seq_len + traffic_seq_len + generation_seq_len
    combined_attention_mask = torch.ones((1, total_seq_len), dtype=torch.long, device=device)
    
    # æ„å»º position_ids
    # è¯­æ–™éƒ¨åˆ†: 0, 1, 2, ..., corpus_seq_len-1
    # æµé‡éƒ¨åˆ†: åœ¨åŸ position_ids åŸºç¡€ä¸ŠåŠ ä¸Š corpus_seq_len çš„åç§»
    # ç”Ÿæˆæç¤ºéƒ¨åˆ†: corpus_seq_len + traffic_seq_len, ..., total_seq_len-1
    corpus_position_ids = torch.arange(corpus_seq_len, dtype=torch.long, device=device).unsqueeze(0).expand(3, -1, -1)
    adjusted_traffic_position_ids = traffic_position_ids.to(device) + corpus_seq_len
    generation_position_ids = torch.arange(
        corpus_seq_len + traffic_seq_len, total_seq_len,
        dtype=torch.long, device=device
    ).unsqueeze(0).expand(3, -1, -1)
    # print(corpus_position_ids.shape)
    # print(adjusted_traffic_position_ids.shape)
    # print(generation_position_ids.shape)
    
    combined_position_ids = torch.cat([
        corpus_position_ids, 
        adjusted_traffic_position_ids, 
        generation_position_ids
    ], dim=2)
    
    # ç”Ÿæˆ
    # æ ¹æ®æ¨¡å‹å‚æ•°è‡ªåŠ¨æ¨æ–­ autocast dtype
    model_dtype = next(generator.parameters()).dtype
    use_autocast = model_dtype in (torch.float16, torch.bfloat16)
    with torch.no_grad(), torch.amp.autocast('cuda', enabled=use_autocast, dtype=model_dtype if use_autocast else None):
        output_ids = generator.generate(
            input_ids=combined_input_ids,
            attention_mask=combined_attention_mask,
            position_ids=combined_position_ids,
            payloads=traffic_payloads,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
            pad_token_id=tokenizer.pad_token_id or tokenizer.eos_token_id
        )
    
    # è§£ç ç”Ÿæˆçš„æ–‡æœ¬ï¼ˆåªå–æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼‰
    response = tokenizer.decode(
        output_ids[0][combined_input_ids.shape[1]:],
        skip_special_tokens=True
    )

    # return response.strip()

    original = tokenizer.decode(
        output_ids[0],
        skip_special_tokens=True
    )
    
    # æ¸…ç†ä¸­é—´å˜é‡é‡Šæ”¾æ˜¾å­˜
    del combined_input_ids, combined_attention_mask, combined_position_ids
    del corpus_ids, generation_ids, output_ids
    torch.cuda.empty_cache()
    
    return response.strip(), original.strip()


def run_rag_pipeline(
    retriever: RAGRetriever,
    generator,  # ProposeModel å®ä¾‹
    tokenizer,
    batch_data: Dict,
    question: str,
    enable_iterative: bool = True,
    max_new_tokens: int = 512,
    unload_embedder_after_initial: bool = True
) -> Dict[str, Any]:
    """
    è¿è¡Œå®Œæ•´çš„ RAG æµç¨‹
    
    Args:
        retriever: RAG æ£€ç´¢å™¨å®ä¾‹
        generator: ProposeModel å®ä¾‹
        tokenizer: tokenizer
        batch_data: collate å‡½æ•°è¾“å‡ºçš„æ‰¹æ¬¡æ•°æ®
        question: é—®é¢˜
        enable_iterative: æ˜¯å¦å¯ç”¨è¿­ä»£å¼æ£€ç´¢
        max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
        unload_embedder_after_initial: åˆå§‹æ£€ç´¢å®Œæˆåæ˜¯å¦å¸è½½ embedder ä»¥èŠ‚çœæ˜¾å­˜
    
    Returns:
        {
            'initial_corpus': List[Dict],    # åˆå§‹æ£€ç´¢ç»“æœ
            'iterative_result': Dict,        # è¿­ä»£æ£€ç´¢ç»“æœï¼ˆå¦‚æœå¯ç”¨ï¼‰
            'final_corpus': List[Dict],      # æœ€ç»ˆä½¿ç”¨çš„è¯­æ–™
            'response': str                  # æœ€ç»ˆç”Ÿæˆçš„ç­”æ¡ˆ
        }
    """
    # 1. åˆå§‹æ£€ç´¢
    print("ğŸ” æ‰§è¡Œåˆå§‹æ£€ç´¢...")
    initial_corpus = get_traffic_corelated_corpus(retriever, batch_data)
    print(f"   - æ£€ç´¢åˆ° {len(initial_corpus)} ä¸ªç›¸å…³è¯­æ–™")
    
    # åˆå§‹æ£€ç´¢å®Œæˆåå¸è½½ embedder ä»¥èŠ‚çœæ˜¾å­˜
    if unload_embedder_after_initial:
        retriever.unload_embedder()
        retriever.unload_vector_index()
    
    # 2. è¿­ä»£å¼æ£€ç´¢ï¼ˆå¯é€‰ï¼‰
    iterative_result = None
    final_corpus = initial_corpus
    reasoning_history = None
    
    if enable_iterative:
        print("ğŸ”„ æ‰§è¡Œè¿­ä»£å¼æ£€ç´¢...")
        iterative_result = retrieve_iteratively(
            retriever=retriever,
            generator=generator,
            tokenizer=tokenizer,
            batch_data=batch_data,
            question=question,
            initial_corpus=initial_corpus
        )
        final_corpus = iterative_result['all_corpus']
        reasoning_history = iterative_result['reasoning_history']
        print(f"   - è¿­ä»£ {len(iterative_result['iterations'])} è½®")
        print(f"   - æœ€ç»ˆè¯­æ–™æ•°: {len(final_corpus)}")
        print(f"   - ç»ˆæ­¢åŸå› : {iterative_result['stopped_by']}")
    
    # 3. æœ€ç»ˆç”Ÿæˆ
    print("âœï¸ ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ...")
    response = generate_response(
        generator=generator,
        tokenizer=tokenizer,
        batch_data=batch_data,
        question=question,
        corpus_list=final_corpus,
        max_new_tokens=max_new_tokens
    )
    
    print("âœ… RAG æµç¨‹å®Œæˆ")
    
    return {
        'initial_corpus': initial_corpus,
        'iterative_result': iterative_result,
        'final_corpus': final_corpus,
        'response': response
    }

if __name__ == "__main__":
    import fire
    fire.Fire()
