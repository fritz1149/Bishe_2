"""
æ¨ç†æµ‹è¯•è„šæœ¬

åŒ…å«ä¸¤ç§æ¨ç†æ¨¡å¼ï¼š
1. å¤æ‚æ¨ç†ï¼ˆRAGæ¨¡å¼ï¼‰ï¼šè°ƒç”¨ run_rag_pipelineï¼ŒåŒ…å«åˆå§‹æ£€ç´¢ã€è¿­ä»£æ£€ç´¢ã€æœ€ç»ˆç”Ÿæˆ
2. ç®€å•æ¨ç†ï¼šç›´æ¥ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œæ— éœ€å¤æ‚çš„æ£€ç´¢æµç¨‹

åŠŸèƒ½ï¼š
- ç¯å¢ƒåˆå§‹åŒ–ï¼ˆå•æ˜¾å¡æ¨¡å¼/æ¨¡å‹å¹¶è¡Œæ¨¡å¼ï¼‰
- æƒé‡åŠ è½½
- æ•°æ®é›†åˆå§‹åŒ–
- æ¨ç†è¿›è¡Œ
- æŒ‡æ ‡è®¡ç®—
"""

import os
import sys
import json
import time
import torch
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Optional, Union
from types import SimpleNamespace
from dataclasses import dataclass, field
from tqdm import tqdm
from collections import defaultdict

from torch.utils.data import DataLoader
from transformers import AutoTokenizer
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report


@dataclass
class InferenceConfig:
    """æ¨ç†é…ç½®"""
    # æ¨¡å‹é…ç½®
    llm_retriever: str = "Qwen3-VL-Embedding-2B"
    llm_generator: str = "Qwen3-VL-8B-Instruct"
    projector: str = "linear"
    linear_output_dim_retriever: int = 2048
    linear_output_dim_generator: int = 4096
    projector_arch: str = "512-512-512"
    
    # é€šç”¨æƒé‡è·¯å¾„
    resume_log: bool = True
    resume_encoder: str = None
    # ProposeModel æƒé‡è·¯å¾„
    resume_linear_0: str = None
    resume_lora0_0: str = None
    # TrafficEmbedder æƒé‡è·¯å¾„ï¼ˆç”¨äºRAGæ¨¡å¼ï¼‰
    resume_linear_1: str = None
    resume_lora0_1: str = None
    
    # æ•°æ®é›†é…ç½®
    dataset_path: str = None
    batch_size: int = 1  # æ¨ç†æ—¶é€šå¸¸ä½¿ç”¨ batch_size=1
    
    # RAG é…ç½®ï¼ˆå¤æ‚æ¨ç†æ¨¡å¼ï¼‰
    # TODOï¼šé€ä¼ 
    vector_index_dir: str = None
    bm25_index_dir: str = None
    initial_top_k: int = 10
    iterative_top_k: int = 1
    max_iterations: int = 5
    enable_iterative: bool = True
    
    # ç”Ÿæˆé…ç½®
    # TODOï¼šé€ä¼ ï¼›æ·»åŠ å‚æ•°ç§ç±»
    max_new_tokens: int = 512
    temperature: float = 0.8
    top_p: float = 0.8
    top_k: float = 20
    presence_penalty: float = 1.5
    do_sample: bool = True
    think_first: bool = True
    
    # è®¾å¤‡é…ç½®
    device: str = None  # None è¡¨ç¤ºè‡ªåŠ¨é€‰æ‹©
    parallel_mode: bool = False  # æ˜¯å¦ä½¿ç”¨æ¨¡å‹å¹¶è¡Œ
    inference_dtype: str = None  # æ¨ç†ç²¾åº¦: "bf16", "fp16", None(fp32)
    
    # è¾“å‡ºé…ç½®
    output_dir: str = "./inference_results"
    save_predictions: bool = True
    verbose: bool = True

    early_stop: Optional[int] = None


class InferenceEngine:
    """æ¨ç†å¼•æ“"""
    
    def __init__(self, config: InferenceConfig):
        self.config = config
        self.device = self._init_device()
        self.tokenizer = None
        self.generator = None
        self.retriever = None
        
    def _init_device(self) -> str:
        """åˆå§‹åŒ–è®¾å¤‡"""
        if self.config.device:
            return self.config.device
        
        if torch.cuda.is_available():
            if self.config.parallel_mode and torch.cuda.device_count() > 1:
                print(f"ğŸ–¥ï¸ ä½¿ç”¨æ¨¡å‹å¹¶è¡Œæ¨¡å¼ (å¯ç”¨GPU: {torch.cuda.device_count()})")
                return "cuda:0"  # ä¸»è®¾å¤‡
            else:
                print(f"ğŸ–¥ï¸ ä½¿ç”¨å•æ˜¾å¡æ¨¡å¼ (GPU: {torch.cuda.get_device_name(0)})")
                return "cuda:0"
        else:
            print("âš ï¸ CUDA ä¸å¯ç”¨ï¼Œä½¿ç”¨ CPU")
            return "cpu"
    
    def _get_device_map(self) -> Optional[Dict]:
        """è·å–æ¨¡å‹å¹¶è¡Œçš„è®¾å¤‡æ˜ å°„"""
        if not self.config.parallel_mode or torch.cuda.device_count() <= 1:
            return None
        
        num_gpus = torch.cuda.device_count()
        # å‡è®¾æ¨¡å‹æœ‰36å±‚ï¼Œå‡åŒ€åˆ†é…åˆ°å¤šä¸ªGPU
        layers_per_gpu = 36 // num_gpus
        
        device_map = {
            "base_model.model.model.language_model.embed_tokens": "cuda:0",
            "base_model.model.model.language_model.norm": f"cuda:{num_gpus-1}",
            "base_model.model.lm_head": f"cuda:{num_gpus-1}",
            "base_model.model.model.visual": "cuda:0",
            "base_model.model.model.language_model.rotary_emb": "cuda:0",
        }
        
        for i in range(36):
            gpu_id = min(i // layers_per_gpu, num_gpus - 1)
            device_map[f"base_model.model.model.language_model.layers.{i}"] = f"cuda:{gpu_id}"
        
        return device_map
    
    def load_tokenizer(self):
        """åŠ è½½ tokenizer"""
        if self.tokenizer is None:
            print(f"â³ æ­£åœ¨åŠ è½½ Tokenizer: {self.config.llm_generator}")
            self.tokenizer = AutoTokenizer.from_pretrained(self.config.llm_generator)
            print("âœ… Tokenizer å·²åŠ è½½")
        return self.tokenizer
    
    def load_generator(self):
        """åŠ è½½ç”Ÿæˆæ¨¡å‹ (ProposeModel)"""
        if self.generator is not None:
            return self.generator
        
        print(f"â³ æ­£åœ¨åŠ è½½ ProposeModel...")
        from z1.model import ProposeModel
        
        # è§£ææ¨ç†ç²¾åº¦
        dtype_map = {'bf16': torch.bfloat16, 'fp16': torch.float16}
        torch_dtype = dtype_map.get(self.config.inference_dtype, None)
        if torch_dtype:
            print(f"ğŸ“ ä½¿ç”¨ {self.config.inference_dtype} ç²¾åº¦åŠ è½½æ¨¡å‹")
        else:
            print("ğŸ“ ä½¿ç”¨é»˜è®¤ç²¾åº¦ (fp32) åŠ è½½æ¨¡å‹")
        
        args = SimpleNamespace(
            llm=self.config.llm_generator,
            projector=self.config.projector,
            linear_output_dim=self.config.linear_output_dim_generator,
            resume_log=self.config.resume_log,
            resume_encoder=self.config.resume_encoder,
            resume_linear=self.config.resume_linear_0,
            resume_lora0=self.config.resume_lora0_0,
            test_mode=False,
            align1_mode=False,
            align2_mode=False,
            finetune_mode=False,
            eval_mode=True,
            torch_dtype=torch_dtype
        )
        
        self.generator = ProposeModel(args)
        self.generator.resume(args)
        
        # æ¨¡å‹å¹¶è¡Œæˆ–å•å¡
        if self.config.parallel_mode:
            device_map = self._get_device_map()
            if device_map:
                self.generator.dispatch(device_map)
        else:
            self.generator = self.generator.to(self.device)
        
        self.generator.device = torch.device(self.device)
        self.generator.eval()
        print(f"âœ… ProposeModel å·²åŠ è½½ (è®¾å¤‡: {self.device})")
        
        return self.generator
    
    def load_retriever(self):
        """åŠ è½½ RAG æ£€ç´¢å™¨ï¼ˆç”¨äºå¤æ‚æ¨ç†æ¨¡å¼ï¼‰"""
        if self.retriever is not None:
            return self.retriever
        
        if not self.config.vector_index_dir or not self.config.bm25_index_dir:
            raise ValueError("å¤æ‚æ¨ç†æ¨¡å¼éœ€è¦æŒ‡å®š vector_index_dir å’Œ bm25_index_dir")
        
        from z2.retrieve_and_generate import RAGRetriever, RAGConfig
        
        # æ„å»º embedder_args
        embedder_args = SimpleNamespace(
            llm=self.config.llm_retriever,
            projector=self.config.projector,
            linear_output_dim=self.config.linear_output_dim_retriever,
            resume_log=self.config.resume_log,
            resume_encoder=self.config.resume_encoder,
            resume_linear=self.config.resume_linear_1,
            resume_lora0=self.config.resume_lora0_1,
            eval_mode=True,
            train_mode=False
        )
        
        # åˆ›å»º RAG é…ç½®
        rag_config = RAGConfig(
            vector_index_dir=self.config.vector_index_dir,
            bm25_index_dir=self.config.bm25_index_dir,
            initial_top_k=self.config.initial_top_k,
            iterative_top_k=self.config.iterative_top_k,
            max_iterations=self.config.max_iterations
        )
        
        self.retriever = RAGRetriever(
            embedder_args=embedder_args,
            config=rag_config,
            device=self.device
        )
        
        print("âœ… RAG æ£€ç´¢å™¨å·²åˆå§‹åŒ–")
        return self.retriever
    
    def unload_retriever(self):
        """å¸è½½æ£€ç´¢å™¨ä»¥é‡Šæ”¾æ˜¾å­˜"""
        if self.retriever is not None:
            self.retriever.unload_embedder()
            self.retriever.unload_vector_index()
            self.retriever = None
            torch.cuda.empty_cache()
            print("ğŸ—‘ï¸ RAG æ£€ç´¢å™¨å·²å¸è½½")
    
    def load_dataset(self) -> DataLoader:
        """åŠ è½½æ•°æ®é›†"""
        if not self.config.dataset_path:
            raise ValueError("éœ€è¦æŒ‡å®š dataset_path")
        
        from dataset import CustomDataset, collate_LLMDataset_leftpadding
        
        print(f"ğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®é›†: {self.config.dataset_path}")
        dataset = CustomDataset(self.config.dataset_path)
        
        dataloader = DataLoader(
            dataset,
            batch_size=self.config.batch_size,
            shuffle=False,
            collate_fn=lambda batch: collate_LLMDataset_leftpadding(batch, keep_labels=False),
            num_workers=0
        )
        
        print(f"âœ… æ•°æ®é›†å·²åŠ è½½ (æ ·æœ¬æ•°: {len(dataset)})")
        return dataloader

class SimpleInference:
    """ç®€å•æ¨ç†æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†"""
    
    def __init__(self, engine: InferenceEngine):
        self.engine = engine
        self.config = engine.config
    
    @torch.no_grad()
    def run(self, dataloader: DataLoader) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œç®€å•æ¨ç†
        
        Args:
            dataloader: æ•°æ®åŠ è½½å™¨
        
        Returns:
            é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        self.engine.load_tokenizer()
        self.engine.load_generator()
        
        results = []
        device = self.engine.device

        from z2.retrieve_and_generate import generate_response
        
        print("\n" + "=" * 60)
        print("ğŸš€ å¼€å§‹ç®€å•æ¨ç†")
        print("=" * 60)
        
        for batch_idx, data in enumerate(tqdm(dataloader, desc="æ¨ç†è¿›åº¦")):
            if self.config.early_stop is not None and batch_idx >= self.config.early_stop:
                break

            batch_data, label = data
            
            generated_text = generate_response(
                generator=self.engine.generator,
                tokenizer=self.engine.tokenizer,
                batch_data=batch_data,
                max_new_tokens=self.config.max_new_tokens,
                think_first=self.config.think_first,
                have_corpus=False,
                corpus_list=[]
            )
                
            result = {
                'batch_idx': batch_idx,
                'sample_idx': batch_idx * self.config.batch_size,
                'generated_text': generated_text,
                'label': label,
            }
            results.append(result)
            
            if self.config.verbose and batch_idx < 3:
                print(f"\n--- æ ·æœ¬ {result['sample_idx']} ---")
                print(f"ç”Ÿæˆ: {generated_text[:200]}...")
        
        print(f"\nâœ… ç®€å•æ¨ç†å®Œæˆï¼Œå…± {len(results)} ä¸ªæ ·æœ¬")
        return results

class ComplexInference:
    """
    å¤æ‚æ¨ç†æ¨¡å¼ï¼šä½¿ç”¨ RAG æµç¨‹è¿›è¡Œæ¨ç†
    
    æµç¨‹åˆ†ä¸ºä¸¤ä¸ªé˜¶æ®µï¼š
    1. åˆå§‹æ£€ç´¢é˜¶æ®µï¼šéå†æ‰€æœ‰æ ·æœ¬ï¼Œä½¿ç”¨ TrafficEmbedder æ‰§è¡Œå‘é‡æ£€ç´¢ï¼Œè®°å½•æ¯ä¸ªæ ·æœ¬çš„åˆå§‹æ£€ç´¢ç»“æœ
    2. åç»­æ¨ç†é˜¶æ®µï¼šå¸è½½ embedder åï¼Œé’ˆå¯¹æ¯ä¸ªæ ·æœ¬åŠ¨æ€åˆ›å»ºä¸´æ—¶ç´¢å¼•ï¼Œæ‰§è¡Œè¿­ä»£æ£€ç´¢å’Œæœ€ç»ˆç”Ÿæˆ
    """
    
    def __init__(self, engine: InferenceEngine):
        self.engine = engine
        self.config = engine.config
    
    @torch.no_grad()
    def _phase1_initial_retrieval(self, dataloader: DataLoader) -> List[Dict[str, Any]]:
        """
        é˜¶æ®µ1ï¼šåˆå§‹æ£€ç´¢
        
        éå†æ‰€æœ‰æ ·æœ¬ï¼Œä½¿ç”¨ TrafficEmbedder æ‰§è¡Œå‘é‡æ£€ç´¢ï¼Œè®°å½•æ¯ä¸ªæ ·æœ¬çš„åˆå§‹æ£€ç´¢ç»“æœID
        
        Returns:
            æ¯ä¸ªæ ·æœ¬çš„åˆå§‹æ£€ç´¢ä¿¡æ¯åˆ—è¡¨ï¼ŒåŒ…å« batch_data, question, labels, initial_corpus_ids
        """
        from z2.retrieve_and_generate import get_traffic_corelated_corpus
        
        self.engine.load_tokenizer()
        self.engine.load_retriever()
        
        initial_retrieval_results = []
        
        print("\n" + "=" * 60)
        print("ğŸ” é˜¶æ®µ1ï¼šåˆå§‹æ£€ç´¢ï¼ˆä½¿ç”¨ TrafficEmbedderï¼‰")
        print("=" * 60)
        
        for batch_idx, batch_data in enumerate(tqdm(dataloader, desc="åˆå§‹æ£€ç´¢")):
            if self.config.early_stop is not None and batch_idx >= self.config.early_stop:
                break

            batch_data, label = batch_data
            
            try:
                # æ‰§è¡Œåˆå§‹æ£€ç´¢
                initial_corpus = get_traffic_corelated_corpus(
                    self.engine.retriever, 
                    batch_data,
                    top_k=self.config.initial_top_k
                )
                
                # è®°å½•åˆå§‹æ£€ç´¢ç»“æœçš„ ID å’Œå†…å®¹
                initial_corpus_ids = [c['id'] for c in initial_corpus]
                
                initial_retrieval_results.append({
                    'batch_idx': batch_idx,
                    'batch_data': batch_data,
                    'label': label,
                    'initial_corpus': initial_corpus,
                    'initial_corpus_ids': initial_corpus_ids
                })
                
                if self.config.verbose and batch_idx < 3:
                    print(f"   æ ·æœ¬ {batch_idx}: æ£€ç´¢åˆ° {len(initial_corpus)} ä¸ªè¯­æ–™")
                    
            except Exception as e:
                import traceback
                print(f"âš ï¸ æ ·æœ¬ {batch_idx} åˆå§‹æ£€ç´¢å¤±è´¥: {e}")
                traceback.print_exc()
                initial_retrieval_results.append({
                    'batch_idx': batch_idx,
                    'batch_data': batch_data,
                    'label': label,
                    'initial_corpus': [],
                    'initial_corpus_ids': [],
                    'error': str(e)
                })
        
        print(f"\nâœ… åˆå§‹æ£€ç´¢å®Œæˆï¼Œå…±å¤„ç† {len(initial_retrieval_results)} ä¸ªæ ·æœ¬")
        return initial_retrieval_results
    
    @torch.no_grad()
    def _phase2_iterative_and_generate(
        self, 
        initial_retrieval_results: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """
        é˜¶æ®µ2ï¼šè¿­ä»£æ£€ç´¢ä¸æœ€ç»ˆç”Ÿæˆ
        
        é’ˆå¯¹æ¯ä¸ªæ ·æœ¬ï¼Œæ ¹æ®å…¶åˆå§‹æ£€ç´¢ç»“æœåŠ¨æ€åˆ›å»ºä¸´æ—¶ BM25 ç´¢å¼•ï¼Œæ‰§è¡Œè¿­ä»£æ£€ç´¢å’Œæœ€ç»ˆç”Ÿæˆ
        
        Args:
            initial_retrieval_results: é˜¶æ®µ1çš„åˆå§‹æ£€ç´¢ç»“æœ
        
        Returns:
            æœ€ç»ˆæ¨ç†ç»“æœåˆ—è¡¨
        """
        from z2.retrieve_and_generate import (
            TempBM25Index, 
            retrieve_iteratively, 
            generate_response,
            RAGConfig
        )
        
        self.engine.load_generator()
        
        results = []
        
        print("\n" + "=" * 60)
        print("ğŸš€ é˜¶æ®µ2ï¼šè¿­ä»£æ£€ç´¢ä¸æœ€ç»ˆç”Ÿæˆï¼ˆä½¿ç”¨ ProposeModelï¼‰")
        print("=" * 60)
        
        # åˆ›å»º RAG é…ç½®
        rag_config = RAGConfig(
            vector_index_dir=self.config.vector_index_dir or "",
            bm25_index_dir=self.config.bm25_index_dir or "",
            initial_top_k=self.config.initial_top_k,
            iterative_top_k=self.config.iterative_top_k,
            max_iterations=self.config.max_iterations
        )
        
        for loop_idx, item in enumerate(tqdm(initial_retrieval_results, desc="æ¨ç†è¿›åº¦")):
            if self.config.early_stop is not None and loop_idx >= self.config.early_stop:
                break

            batch_idx = item['batch_idx']
            batch_data = item['batch_data']
            initial_corpus = item['initial_corpus']
            label = item['label']
            
            # æ£€æŸ¥æ˜¯å¦æœ‰åˆå§‹æ£€ç´¢é”™è¯¯
            if 'error' in item:
                results.append({
                    'batch_idx': batch_idx,
                    'sample_idx': batch_idx * self.config.batch_size,
                    'error': f"åˆå§‹æ£€ç´¢å¤±è´¥: {item['error']}",
                    'label': label
                })
                continue
            
            try:
                # è¿­ä»£æ£€ç´¢ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                iterative_result = None
                final_corpus = initial_corpus
                
                if self.config.enable_iterative and initial_corpus:
                    # æ‰§è¡Œè¿­ä»£æ£€ç´¢ï¼ˆä¸éœ€è¦ retrieverï¼Œå› ä¸ºåªä½¿ç”¨ä¸´æ—¶ç´¢å¼•ï¼‰
                    iterative_result = retrieve_iteratively(
                        generator=self.engine.generator,
                        tokenizer=self.engine.tokenizer,
                        batch_data=batch_data,
                        initial_corpus=initial_corpus,
                        config=rag_config
                    )
                    final_corpus = iterative_result['all_corpus']
                
                # æœ€ç»ˆç”Ÿæˆ
                response, original = generate_response(
                    generator=self.engine.generator,
                    tokenizer=self.engine.tokenizer,
                    batch_data=batch_data,
                    corpus_list=final_corpus,
                    max_new_tokens=self.config.max_new_tokens
                )
                
                result = {
                    'batch_idx': batch_idx,
                    'sample_idx': batch_idx * self.config.batch_size,
                    'generated_text': response,
                    'original_text': original,
                    'label': label,
                    'initial_corpus_count': len(initial_corpus),
                    'initial_corpus_ids': item['initial_corpus_ids'],
                    'final_corpus_count': len(final_corpus),
                    'iterations': len(iterative_result['iterations']) if iterative_result else 0
                }
                results.append(result)
                
                if self.config.verbose and batch_idx < 3:
                    print(f"\n--- æ ·æœ¬ {result['sample_idx']} ---")
                    print(f"ç”Ÿæˆ: {response[:200]}...")
                    print(f"åˆå§‹è¯­æ–™æ•°: {result['initial_corpus_count']}, æœ€ç»ˆè¯­æ–™æ•°: {result['final_corpus_count']}")
                
                # æ¯ä¸ªæ ·æœ¬å¤„ç†å®Œåæ¸…ç†æ˜¾å­˜
                torch.cuda.empty_cache()
                    
            except Exception as e:
                print(f"âš ï¸ æ ·æœ¬ {batch_idx} æ¨ç†å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
                results.append({
                    'batch_idx': batch_idx,
                    'sample_idx': batch_idx * self.config.batch_size,
                    'error': str(e),
                    'label': label
                })
                # å‡ºé”™æ—¶ä¹Ÿæ¸…ç†æ˜¾å­˜ï¼Œé¿å…ç´¯ç§¯
                torch.cuda.empty_cache()
        
        print(f"\nâœ… æ¨ç†å®Œæˆï¼Œå…± {len(results)} ä¸ªæ ·æœ¬")
        return results
    
    @torch.no_grad()
    def run(self, dataloader: DataLoader) -> List[Dict[str, Any]]:
        """
        æ‰§è¡Œå¤æ‚æ¨ç†ï¼ˆRAGæ¨¡å¼ï¼‰
        
        åˆ†ä¸¤é˜¶æ®µæ‰§è¡Œï¼š
        1. åˆå§‹æ£€ç´¢ï¼šéå†æ‰€æœ‰æ ·æœ¬ï¼Œè®°å½•åˆå§‹æ£€ç´¢ç»“æœ
        2. å¸è½½ embedder åï¼Œé’ˆå¯¹æ¯ä¸ªæ ·æœ¬æ‰§è¡Œè¿­ä»£æ£€ç´¢å’Œæœ€ç»ˆç”Ÿæˆ
        
        Args:
            dataloader: æ•°æ®åŠ è½½å™¨
        
        Returns:
            é¢„æµ‹ç»“æœåˆ—è¡¨
        """
        print("\n" + "=" * 60)
        print("ğŸš€ å¼€å§‹å¤æ‚æ¨ç†ï¼ˆRAGæ¨¡å¼ - ä¸¤é˜¶æ®µï¼‰")
        print("=" * 60)
        
        # é˜¶æ®µ1ï¼šåˆå§‹æ£€ç´¢
        initial_retrieval_results = self._phase1_initial_retrieval(dataloader)
        
        # å¸è½½ embedder ä»¥é‡Šæ”¾æ˜¾å­˜
        print("\nğŸ—‘ï¸ å¸è½½ TrafficEmbedder ä»¥é‡Šæ”¾æ˜¾å­˜...")
        self.engine.unload_retriever()
        
        # é˜¶æ®µ2ï¼šè¿­ä»£æ£€ç´¢ä¸æœ€ç»ˆç”Ÿæˆ
        results = self._phase2_iterative_and_generate(initial_retrieval_results)
        
        print(f"\nâœ… å¤æ‚æ¨ç†å®Œæˆï¼Œå…± {len(results)} ä¸ªæ ·æœ¬")
        return results

class MetricsCalculator:
    """æŒ‡æ ‡è®¡ç®—å™¨"""
    
    @staticmethod
    def parse_prediction(text: str) -> Optional[str]:
        """
        ä»ç”Ÿæˆæ–‡æœ¬ä¸­è§£æé¢„æµ‹ç±»åˆ«
        
        æœŸæœ›æ ¼å¼ï¼š
        ç±»åˆ«ï¼š[åˆ†ç±»æ ‡ç­¾]
        è§£é‡Šï¼š[è§£é‡Šæ–‡æœ¬]
        """
        import re
        
        # ä»…åŒ¹é… "ç±»åˆ«ï¼šXXX" æ ¼å¼ï¼ŒåŒ¹é…åˆ°è¡Œå°¾
        pattern = r'ç±»åˆ«[ï¼š:]\s*(.+?)$'
        match = re.search(pattern, text, re.MULTILINE)
        if match:
            return match.group(1).strip()
        
        # æœªåŒ¹é…åˆ°é¢„æœŸæ ¼å¼ï¼ŒæŠ›å‡ºé”™è¯¯
        raise ValueError(f"æ— æ³•è§£æé¢„æµ‹ç»“æœï¼Œæœªæ‰¾åˆ°'ç±»åˆ«ï¼š'æ ¼å¼ã€‚åŸæ–‡æœ¬: {text[:100]}...")
    
    @staticmethod
    def calculate_metrics(
        results: List[Dict[str, Any]], 
        id2label: Dict[str, str] = None
    ) -> Dict[str, Any]:
        """
        è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        
        Args:
            results: æ¨ç†ç»“æœåˆ—è¡¨
            id2label: æ ‡ç­¾IDåˆ°æ ‡ç­¾åçš„æ˜ å°„
        
        Returns:
            æŒ‡æ ‡å­—å…¸
        """
        predictions = []
        ground_truths = []
        parse_failures = 0
        error_samples = 0
        
        def _get_label2id(id2label: Dict[str, str]) -> Dict[str, int]:
            """ä» id2label è®¡ç®— label2id"""
            return {v: int(k) for k, v in id2label.items()}

        label2id = _get_label2id(id2label)
        print(f"\nğŸ“Š æ ‡ç­¾æ˜ å°„ (label2id): {_get_label2id(id2label) if id2label else 'None'}")

        
        for result in results:
            label = result.get('label')
            assert label is not None
            # è½¬æ¢ ground truth
            ground_truths.append(label2id[str(label[0])])

            if 'error' in result:
                predictions.append(len(label2id))
                error_samples += 1
                continue
            
            generated_text = result.get('generated_text', '')
            try:
                pred = MetricsCalculator.parse_prediction(generated_text)
                predictions.append(label2id[pred])
            except Exception as e:
                predictions.append(len(label2id))
                parse_failures += 1
                continue
        
        if not predictions:
            return {
                'error': 'æ— æ³•è®¡ç®—æŒ‡æ ‡ï¼šæ²¡æœ‰æœ‰æ•ˆçš„é¢„æµ‹ç»“æœ',
                'total_samples': len(results),
                'parse_failures': parse_failures,
                'error_samples': error_samples
            }
        
        # è®¡ç®—å‡†ç¡®ç‡
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦å¤„ç†é¢„æµ‹æ ‡ç­¾å’ŒçœŸå®æ ‡ç­¾çš„åŒ¹é…é—®é¢˜
        correct = sum(1 for p, g in zip(predictions, ground_truths) if p == g)
        accuracy = correct / len(predictions)
        
        metrics = {
            'total_samples': len(results),
            'valid_predictions': len(predictions),
            'parse_failures': parse_failures,
            'error_samples': error_samples,
            'accuracy': accuracy,
            'correct_count': correct
        }
        
        # å¦‚æœæ ‡ç­¾æ•°é‡æœ‰é™ï¼Œè®¡ç®—æ›´è¯¦ç»†çš„æŒ‡æ ‡
        unique_labels = list(set(ground_truths))
        try:
            precision, recall, f1, support = precision_recall_fscore_support(
                ground_truths, predictions, average='macro', zero_division=0
            )
            metrics.update({
                'precision': precision,
                'recall': recall,
                'f1_score': f1
            })
        except Exception as e:
            metrics['metrics_error'] = str(e)
        
        return metrics


def run_inference(
    # æ¨¡å‹å‚æ•°
    mode: str = "simple",
    llm_generator: str = "Qwen3-VL-8B-Instruct",
    llm_retriever: str = "Qwen3-VL-Embedding-2B",
    projector: str = "linear",
    linear_output_dim_generator: int = 4096,
    linear_output_dim_retriever: int = 2048,
    # åŠ è½½å‚æ•°
    resume_log: bool = False,
    resume_encoder: str = None,
    resume_linear_0: str = None,
    resume_lora0_0: str = None,
    resume_linear_1: str = None,
    resume_lora0_1: str = None,
    # æ•°æ®é›†å‚æ•°
    dataset_path: str = None,
    batch_size: int = 1,
    # RAGé…ç½®å‚æ•°
    vector_index_dir: str = None,
    bm25_index_dir: str = None,
    initial_top_k: int = 10,
    iterative_top_k: int = 1,
    max_iterations: int = 5,
    enable_iterative: bool = True,
    # ç”Ÿæˆé…ç½®
    max_new_tokens: int = 512,
    temperature: float = 0.8,
    top_p: float = 0.8,
    top_k: int = 20,
    presence_penalty: float = 1.5,
    do_sample: bool = True,
    # è®¾å¤‡é…ç½®
    device: str = None,
    parallel_mode: bool = False,
    inference_dtype: str = None,
    # è¾“å‡ºé…ç½®
    output_dir: str = "./inference_results",
    verbose: bool = True,
    early_stop: Optional[int] = None
):
    """
    è¿è¡Œæ¨ç†æµ‹è¯•
    
    Args:
        mode: æ¨ç†æ¨¡å¼ ("simple" æˆ– "complex")
        llm: LLM æ¨¡å‹è·¯å¾„
        dataset_path: æ•°æ®é›†è·¯å¾„
        output_dir: è¾“å‡ºç›®å½•
        generator_weights: ç”Ÿæˆæ¨¡å‹æƒé‡è·¯å¾„
        embedder_weights: åµŒå…¥æ¨¡å‹æƒé‡è·¯å¾„ï¼ˆRAGæ¨¡å¼éœ€è¦ï¼‰
        vector_index_dir: å‘é‡ç´¢å¼•ç›®å½•ï¼ˆRAGæ¨¡å¼éœ€è¦ï¼‰
        bm25_index_dir: BM25ç´¢å¼•ç›®å½•ï¼ˆRAGæ¨¡å¼éœ€è¦ï¼‰
        parallel_mode: æ˜¯å¦ä½¿ç”¨æ¨¡å‹å¹¶è¡Œ
        max_new_tokens: æœ€å¤§ç”Ÿæˆtokenæ•°
        max_samples: æœ€å¤§æ ·æœ¬æ•°ï¼ˆç”¨äºè°ƒè¯•ï¼‰
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    """
    print("=" * 60)
    print(f"ğŸ”¬ æ¨ç†æµ‹è¯• - {mode.upper()} æ¨¡å¼")
    print("=" * 60)
    
    # åˆ›å»ºé…ç½®
    config = InferenceConfig(
        llm_generator=llm_generator,
        llm_retriever=llm_retriever,
        projector=projector,
        linear_output_dim_generator=linear_output_dim_generator,
        linear_output_dim_retriever=linear_output_dim_retriever,
        resume_log=resume_log,
        resume_encoder=resume_encoder,
        resume_linear_0=resume_linear_0,
        resume_lora0_0=resume_lora0_0,
        resume_linear_1=resume_linear_1,
        resume_lora0_1=resume_lora0_1,
        dataset_path=dataset_path,
        batch_size=batch_size,
        vector_index_dir=vector_index_dir,
        bm25_index_dir=bm25_index_dir,
        initial_top_k=initial_top_k,
        iterative_top_k=iterative_top_k,
        max_iterations=max_iterations,
        enable_iterative=enable_iterative,
        max_new_tokens=max_new_tokens,
        temperature=temperature,
        top_p=top_p,
        top_k=top_k,
        presence_penalty=presence_penalty,
        do_sample=do_sample,
        device=device,
        parallel_mode=parallel_mode,
        inference_dtype=inference_dtype,
        output_dir=output_dir,
        verbose=verbose,
        early_stop=early_stop
    )
    
    # åˆ›å»ºå¼•æ“
    engine = InferenceEngine(config)
    
    # åŠ è½½æ•°æ®é›†
    dataloader = engine.load_dataset()
    
    # æ‰§è¡Œæ¨ç†
    start_time = time.time()
    
    if mode.lower() == "simple":
        inference = SimpleInference(engine)
    elif mode.lower() == "complex":
        inference = ComplexInference(engine)
    else:
        raise ValueError(f"æœªçŸ¥çš„æ¨ç†æ¨¡å¼: {mode}ï¼Œè¯·ä½¿ç”¨ 'simple' æˆ– 'complex'")
    
    results = inference.run(dataloader)
    
    elapsed_time = time.time() - start_time
    
    # è®¡ç®—æŒ‡æ ‡
    print("\n" + "=" * 60)
    print("ğŸ“Š è®¡ç®—è¯„ä¼°æŒ‡æ ‡")
    print("=" * 60)
    
    id2label = dataloader.dataset.id2label if hasattr(dataloader.dataset, 'id2label') else None
    assert id2label is not None
    
    metrics = MetricsCalculator.calculate_metrics(results, id2label)
    metrics['elapsed_time_seconds'] = elapsed_time
    metrics['samples_per_second'] = len(results) / elapsed_time if elapsed_time > 0 else 0
    
    print(f"\nğŸ“ˆ è¯„ä¼°ç»“æœ:")
    print(f"   - æ€»æ ·æœ¬æ•°: {metrics.get('total_samples', 'N/A')}")
    print(f"   - æœ‰æ•ˆé¢„æµ‹: {metrics.get('valid_predictions', 'N/A')}")
    print(f"   - è§£æå¤±è´¥: {metrics.get('parse_failures', 'N/A')}")
    print(f"   - å‡†ç¡®ç‡: {metrics.get('accuracy', 'N/A'):.4f}" if 'accuracy' in metrics else "")
    print(f"   - F1åˆ†æ•°: {metrics.get('f1_score', 'N/A'):.4f}" if 'f1_score' in metrics else "")
    print(f"   - è€—æ—¶: {elapsed_time:.2f}s")
    print(f"   - é€Ÿåº¦: {metrics['samples_per_second']:.2f} æ ·æœ¬/ç§’")
    
    # ä¿å­˜ç»“æœ
    if config.save_predictions:
        os.makedirs(output_dir, exist_ok=True)
        
        # ä¿å­˜é¢„æµ‹ç»“æœ
        predictions_path = os.path.join(output_dir, f"{mode}_predictions.json")
        with open(predictions_path, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        print(f"\nğŸ’¾ é¢„æµ‹ç»“æœå·²ä¿å­˜: {predictions_path}")
        
        # ä¿å­˜æŒ‡æ ‡
        metrics_path = os.path.join(output_dir, f"{mode}_metrics.json")
        with open(metrics_path, 'w', encoding='utf-8') as f:
            json.dump(metrics, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ è¯„ä¼°æŒ‡æ ‡å·²ä¿å­˜: {metrics_path}")
    
    print("\n" + "=" * 60)
    print("âœ… æ¨ç†æµ‹è¯•å®Œæˆ")
    print("=" * 60)
    
    # return results, metrics

if __name__ == "__main__":
    import fire
    fire.Fire(run_inference)
