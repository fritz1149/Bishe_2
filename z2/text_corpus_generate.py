"""
æ–‡æœ¬è¯­æ–™ç”Ÿæˆæ¨¡å—

ä¸»è¦åŠŸèƒ½ï¼š
1. TextCorpusGenerator: ä½¿ç”¨ ProposeModel ç”Ÿæˆæ–‡æœ¬è¯­æ–™
2. BM25 è¯­æ–™åº“çš„æ„å»º
"""

import json
import os
import torch
from typing import List, Dict, Optional, Tuple
from tqdm import tqdm


class TextCorpusGenerator:
    """
    æ–‡æœ¬è¯­æ–™ç”Ÿæˆå™¨ - ä»é¢„å¤„ç†çš„æ•°æ®é›†ä½¿ç”¨ LLM ç”Ÿæˆæ–‡æœ¬è¯­æ–™
    
    ä½¿ç”¨ ProposeModel å¯¹æµé‡ç†è§£é—®é¢˜è¿›è¡Œå›ç­”ï¼Œç”Ÿæˆæ–‡æœ¬è¯­æ–™å¹¶å­˜å‚¨ã€‚
    """
    
    def __init__(self, args, device: str = None):
        """
        åˆå§‹åŒ–æ–‡æœ¬è¯­æ–™ç”Ÿæˆå™¨
        
        Args:
            args: æ¨¡å‹å‚æ•°ï¼Œéœ€åŒ…å« ProposeModel æ‰€éœ€çš„é…ç½®
            device: è®¾å¤‡ ('cuda' æˆ– 'cpu')ï¼ŒNone åˆ™è‡ªåŠ¨é€‰æ‹©
        """
        self.args = args
        # self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
        self.model = None
        self.tokenizer = None
    
    def _load_model(self):
        """å»¶è¿ŸåŠ è½½æ¨¡å‹"""
        if self.model is None:
            from z1.model import ProposeModel
            from transformers import AutoTokenizer
            
            self.model = ProposeModel(self.args)
            self.model.eval()
            # self.model.to(self.device)
            self.model.dispatch()
            self.model.resume(self.args)
            
            self.tokenizer = AutoTokenizer.from_pretrained(self.args.llm)
            
            print(f"âœ… ProposeModel å·²åŠ è½½")
    
    @torch.no_grad()
    def generate_corpus(
        self,
        dataset_path: str,
        output_dir: str,
        num_generations: int = 5,
        entropy_threshold: float = 1.5,
        save_threshold: int = 1000,
        max_new_tokens: int = 512,
        min_new_tokens: int = 64,
        temperature: float = 0.7,
        repetition_penalty: float = 1.25,
        generation_mode: str = "batch",
        early_stop_batch: int = None,
        skip_clustering: bool = False
    ) -> None:
        """
        ä»æ•°æ®é›†ç”Ÿæˆæ–‡æœ¬è¯­æ–™ï¼Œä½¿ç”¨å¤šç»“æœç”Ÿæˆã€LLMèšç±»å’Œç†µç­›é€‰
        
        å¯¹æ¯ä¸ªæ ·æœ¬ï¼š
        1. ç”Ÿæˆ num_generations æ¡ç»“æœ
        2. å°†ç»“æœæ‹¼æ¥åè¾“å…¥ LLM è¿›è¡Œè¯­ä¹‰èšç±»
        3. æ ¹æ®èšç±»ç»“æœè®¡ç®—ç†µ
        4. è‹¥ç†µè¶…è¿‡é˜ˆå€¼åˆ™æŠ›å¼ƒæœ¬è½®è¾“å‡ºï¼Œå¦åˆ™ä»ç»“æœæ•°é‡æœ€å¤šçš„èšç±»ä¸­éšæœºé€‰æ‹©ä¸€æ¡
        
        Args:
            dataset_path: æ•°æ®é›†ç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆæ¯æ‰¹ä¼šä¿å­˜åˆ°å•ç‹¬çš„æ–‡ä»¶ï¼‰
            num_generations: æ¯ä¸ªæ ·æœ¬ç”Ÿæˆçš„ç»“æœæ•°é‡
            entropy_threshold: ç†µé˜ˆå€¼ï¼Œè¶…è¿‡åˆ™æŠ›å¼ƒæœ¬è½®è¾“å‡º
            save_threshold: ç´¯ç§¯å¤šå°‘æ ·æœ¬åä¿å­˜ä¸€æ¬¡
            max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
            generation_mode: ç”Ÿæˆæ¨¡å¼ï¼ˆ"batch" ä¸€æ¬¡æ€§ç”Ÿæˆ / "loop" å¾ªç¯é€æ¡ç”Ÿæˆï¼‰
            skip_clustering: æ˜¯å¦è·³è¿‡èšç±»ç­›é€‰ï¼Œç›´æ¥ä¿å­˜æ‰€æœ‰ç”Ÿæˆç»“æœ
        """
        import math
        import random
        import re
        from torch.utils.data import DataLoader
        from dataset import CustomDataset, collate_LLMDataset_leftpadding
        
        self._load_model()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # åŠ è½½æ•°æ®é›†
        print(f"ğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®é›†: {dataset_path}")
        dataset = CustomDataset(dataset_path)
        
        # åˆ›å»º DataLoaderï¼Œbatch_size å›ºå®šä¸º 1
        dataloader = DataLoader(
            dataset,
            batch_size=1,
            shuffle=True,
            collate_fn=lambda batch: collate_LLMDataset_leftpadding(batch, keep_labels=False),
            num_workers=0
        )

        generation_mode = generation_mode.lower()
        if generation_mode not in {"batch", "loop"}:
            raise ValueError(f"generation_mode å¿…é¡»æ˜¯ 'batch' æˆ– 'loop'ï¼Œå½“å‰: {generation_mode}")
        
        print(f"ğŸ”„ å¼€å§‹ç”Ÿæˆæ–‡æœ¬è¯­æ–™...")
        print(f"   - æ¯æ ·æœ¬ç”Ÿæˆæ•°é‡: {num_generations}")
        print(f"   - ç”Ÿæˆæ¨¡å¼: {generation_mode}")
        print(f"   - ç†µé˜ˆå€¼: {entropy_threshold}")
        print(f"   - å­˜å‚¨é˜ˆå€¼: {save_threshold}")
        print(f"   - æœ€å¤§ç”Ÿæˆtokenæ•°: {max_new_tokens}")
        print(f"   - æœ€å°ç”Ÿæˆtokenæ•°: {min_new_tokens}")
        print(f"   - æ¸©åº¦: {temperature}")
        print(f"   - é‡å¤æƒ©ç½š: {repetition_penalty}")
        print(f"   - è·³è¿‡èšç±»ç­›é€‰: {skip_clustering}")
        if early_stop_batch is not None:
            print(f"   - æå‰åœæ­¢æ‰¹æ¬¡: {early_stop_batch}")

        
        total_samples = 0
        discarded_samples = 0
        save_batch_idx = 0
        
        # ç´¯ç§¯çš„è¯­æ–™å’Œids
        accumulated_corpus = []
        accumulated_ids = []
        
        def compute_entropy(cluster_sizes: list) -> float:
            """è®¡ç®—èšç±»ç»“æœçš„ç†µ"""
            total = sum(cluster_sizes)
            if total == 0:
                return 0.0
            entropy = 0.0
            for size in cluster_sizes:
                if size > 0:
                    p = size / total
                    entropy -= p * math.log2(p)
            return entropy
        
        def parse_clusters(cluster_text: str, num_results: int) -> dict:
            """è§£æ LLM è¾“å‡ºçš„èšç±»ç»“æœ
            
            æœŸæœ›æ ¼å¼: 
            èšç±»1: 1, 3, 5
            èšç±»2: 2, 4
            
            Raises:
                ValueError: è§£æå¤±è´¥æˆ–èšç±»ç»“æœæœªè¦†ç›–æ‰€æœ‰ç¼–å·æˆ–æœ‰é‡å¤ç¼–å·
            """
            clusters = {}
            # è¦†ç›–çŠ¶æ€æ•°ç»„ï¼ŒFalseè¡¨ç¤ºæœªè¦†ç›–ï¼ŒTrueè¡¨ç¤ºå·²è¦†ç›–
            covered = [False] * (num_results + 1)  # ç´¢å¼•0ä¸ä½¿ç”¨ï¼Œ1~num_resultså¯¹åº”ç¼–å·
            uncovered_count = num_results  # å‰©ä½™æœªè¢«è¦†ç›–çš„ç¼–å·æ•°é‡
            
            # åŒ¹é… "èšç±»X: 1, 2, 3" æˆ– "ç±»åˆ«X: 1, 2, 3" æ ¼å¼
            pattern = r'(?:èšç±»|ç±»åˆ«|Cluster)\s*(\d+)\s*[:ï¼š]\s*([\d,ï¼Œ\s]+)'
            matches = re.findall(pattern, cluster_text, re.IGNORECASE)
            
            for cluster_id, members_str in matches:
                # è§£ææˆå‘˜ç¼–å·
                members_str = members_str.replace('ï¼Œ', ',')
                members = []
                for m in members_str.split(','):
                    m = m.strip()
                    if m.isdigit():
                        idx = int(m)
                        if 1 <= idx <= num_results:
                            if not covered[idx]:
                                covered[idx] = True
                                uncovered_count -= 1
                                members.append(idx)
                            else:
                                raise ValueError(f"èšç±»è§£æå¤±è´¥: ç¼–å· {idx} é‡å¤å‡ºç°")
                if members:
                    clusters[int(cluster_id)] = members
            
            # å¦‚æœè§£æå¤±è´¥ï¼ŒæŠ›å‡ºé”™è¯¯
            if not clusters:
                raise ValueError(f"èšç±»è§£æå¤±è´¥: æœªèƒ½ä»è¾“å‡ºä¸­æå–æœ‰æ•ˆèšç±»")
            
            # æ£€æŸ¥æ˜¯å¦è¦†ç›–äº†æ‰€æœ‰ç¼–å·
            if uncovered_count != 0:
                missing = [i for i in range(1, num_results + 1) if not covered[i]]
                raise ValueError(f"èšç±»ç»“æœä¸å®Œæ•´: ç¼ºå°‘ç¼–å· {missing}")
            
            return clusters
        
        def build_clustering_prompt(results: list) -> str:
            """æ„å»ºèšç±» prompt"""
            results_text = "\n".join([
                f"[{i+1}] {r}" for i, r in enumerate(results)
            ])
            prompt = f"""<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªæ–‡æœ¬èšç±»åŠ©æ‰‹ï¼Œæ“…é•¿ä»è¯­ä¹‰è§’åº¦å¯¹æ–‡æœ¬è¿›è¡Œèšç±»ã€‚<|im_end|>
<|im_start|>user
ä»¥ä¸‹æ˜¯é’ˆå¯¹åŒä¸€ç½‘ç»œæµé‡çš„å¤šæ¡åˆ†æç»“æœï¼Œè¯·ä»è¯­ä¹‰è§’åº¦å°†å®ƒä»¬èšç±»ï¼Œç›´æ¥è¾“å‡ºèšç±»ç»“æœä¸éœ€è¦è¾“å‡ºè§£é‡Šã€‚

åˆ†æç»“æœï¼š
{results_text}

è¯·æŒ‰ä»¥ä¸‹æ ¼å¼è¾“å‡ºèšç±»ç»“æœï¼Œæ¯è¡Œä¸€ä¸ªèšç±»ï¼ŒåŒ…å«å±äºè¯¥èšç±»çš„ç»“æœç¼–å·ï¼š
èšç±»1: 1, 3, 5
èšç±»2: 2, 4
...<|im_end|>
<|im_start|>assistant
"""
            return prompt
        
        # åˆ†æ‰¹å¤„ç†å’Œå­˜å‚¨
        batch_idx = 0
        import sys
        for batch_data, txt_filenames in tqdm(dataloader, desc="ç”Ÿæˆè¯­æ–™"):
            batch_idx += 1
            print(f"batch_idx: {batch_idx}")
            sys.stdout.flush()
            if early_stop_batch is not None and batch_idx > early_stop_batch:
                break
            try:
                # batch_size=1ï¼Œæ‰€ä»¥ txt_filenames åªæœ‰ä¸€ä¸ªå…ƒç´ 
                sample_id = txt_filenames[0]
                input_length = batch_data['input_ids'].shape[1]
                # è§£ç batch_dataè·å–é—®é¢˜ï¼ˆç¬¬å…­è¡Œï¼‰
                decoded_text = self.tokenizer.decode(batch_data['input_ids'][0], skip_special_tokens=True)
                lines = decoded_text.strip().split('\n')
                if len(lines) >= 6:
                    question = lines[5]  # ç¬¬å…­è¡Œï¼ˆç´¢å¼•5ï¼‰
                
                if generation_mode == "batch":
                    # 1. å¯¹åŒä¸€æ ·æœ¬ç”Ÿæˆå¤šæ¡ç»“æœï¼ˆé€šè¿‡å¤åˆ¶ batch_data å®ç°æ‰¹é‡ç”Ÿæˆï¼‰
                    expanded_batch = {}
                    for key, value in batch_data.items():
                        if key == 'labels':
                            continue
                        if key == 'payloads':
                            # payloads æ˜¯åˆ—è¡¨ï¼Œéœ€è¦å¤åˆ¶æ¯ä¸ªå…ƒç´ 
                            assert len(value) == 1 and isinstance(value[0], tuple) and len(value[0]) == 3
                            expanded_batch[key] = [value[0] for _ in range(num_generations)]
                        elif key == 'position_ids':
                            # position_ids åœ¨ç¬¬äºŒä¸ªç»´åº¦å¤åˆ¶ï¼Œå½¢çŠ¶ä» [3, 1, seq_len] å˜æˆ [3, num_generations, seq_len]
                            expanded_batch[key] = value.repeat(1, num_generations, 1)
                        else:
                            # å…¶ä»– tensor æ•°æ®ï¼Œåœ¨ç¬¬ä¸€ä¸ªç»´åº¦å¤åˆ¶
                            expanded_batch[key] = value.repeat(num_generations, 1)

                    # expanded_batch = {
                    #     k: (
                    #         [x.to(self.device) if torch.is_tensor(x) else x for x in v]
                    #         if isinstance(v, list)
                    #         else (v.to(self.device) if torch.is_tensor(v) else v)
                    #     )
                    #     for k, v in expanded_batch.items()
                    # }
                    
                    # æ‰¹é‡ç”Ÿæˆ
                    outputs = self.model.generate(
                        **expanded_batch,
                        max_new_tokens=max_new_tokens,
                        # min_new_tokens=min_new_tokens,
                        repetition_penalty=repetition_penalty,
                        do_sample=True,
                        temperature=temperature,
                        top_p=0.9
                    ).cpu()
                else:
                    # é€æ¡ç”Ÿæˆï¼ˆå¾ªç¯ num_generations æ¬¡ï¼Œæ¯æ¬¡åªå¤„ç†ä¸€æ¡æ•°æ®ï¼‰
                    outputs_list = []
                    for _ in range(num_generations):
                        output = self.model.generate(
                            **batch_data,
                            max_new_tokens=max_new_tokens,
                            min_new_tokens=min_new_tokens,
                            repetition_penalty=repetition_penalty,
                            do_sample=True,
                            temperature=temperature,
                            top_p=0.9,
                        ).cpu()
                        outputs_list.append(output[0])

                    pad_token_id = (
                        self.tokenizer.pad_token_id
                        if self.tokenizer.pad_token_id is not None
                        else 0
                    )
                    outputs = torch.nn.utils.rnn.pad_sequence(
                        outputs_list,
                        batch_first=True,
                        padding_value=pad_token_id
                    )
                # è§£ç æ‰€æœ‰ç”Ÿæˆçš„æ–‡æœ¬
                generated_results = []
                for i in range(num_generations):
                    generated_text = self.tokenizer.decode(
                        outputs[i][input_length:],
                        skip_special_tokens=True
                    )
                    generated_results.append(generated_text)
                
                if skip_clustering or early_stop_batch is not None:
                    # è·³è¿‡èšç±»ç­›é€‰ï¼Œç›´æ¥ä¿å­˜æ‰€æœ‰ç”Ÿæˆç»“æœ
                    # for idx, result in enumerate(generated_results):
                    #     accumulated_corpus.append({
                    #         'id': f"{sample_id}_{idx}",
                    #         'contents': result
                    #     })
                    #     accumulated_ids.append(f"{sample_id}_{idx}")
                    accumulated_corpus.append({
                        'id': sample_id,
                        'contents': generated_results,
                        'question': question or ""
                    })
                else:
                    #TODOï¼šåˆ†ç±»ä»»åŠ¡ä½œä¸ºé—®é¢˜æ—¶ï¼Œå¯ä»¥ç›´æ¥åˆ†ç±»ï¼Œä¸ç”¨LLMèšç±»
                    # 2. æ„å»ºèšç±» prompt å¹¶è®© LLM è¿›è¡Œèšç±»
                    clustering_prompt = build_clustering_prompt(generated_results)
                    clustering_input = self.tokenizer(
                        clustering_prompt, 
                        return_tensors='pt', 
                        add_special_tokens=False
                    )
                    clustering_input['position_ids'] = torch.arange(
                        clustering_input['input_ids'].shape[1]
                    ).unsqueeze(0).expand(3, -1, -1)
                    clustering_input = {k: v.to(self.device) for k, v in clustering_input.items()}
                    
                    cluster_output = self.model.generate(
                        **clustering_input,
                        max_new_tokens=64,
                        do_sample=False,
                        temperature=0.1
                    ).cpu()
                    
                    cluster_text = self.tokenizer.decode(
                        cluster_output[0][clustering_input['input_ids'].shape[1]:],
                        skip_special_tokens=True
                    )
                    
                    # 3. è§£æèšç±»ç»“æœ
                    clusters = parse_clusters(cluster_text, num_generations)
                    cluster_sizes = [len(members) for members in clusters.values()]
                    
                    # 4. è®¡ç®—ç†µ
                    entropy = compute_entropy(cluster_sizes)
                    
                    # 5. æ ¹æ®ç†µå†³å®šæ˜¯å¦ä¿ç•™ç»“æœ
                    if entropy > entropy_threshold:
                        # ç†µè¶…è¿‡é˜ˆå€¼ï¼ŒæŠ›å¼ƒæœ¬è½®è¾“å‡º
                        discarded_samples += 1
                        continue
                    
                    # 6. ä»ç»“æœæ•°é‡æœ€å¤šçš„èšç±»ä¸­éšæœºé€‰æ‹©ä¸€æ¡
                    largest_cluster_id = max(clusters.keys(), key=lambda k: len(clusters[k]))
                    largest_cluster_members = clusters[largest_cluster_id]
                    selected_idx = random.choice(largest_cluster_members) - 1  # è½¬ä¸º 0-indexed
                    selected_result = generated_results[selected_idx]
                    
                    accumulated_corpus.append({
                        'id': sample_id,
                        'contents': selected_result
                    })
                    accumulated_ids.append(sample_id)
            except Exception as e:
                import traceback
                error_detail = traceback.format_exc()
                print(f"Error processing sample {sample_id}: {str(e)}\nè¯¦ç»†å †æ ˆä¿¡æ¯:\n{error_detail}")
                continue
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°å­˜å‚¨é˜ˆå€¼
            if len(accumulated_corpus) >= save_threshold:
                # ä¿å­˜å½“å‰æ‰¹æ¬¡
                batch_file = os.path.join(output_dir, f'corpus_batch_{save_batch_idx:05d}.jsonl')
                with open(batch_file, 'w', encoding='utf-8') as f:
                    for entry in accumulated_corpus:
                        f.write(json.dumps(entry, ensure_ascii=False) + '\n')
                
                total_samples += len(accumulated_corpus)
                save_batch_idx += 1
                
                # é‡ç½®ç´¯ç§¯æ•°æ®
                accumulated_corpus = []
                accumulated_ids = []
        
        # ä¿å­˜å‰©ä½™çš„æ•°æ®
        if len(accumulated_corpus) > 0:
            batch_file = os.path.join(output_dir, f'corpus_batch_{save_batch_idx:05d}.jsonl')
            with open(batch_file, 'w', encoding='utf-8') as f:
                for entry in accumulated_corpus:
                    f.write(json.dumps(entry, ensure_ascii=False) + '\n')
            
            total_samples += len(accumulated_corpus)
            save_batch_idx += 1
        
        print(f"\nâœ… è¯­æ–™ç”Ÿæˆå®Œæˆï¼")
        print(f"   - æœ‰æ•ˆæ ·æœ¬æ•°: {total_samples}")
        print(f"   - æŠ›å¼ƒæ ·æœ¬æ•°: {discarded_samples} (ç†µè¶…è¿‡é˜ˆå€¼)")
        print(f"   - ä¿å­˜æ‰¹æ¬¡æ•°é‡: {save_batch_idx}")
        print(f"   - è¾“å‡ºç›®å½•: {output_dir}")
    
    def build_bm25_index(
        self,
        corpus_path: str,
        index_dir: str,
        analyzer_name: str = 'whitespace',
        verbose: bool = True
    ) -> None:
        """
        ä»è¯­æ–™æ–‡ä»¶æ„å»º BM25 ç´¢å¼•
        
        Args:
            corpus_path: è¯­æ–™æ–‡ä»¶è·¯å¾„ (.jsonl)
            index_dir: ç´¢å¼•ä¿å­˜ç›®å½•
            analyzer_name: Lucene åˆ†æå™¨åç§°
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        """
        from z2.RAG.retriever.BM25 import build_index
        
        build_index(
            corpus_file=corpus_path,
            index_dir=index_dir,
            analyzer_name=analyzer_name,
            verbose=verbose
        )


def run_text_corpus_pipeline(
    # æ•°æ®é›†è·¯å¾„
    dataset_path: str,
    # è¾“å‡ºè·¯å¾„
    corpus_output_dir: str,
    index_output_dir: str,
    # æ¨¡å‹å‚æ•°
    llm: str = 'Qwen3-VL-8B-Instruct',
    projector: str = 'linear',
    linear_output_dim: int = 4096,
    # ç”Ÿæˆå‚æ•°
    num_generations: int = 5,
    entropy_threshold: float = 1.5,
    save_threshold: int = 1000,
    max_new_tokens: int = 512,
    generation_mode: str = "batch",
    # ç´¢å¼•å‚æ•°
    analyzer_name: str = 'whitespace',
    verbose: bool = True,
    # åŠ è½½å‚æ•°
    resume_log: bool = True,
    resume_encoder: str = None,
    resume_linear: str = None,
    resume_lora0: str = None,
    resume_lora1: str = None,
    # å…¶ä»–å‚æ•°
    early_stop_batch: int = None,
    skip_clustering: bool = False,
    skip_indexing: bool = False
):
    """
    æ–‡æœ¬è¯­æ–™ç”Ÿæˆæµæ°´çº¿ï¼šç”Ÿæˆè¯­æ–™ + æ„å»º BM25 ç´¢å¼•
    
    Args:
        dataset_path: æ•°æ®é›†ç›®å½•è·¯å¾„
        corpus_output_dir: è¯­æ–™è¾“å‡ºç›®å½•
        index_output_dir: BM25 ç´¢å¼•è¾“å‡ºç›®å½•
        llm: LLM æ¨¡å‹è·¯å¾„
        train_mode: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
        eval_mode: æ˜¯å¦ä¸ºè¯„ä¼°æ¨¡å¼
        adapter_path: é€‚é…å™¨è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        num_generations: æ¯ä¸ªæ ·æœ¬ç”Ÿæˆçš„ç»“æœæ•°é‡
        entropy_threshold: ç†µé˜ˆå€¼
        save_threshold: å­˜å‚¨é˜ˆå€¼
        max_new_tokens: æœ€å¤§ç”Ÿæˆ token æ•°
        generation_mode: ç”Ÿæˆæ¨¡å¼ï¼ˆ"batch" ä¸€æ¬¡æ€§ç”Ÿæˆ / "loop" å¾ªç¯é€æ¡ç”Ÿæˆï¼‰
        analyzer_name: Lucene åˆ†æå™¨åç§°
        device: è®¾å¤‡
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    """
    from types import SimpleNamespace
    
    print("=" * 60)
    print("æ–‡æœ¬è¯­æ–™ç”Ÿæˆæµæ°´çº¿")
    print("=" * 60)
    
    # æ„å»º args
    args = SimpleNamespace(
        llm=llm,
        linear_output_dim=linear_output_dim,
        resume_log=resume_log,
        resume_encoder=resume_encoder,
        resume_linear=resume_linear,
        resume_lora0=resume_lora0,
        resume_lora1=resume_lora1,
        align1_mode=False, align2_mode=False, test_mode=False, eval_mode=True,
        finetune_mode=False,
        projector=projector
    )
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = TextCorpusGenerator(args)
    
    # æ­¥éª¤ 1: ç”Ÿæˆæ–‡æœ¬è¯­æ–™
    print("\nã€æ­¥éª¤ 1/2ã€‘ç”Ÿæˆæ–‡æœ¬è¯­æ–™")
    print("-" * 60)
    generator.generate_corpus(
        dataset_path=dataset_path,
        output_dir=corpus_output_dir,
        num_generations=num_generations,
        entropy_threshold=entropy_threshold,
        save_threshold=save_threshold,
        max_new_tokens=max_new_tokens,
        generation_mode=generation_mode,
        early_stop_batch=early_stop_batch,
        skip_clustering=skip_clustering
    )
    
    if skip_indexing or early_stop_batch is not None:
        print("\n" + "=" * 60)
        print("âœ… è¯­æ–™ç”Ÿæˆå®Œæˆï¼Œè·³è¿‡æ„å»ºç´¢å¼•æ­¥éª¤")
        print("=" * 60)
        return
    
    # æ­¥éª¤ 2: æ„å»º BM25 ç´¢å¼•
    print("\nã€æ­¥éª¤ 2/2ã€‘æ„å»º BM25 ç´¢å¼•")
    print("-" * 60)
    generator.build_bm25_index(
        corpus_path=corpus_output_dir,
        index_dir=index_output_dir,
        analyzer_name=analyzer_name,
        verbose=verbose
    )
    
    print("\n" + "=" * 60)
    print("âœ… æ–‡æœ¬è¯­æ–™ç”Ÿæˆæµæ°´çº¿å®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    import fire
    fire.Fire()
