"""
æµé‡åµŒå…¥å‘é‡ç”Ÿæˆæ¨¡å—

ä¸»è¦åŠŸèƒ½ï¼š
1. TrafficEmbeddingGenerator: ä½¿ç”¨ TrafficEmbedder ç”Ÿæˆæµé‡å‘é‡
2. å‘é‡åº“çš„æ„å»º
"""

import os
import pickle
import numpy as np
import torch
from typing import List, Dict, Optional, Tuple
from tqdm import tqdm


class TrafficEmbeddingGenerator:
    """
    æµé‡åµŒå…¥ç”Ÿæˆå™¨ - ä»é¢„å¤„ç†çš„æ•°æ®é›†ç”Ÿæˆæµé‡å‘é‡
    
    ä½¿ç”¨ TrafficEmbedder æ¨¡å‹å¯¹æµé‡è¿›è¡Œç¼–ç ï¼Œç”Ÿæˆå‘é‡å¹¶å­˜å‚¨ã€‚
    """
    
    def __init__(self, args):
        """
        åˆå§‹åŒ–æµé‡åµŒå…¥ç”Ÿæˆå™¨
        
        Args:
            args: æ¨¡å‹å‚æ•°ï¼Œéœ€åŒ…å« TrafficEmbedder æ‰€éœ€çš„é…ç½®
            device: è®¾å¤‡ ('cuda' æˆ– 'cpu')ï¼ŒNone åˆ™è‡ªåŠ¨é€‰æ‹©
        """
        self.args = args
        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
        self.model = None
        self.processor = None
    
    def _load_model(self):
        """å»¶è¿ŸåŠ è½½æ¨¡å‹"""
        if self.model is None:
            from z2.model import TrafficEmbedder
            from transformers import AutoProcessor
            
            self.model = TrafficEmbedder(self.args)
            self.model.to(self.device)
            # self.model.dispatch()
            self.model.resume(self.args)
            self.model.eval()
            
            self.processor = AutoProcessor.from_pretrained(self.args.llm)
            
            print(f"âœ… TrafficEmbedder å·²åŠ è½½ (è®¾å¤‡: {self.device})")
    
    @torch.no_grad()
    def generate_embeddings(
        self,
        dataset_path: str,
        output_dir: str,
        batch_size: int = 8,
        save_threshold: int = 1000,
        normalize: bool = True
    ) -> None:
        """
        ä»æ•°æ®é›†ç”Ÿæˆæµé‡åµŒå…¥å‘é‡ï¼Œä½¿ç”¨ CustomDataset å’Œç´¯ç§¯é˜ˆå€¼å­˜å‚¨
        
        Args:
            dataset_path: æ•°æ®é›†ç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„ï¼ˆæ¯æ‰¹ä¼šä¿å­˜åˆ°å•ç‹¬çš„æ–‡ä»¶ï¼‰
            batch_size: æ•°æ®å¤„ç†æ‰¹å¤„ç†å¤§å°
            save_threshold: ç´¯ç§¯å¤šå°‘æ ·æœ¬åä¿å­˜ä¸€æ¬¡
            normalize: æ˜¯å¦å½’ä¸€åŒ–åµŒå…¥å‘é‡
        """
        from torch.utils.data import DataLoader
        from dataset import CustomDataset, collate_TrafficEmbeddingDataset
        
        self._load_model()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # åŠ è½½æ•°æ®é›†
        print(f"ğŸ“‚ æ­£åœ¨åŠ è½½æ•°æ®é›†: {dataset_path}")
        dataset = CustomDataset(dataset_path)
        
        # åˆ›å»º DataLoader
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=False,
            collate_fn=collate_TrafficEmbeddingDataset,
            num_workers=0
        )
        
        print(f"ğŸ”„ å¼€å§‹ç”ŸæˆåµŒå…¥å‘é‡...")
        print(f"   - å¤„ç†æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"   - å­˜å‚¨é˜ˆå€¼: {save_threshold}")
        
        total_samples = 0
        save_batch_idx = 0
        
        # ç´¯ç§¯çš„embeddingså’Œids
        accumulated_embeddings = []
        accumulated_ids = []
        
        # åˆ†æ‰¹å¤„ç†å’Œå­˜å‚¨
        for batch_data, txt_filenames in tqdm(dataloader, desc="ç”ŸæˆåµŒå…¥"):
            # batch_data æ˜¯å­—å…¸: {input_ids, payloads, position_ids, attention_mask}
            # labels æ˜¯åˆ—è¡¨ï¼ŒåŒ…å« txt_filename (id)
            
            # ç”ŸæˆåµŒå…¥å¹¶ç›´æ¥è½¬åˆ°cpu
            embeddings = self.model(**batch_data, normalize=normalize).cpu()
            embeddings_np = embeddings.numpy()
            
            # ä» labels ä¸­ç›´æ¥è·å– id
            accumulated_ids.extend(txt_filenames)
            
            # ç´¯ç§¯embeddings
            accumulated_embeddings.append(embeddings_np)
            
            # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°å­˜å‚¨é˜ˆå€¼
            if len(accumulated_ids) >= save_threshold:
                # åˆå¹¶å¹¶ä¿å­˜
                merged_embeddings = np.vstack(accumulated_embeddings)
                
                batch_output = {
                    'embeddings': merged_embeddings,
                    'ids': accumulated_ids,
                    'dim': merged_embeddings.shape[1],
                    'num_samples': len(accumulated_ids)
                }
                
                batch_file = os.path.join(output_dir, f'embeddings_batch_{save_batch_idx:05d}.pkl')
                with open(batch_file, 'wb') as f:
                    pickle.dump(batch_output, f)
                
                total_samples += len(accumulated_ids)
                save_batch_idx += 1
                
                # é‡ç½®ç´¯ç§¯æ•°æ®
                accumulated_embeddings = []
                accumulated_ids = []
        
        # ä¿å­˜å‰©ä½™çš„æ•°æ®
        if len(accumulated_ids) > 0:
            merged_embeddings = np.vstack(accumulated_embeddings)
            
            batch_output = {
                'embeddings': merged_embeddings,
                'ids': accumulated_ids,
                'dim': merged_embeddings.shape[1],
                'num_samples': len(accumulated_ids)
            }
            
            batch_file = os.path.join(output_dir, f'embeddings_batch_{save_batch_idx:05d}.pkl')
            with open(batch_file, 'wb') as f:
                pickle.dump(batch_output, f)
            
            total_samples += len(accumulated_ids)
            save_batch_idx += 1
        
        print(f"\nâœ… åµŒå…¥ç”Ÿæˆå®Œæˆï¼")
        print(f"   - æ ·æœ¬æ€»æ•°: {total_samples}")
        print(f"   - ä¿å­˜æ‰¹æ¬¡æ•°é‡: {save_batch_idx}")
        print(f"   - è¾“å‡ºç›®å½•: {output_dir}")
    
    #TODOï¼šåˆ†æ‰¹æ„å»ºç´¢å¼•
    def build_vector_index(
        self,
        embeddings_path: str,
        index_dir: str,
        index_type: str = 'flat',
        verbose: bool = True
    ) -> None:
        """
        ä»åµŒå…¥æ–‡ä»¶æ„å»º FAISS å‘é‡ç´¢å¼•ï¼ˆä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
        
        Args:
            embeddings_path: åµŒå…¥æ–‡ä»¶è·¯å¾„ (.pkl)
            index_dir: ç´¢å¼•ä¿å­˜ç›®å½•
            index_type: ç´¢å¼•ç±»å‹ ('flat' æˆ– 'ivf')
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
        """
        from z2.RAG.vector_utils import build_faiss_index
        
        # éå†è¾“å…¥ç›®å½•ä¸‹çš„æ‰€æœ‰ pkl æ–‡ä»¶
        embeddings_dir = embeddings_path
        pkl_files = sorted([f for f in os.listdir(embeddings_dir) if f.endswith('.pkl')])
        
        if not pkl_files:
            raise FileNotFoundError(f"ç›®å½•ä¸‹æ²¡æœ‰æ‰¾åˆ° .pkl æ–‡ä»¶: {embeddings_dir}")
        
        all_embeddings = []
        all_ids = []
        
        for pkl_file in pkl_files:
            pkl_path = os.path.join(embeddings_dir, pkl_file)
            with open(pkl_path, 'rb') as f:
                data = pickle.load(f)
            all_embeddings.append(data['embeddings'])
            all_ids.extend(data['ids'])
        
        embeddings = np.vstack(all_embeddings)
        
        # ä½¿ç”¨å…±äº«çš„ç´¢å¼•æ„å»ºå‡½æ•°
        build_faiss_index(
            embeddings=embeddings,
            doc_ids=all_ids,
            doc_contents=None,
            index_dir=index_dir,
            index_type=index_type,
            verbose=verbose
        )


def run_traffic_embedding_pipeline(
    # æ•°æ®é›†è·¯å¾„
    dataset_path: str,
    # è¾“å‡ºè·¯å¾„
    embeddings_output_dir: str,
    index_output_dir: str,
    # æ¨¡å‹å‚æ•°
    llm: str = 'Qwen3-VL-Embedding-2B',
    projector: str = 'linear',
    linear_output_dim: int = 2048,
    # ç”Ÿæˆå‚æ•°
    batch_size: int = 8,
    save_threshold: int = 1000,
    normalize: bool = True,
    # ç´¢å¼•å‚æ•°
    index_type: str = 'flat',
    verbose: bool = True,
    # åŠ è½½å‚æ•°
    resume_log: bool = True,
    resume_encoder: str = None,
    resume_linear: str = None,
    resume_lora0: str = None
):
    """
    æµé‡åµŒå…¥ç”Ÿæˆæµæ°´çº¿ï¼šç”ŸæˆåµŒå…¥å‘é‡ + æ„å»ºå‘é‡ç´¢å¼•
    
    Args:
        dataset_path: æ•°æ®é›†ç›®å½•è·¯å¾„
        embeddings_output_dir: åµŒå…¥å‘é‡è¾“å‡ºç›®å½•
        index_output_dir: å‘é‡ç´¢å¼•è¾“å‡ºç›®å½•
        llm: LLM æ¨¡å‹è·¯å¾„
        train_mode: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
        eval_mode: æ˜¯å¦ä¸ºè¯„ä¼°æ¨¡å¼
        projector: æŠ•å½±å™¨ç±»å‹
        projector_arch: æŠ•å½±å™¨æ¶æ„
        adapter_path: é€‚é…å™¨è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        batch_size: æ‰¹å¤„ç†å¤§å°
        save_threshold: å­˜å‚¨é˜ˆå€¼
        normalize: æ˜¯å¦å½’ä¸€åŒ–
        index_type: ç´¢å¼•ç±»å‹ ('flat' æˆ– 'ivf')
        device: è®¾å¤‡
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    """
    from types import SimpleNamespace
    
    print("=" * 60)
    print("æµé‡åµŒå…¥ç”Ÿæˆæµæ°´çº¿")
    print("=" * 60)
    
    # æ„å»º args
    args = SimpleNamespace(
        llm=llm,
        projector=projector,
        linear_output_dim=linear_output_dim,
        resume_log=resume_log,
        resume_encoder=resume_encoder,
        resume_linear=resume_linear,
        resume_lora0=resume_lora0,
        eval_mode=True,
        train_mode=False
    )
    
    # åˆ›å»ºç”Ÿæˆå™¨
    generator = TrafficEmbeddingGenerator(args)
    
    # æ­¥éª¤ 1: ç”ŸæˆåµŒå…¥å‘é‡
    print("\nã€æ­¥éª¤ 1/2ã€‘ç”Ÿæˆæµé‡åµŒå…¥å‘é‡")
    print("-" * 60)
    generator.generate_embeddings(
        dataset_path=dataset_path,
        output_dir=embeddings_output_dir,
        batch_size=batch_size,
        save_threshold=save_threshold,
        normalize=normalize
    )
    
    # æ­¥éª¤ 2: æ„å»ºå‘é‡ç´¢å¼•
    print("\nã€æ­¥éª¤ 2/2ã€‘æ„å»ºå‘é‡ç´¢å¼•")
    print("-" * 60)
    generator.build_vector_index(
        embeddings_path=embeddings_output_dir,
        index_dir=index_output_dir,
        index_type=index_type,
        verbose=verbose
    )
    
    print("\n" + "=" * 60)
    print("âœ… æµé‡åµŒå…¥ç”Ÿæˆæµæ°´çº¿å®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    import fire
    fire.Fire()
