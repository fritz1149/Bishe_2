def _bert_tokenizer(vocab_path: str = "config/encryptd_vocab.txt"):
    # æ‡’åŠ è½½çš„tokenizer
    if not hasattr(_bert_tokenizer, "_cache"):
        import argparse
        parser = argparse.ArgumentParser(
            formatter_class=argparse.ArgumentDefaultsHelpFormatter)
        from uer.uer.opts import tokenizer_opts
        tokenizer_opts(parser)
        args = parser.parse_args([])  # ä½¿ç”¨ç©ºåˆ—è¡¨ï¼Œé‡‡ç”¨é»˜è®¤å‚æ•°è€Œéå‘½ä»¤è¡Œ
        args.vocab_path = vocab_path
        from uer.uer.utils import str2tokenizer
        args.tokenizer = str2tokenizer[args.tokenizer](args)
        args.vocab = args.tokenizer.vocab
        SEQ_LENGTH = 1500
        from uer.uer.utils import CLS_TOKEN, PAD_TOKEN
        PAD_ID = args.tokenizer.convert_tokens_to_ids([PAD_TOKEN])[0]
        CLS_ID = args.tokenizer.convert_tokens_to_ids([CLS_TOKEN])[0]
        _bert_tokenizer._cache = (args.tokenizer, SEQ_LENGTH, CLS_TOKEN, PAD_TOKEN, CLS_ID, PAD_ID)
    return _bert_tokenizer._cache

def _Qwen3VL_tokenizer(model_path: str = "./Qwen3-VL-8B-Instruct", SEQ_LENGTH: int = 1024):
    # æ‡’åŠ è½½çš„tokenizer
    if not hasattr(_Qwen3VL_tokenizer, "_cache"):
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        PAD_TOKEN = tokenizer.special_tokens_map.get("pad_token", None)
        PAD_ID = tokenizer.convert_tokens_to_ids([PAD_TOKEN])[0]
        _Qwen3VL_tokenizer._cache = (tokenizer, SEQ_LENGTH, None, PAD_TOKEN, None, PAD_ID)
    return _Qwen3VL_tokenizer._cache

def _Qwen3VL_embedder_tokenizer(model_path: str = "./Qwen3-VL-Embedding-2B", SEQ_LENGTH: int = 1024):
    if not hasattr(_Qwen3VL_embedder_tokenizer, "_cache"):
        from transformers import AutoTokenizer
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        PAD_TOKEN = tokenizer.special_tokens_map.get("pad_token", None)
        PAD_ID = tokenizer.convert_tokens_to_ids([PAD_TOKEN])[0]
        _Qwen3VL_embedder_tokenizer._cache = (tokenizer, SEQ_LENGTH, None, PAD_TOKEN, None, PAD_ID)
    return _Qwen3VL_embedder_tokenizer._cache

def _cut_bursts(in_path):
    bursts = []
    with open(in_path, "r", encoding="utf-8") as fin:
        lines = fin.readlines()
        src_ip = lines[0][:-1].split("\t")[src_index]
        current_burst = []
        payloads_in_current_burst = []
        payloads_index_in_current_burst = []
        i = 0
        for line in lines:
            values = line[:-1].split("\t")
            ip = values[src_index]
            if src_ip != ip:
                bursts.append({"packets": current_burst, "payloads": payloads_in_current_burst, "payloads_index": payloads_index_in_current_burst})
                current_burst = []
                payloads_in_current_burst = []
                payloads_index_in_current_burst = []
                src_ip = ip
                i = 0
            current_burst.append(line)
            payload = values[tcp_payload_index]
            if payload == "":
                payload = values[udp_payload_index]
            if payload != "":
                payloads_in_current_burst.append(payload)
                payloads_index_in_current_burst.append(i)
            i += 1
        if len(current_burst) > 0:
                bursts.append({"packets": current_burst, "payloads": payloads_in_current_burst, "payloads_index": payloads_index_in_current_burst})
        return bursts

def _dump_in_chunks(items, out_dir, chunk_size, name=""):
    import os
    import pickle
    os.makedirs(out_dir, exist_ok=True)
    idx = 0
    if chunk_size == -1:
        chunk_size = len(items)
    assert len(items) > 0
    for start in range(0, len(items), chunk_size):
        chunk = items[start:start + chunk_size]
        file_name = f"{name}_part_{idx:05d}.pkl"
        file_path = os.path.join(out_dir, file_name)
        with open(file_path, "wb") as fout:
            pickle.dump(chunk, fout)
        idx += 1

fields = ["frame.encap_type", "frame.time", "frame.offset_shift", "frame.time_epoch", "frame.time_delta",
                "frame.time_relative", "frame.number", "frame.len", "frame.marked", "frame.protocols", "eth.dst",
                "eth.dst_resolved", "eth.src", "eth.src_resolved", "eth.type",
                "ip.version", "ip.hdr_len", "ip.dsfield", "ip.dsfield.dscp", "ip.dsfield.ecn", "ip.len", "ip.id",
                "ip.flags", "ip.flags.rb", "ip.flags.df", "ip.flags.mf", "ip.frag_offset", "ip.ttl", "ip.proto",
                "ip.checksum", "ip.checksum.status", "ip.src", "ip.dst", "tcp.srcport", "tcp.dstport", "tcp.stream",
                "tcp.len", "tcp.seq", "tcp.nxtseq", "tcp.ack", "tcp.hdr_len", "tcp.flags",
                "tcp.flags.res", "tcp.flags.cwr", "tcp.flags.urg", "tcp.flags.ack",
                "tcp.flags.push", "tcp.flags.reset", "tcp.flags.syn", "tcp.flags.fin", "tcp.flags.str",
                "tcp.window_size", "tcp.window_size_scalefactor", "tcp.checksum", "tcp.checksum.status", "tcp.urgent_pointer",
                "tcp.time_relative", "tcp.time_delta", "tcp.analysis.bytes_in_flight", "tcp.analysis.push_bytes_sent", "tcp.segment",
                "tcp.segment.count", "tcp.reassembled.length", "tcp.payload", "udp.srcport", "udp.dstport", "udp.length",
                "udp.checksum", "udp.checksum.status", "udp.stream", "data.len", "udp.payload"]

src_index = fields.index("ip.src")
dst_index = fields.index("ip.dst")
ip_version_index = fields.index("ip.version")
tcp_payload_index = fields.index("tcp.payload")
udp_payload_index = fields.index("udp.payload")
tcp_segment_index = fields.index("tcp.segment")
tcp_segment_count_index = fields.index("tcp.segment.count")
frame_protocol_index = fields.index("frame.protocols")
udp_srcport_index = fields.index("udp.srcport")
udp_dstport_index = fields.index("udp.dstport")
tcp_srcport_index = fields.index("tcp.srcport")
tcp_dstport_index = fields.index("tcp.dstport")
tcp_field_indexes = [i for i, field in enumerate(fields) 
    if "payload" not in field and ("frame" in field or "eth" in field or "ip" in field or "tcp" in field)]
tcp_biased_avoid_field_indexes = [i for i, field in enumerate(fields) 
    if "payload" not in field and ("frame" in field or "ip" in field or "tcp" in field)]
udp_field_indexes = [i for i, field in enumerate(fields) 
    if "payload" not in field and ("frame" in field or "eth" in field or "ip" in field or "udp" in field or "data" in field)]
udp_biased_avoid_field_indexes = [i for i, field in enumerate(fields) 
    if "payload" not in field and ("frame" in field or "ip" in field or "udp" in field or "data" in field)]
# print([fields[i] for i in tcp_field_indexes])
# print([fields[i] for i in tcp_biased_avoid_field_indexes])
# print([fields[i] for i in udp_field_indexes])
# print([fields[i] for i in udp_biased_avoid_field_indexes])

def _bigram_generation(packet_datagram, packet_len=64):
    def cut(obj, sec):
        sec = sec % 4 + sec
        return [obj[i: i + sec] for i in range(0, len(obj), sec)]

    result = ""
    generated_datagram = cut(packet_datagram, 1)
    token_count = 0
    for sub_string_index in range(len(generated_datagram)):
        if sub_string_index != (len(generated_datagram) - 1):
            token_count += 1
            if token_count > packet_len:
                break
            else:
                merge_word_bigram = (
                    generated_datagram[sub_string_index]
                    + generated_datagram[sub_string_index + 1]
                )
        else:
            break
        result += merge_word_bigram
        result += " "

    return result

def _str_to_ids(text: str, seq_length: int = None, type: str = "bert", CLS_front: bool = False, padding: bool = False):
    if type == "bert":
        tokenizer, SEQ_LENGTH, CLS_TOKEN, PAD_TOKEN, CLS_ID, PAD_ID = _bert_tokenizer()
        if seq_length is None:
            seq_length = SEQ_LENGTH
    elif type == "qwen3vl":
        tokenizer, SEQ_LENGTH, CLS_TOKEN, PAD_TOKEN, CLS_ID, PAD_ID = _Qwen3VL_tokenizer()
        if seq_length is None:
            seq_length = SEQ_LENGTH
    elif type == "qwen3vl-emb":
        tokenizer, SEQ_LENGTH, CLS_TOKEN, PAD_TOKEN, CLS_ID, PAD_ID = _Qwen3VL_embedder_tokenizer()
        if seq_length is None:
            seq_length = SEQ_LENGTH
    # print(f"[æç¤º] å½“å‰ä½¿ç”¨çš„tokenizerç±»å‹: {type}, tokenizerå¯¹è±¡: {tokenizer}")
    tokens = tokenizer.tokenize(text)
    # è¾“å‡ºtokensçš„é•¿åº¦å’Œå†…å®¹ï¼Œä¾¿äºè°ƒè¯•
    # print(f"Token count: {len(tokens)}\nTokens: {tokens}")
    if CLS_front:
        tokens = [CLS_TOKEN] + tokens
    token_len = len(tokens)
    if padding:
        assert len(tokens) <= seq_length
        tokens = tokens + [PAD_TOKEN] * (seq_length - len(tokens))
    return tokenizer.convert_tokens_to_ids(tokens), token_len

def _ids_to_str(ids, type: str = "bert"):
    if type == "bert":
        tokenizer, SEQ_LENGTH, CLS_TOKEN, PAD_TOKEN, CLS_ID, PAD_ID = _bert_tokenizer()
    elif type == "qwen3vl":
        tokenizer, SEQ_LENGTH, CLS_TOKEN, PAD_TOKEN, CLS_ID, PAD_ID = _Qwen3VL_tokenizer()
    elif type == "qwen3vl-emb":
        tokenizer, SEQ_LENGTH, CLS_TOKEN, PAD_TOKEN, CLS_ID, PAD_ID = _Qwen3VL_embedder_tokenizer()
    return tokenizer.decode(ids, skip_special_tokens=False)

def _pad(token_ids_list, seqlen, type: str = "bert"):
    if type == "bert":
        tokenizer, SEQ_LENGTH, CLS_TOKEN, PAD_TOKEN, CLS_ID, PAD_ID = _bert_tokenizer()
    elif type == "qwen3vl":
        tokenizer, SEQ_LENGTH, CLS_TOKEN, PAD_TOKEN, CLS_ID, PAD_ID = _Qwen3VL_tokenizer()
    elif type == "qwen3vl-emb":
        tokenizer, SEQ_LENGTH, CLS_TOKEN, PAD_TOKEN, CLS_ID, PAD_ID = _Qwen3VL_embedder_tokenizer()
    ret = [token_ids + [PAD_ID] * (seqlen - len(token_ids)) for token_ids in token_ids_list]
    # for token_ids in ret:
    #     print(len(token_ids), end="  ")
    # print()
    return ret

def _random_normal_ip(ip_version, ip_that_could_not_be):
    """
    ç”Ÿæˆä¸€ä¸ªéšæœºçš„ã€éç‰¹æ®Šçš„IPåœ°å€ã€‚
    
    Args:
        ip_version: IPç‰ˆæœ¬ï¼Œ"4" æˆ– "6"
        ip_that_could_not_be: ä¸èƒ½ç”Ÿæˆçš„IPåœ°å€ï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼‰
        
    Returns:
        éšæœºç”Ÿæˆçš„éç‰¹æ®ŠIPåœ°å€ï¼ˆå­—ç¬¦ä¸²æ ¼å¼ï¼‰
    """
    import random
    import ipaddress
    
    # å°è¯•ç”ŸæˆIPçš„æœ€å¤§æ¬¡æ•°
    max_attempts = 10
    
    if ip_version == "4":
        # IPv4 éœ€è¦é¿å…çš„ç‰¹æ®Šåœ°å€èŒƒå›´
        forbidden_ranges = [
            ipaddress.IPv4Network("127.0.0.0/8"),      # å›ç¯åœ°å€
            ipaddress.IPv4Network("169.254.0.0/16"),   # é“¾è·¯æœ¬åœ°åœ°å€
            ipaddress.IPv4Network("224.0.0.0/4"),      # å¤šæ’­åœ°å€
            ipaddress.IPv4Network("0.0.0.0/8"),        # æœ¬ç½‘ç»œ
            ipaddress.IPv4Network("10.0.0.0/8"),       # ç§æœ‰åœ°å€
            ipaddress.IPv4Network("172.16.0.0/12"),    # ç§æœ‰åœ°å€
            ipaddress.IPv4Network("192.168.0.0/16"),   # ç§æœ‰åœ°å€
            ipaddress.IPv4Network("100.64.0.0/10"),    # Carrier-grade NAT
            ipaddress.IPv4Network("198.18.0.0/15"),    # Benchmark testing
            ipaddress.IPv4Network("240.0.0.0/4"),      # ä¿ç•™åœ°å€ï¼ˆEç±»ï¼‰
        ]
        
        # å…¬å…±DNSæœåŠ¡å™¨å’Œå…¶ä»–ç‰¹æ®ŠIP
        forbidden_ips = [
            "8.8.8.8",           # Google DNS
            "8.8.4.4",           # Google DNS
            "1.1.1.1",           # Cloudflare DNS
            "1.0.0.1",           # Cloudflare DNS
            "255.255.255.255",   # å¹¿æ’­åœ°å€
            "0.0.0.0",           # æœªæŒ‡å®šåœ°å€
        ]
        
        # æœ‰æ•ˆèŒƒå›´ï¼š1.0.0.1 åˆ° 223.255.255.254ï¼ˆæ’é™¤æ‰€æœ‰ç‰¹æ®Šåœ°å€åï¼‰
        # ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬ä»å…¬å…±åœ°å€æ± ä¸­éšæœºé€‰æ‹©
        # ä½¿ç”¨éšæœºç”Ÿæˆçš„åœ°å€ï¼Œç„¶åæ£€æŸ¥æ˜¯å¦åœ¨ç¦æ­¢èŒƒå›´å†…
        
        ip_that_could_not_be_obj = None
        if ip_that_could_not_be is not None:
            try:
                ip_that_could_not_be_obj = ipaddress.IPv4Address(ip_that_could_not_be)
            except ValueError:
                pass
        
        for _ in range(max_attempts):
            # ç”ŸæˆéšæœºIPï¼šåœ¨ 1.0.0.1 åˆ° 223.255.255.254 èŒƒå›´å†…
            # ä½†æ’é™¤å·²ç¦æ­¢çš„èŒƒå›´
            # ä½¿ç”¨å…¬å…±åœ°å€æ± ï¼š1.0.0.1 - 9.255.255.254, 11.0.0.1 - 126.255.255.254,
            # 128.0.0.1 - 168.253.255.254, 170.0.0.1 - 172.15.255.254, 
            # 172.32.0.1 - 192.167.255.254, 192.169.0.1 - 198.17.255.254,
            # 198.20.0.1 - 223.255.255.254
            parts = [random.randint(1, 223), random.randint(0, 255), 
                    random.randint(0, 255), random.randint(1, 254)]
            ip_str = ".".join(map(str, parts))
            ip = ipaddress.IPv4Address(ip_str)
            
            # æ£€æŸ¥æ˜¯å¦åœ¨ç¦æ­¢èŒƒå›´å†…
            in_forbidden = any(ip in network for network in forbidden_ranges)
            if in_forbidden:
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç¦æ­¢çš„å•ä¸ªIP
            if ip_str in forbidden_ips:
                continue
            
            # æ£€æŸ¥æ˜¯å¦ç­‰äº ip_that_could_not_be
            if ip_that_could_not_be_obj is not None and ip == ip_that_could_not_be_obj:
                continue
            
            return ip_str
            
        # å¦‚æœå°è¯•å¤šæ¬¡éƒ½å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªé»˜è®¤çš„å…¬å…±IP
        return "93.184.216.34"  # example.com çš„IP
        
    elif ip_version == "6":
        # IPv6 éœ€è¦é¿å…çš„ç‰¹æ®Šåœ°å€èŒƒå›´
        forbidden_ranges = [
            ipaddress.IPv6Network("::1/128"),              # å›ç¯åœ°å€
            ipaddress.IPv6Network("fe80::/10"),            # é“¾è·¯æœ¬åœ°åœ°å€
            ipaddress.IPv6Network("ff00::/8"),             # å¤šæ’­åœ°å€
            ipaddress.IPv6Network("::/128"),               # æœªæŒ‡å®šåœ°å€
            ipaddress.IPv6Network("::ffff:0:0/96"),        # IPv4æ˜ å°„åœ°å€
            ipaddress.IPv6Network("2001:db8::/32"),        # æ–‡æ¡£åœ°å€
            ipaddress.IPv6Network("2001:10::/28"),         # ORCHID
            ipaddress.IPv6Network("fc00::/7"),             # å”¯ä¸€æœ¬åœ°åœ°å€
        ]
        
        # å…¬å…±DNSæœåŠ¡å™¨
        forbidden_ips = [
            "2001:4860:4860::8888",      # Google DNS
            "2001:4860:4860::8844",      # Google DNS
            "2606:4700:4700::1111",      # Cloudflare DNS
            "2606:4700:4700::1001",      # Cloudflare DNS
        ]
        
        ip_that_could_not_be_obj = None
        if ip_that_could_not_be is not None:
            try:
                ip_that_could_not_be_obj = ipaddress.IPv6Address(ip_that_could_not_be)
            except ValueError:
                pass
        
        for _ in range(max_attempts):
            # ç”ŸæˆéšæœºIPv6åœ°å€ï¼ˆå…¨çƒå•æ’­åœ°å€èŒƒå›´ï¼š2000::/3ï¼‰
            # ä½†æ’é™¤å·²ç¦æ­¢çš„èŒƒå›´
            # ç”Ÿæˆ 2000:: åˆ° 3fff:ffff:ffff:ffff:ffff:ffff:ffff:ffff èŒƒå›´å†…çš„åœ°å€
            # ä½†æ’é™¤ 2001:db8::/32, 2001:10::/28
            # ç®€åŒ–ï¼šç”Ÿæˆéšæœº8ä¸ª16ä½æ•°å­—æ®µ
            parts_hex = []
            # ç¬¬ä¸€ä¸ªæ®µï¼š2000-3fff
            first_seg = random.randint(0x2000, 0x3fff)
            parts_hex.append(first_seg)
            
            # ç¬¬äºŒä¸ªæ®µ
            if first_seg == 0x2001:
                # å¦‚æœç¬¬ä¸€ä¸ªæ®µæ˜¯2001ï¼Œç¬¬äºŒä¸ªæ®µéœ€è¦æ’é™¤ db8-dbff (2001:db8::/32) å’Œ 10 (2001:10::/28)
                # 2001:10::/28 æ„å‘³ç€ç¬¬äºŒä¸ªæ®µåœ¨ 10-1f èŒƒå›´å†…ï¼ˆä½†/28å‰ç¼€æ„å‘³ç€ç¬¬äºŒä¸ªæ®µçš„ä½4ä½å¯ä»¥æ˜¯ä»»æ„å€¼ï¼‰
                # å®é™…ä¸Š 2001:10::/28 è¦†ç›–äº† 2001:0010:0000:0000:0000:0000:0000:0000 åˆ° 2001:001f:ffff:ffff:ffff:ffff:ffff:ffff
                # æ‰€ä»¥ç¬¬äºŒä¸ªæ®µéœ€è¦æ’é™¤ 0x0010-0x001f
                # ä»¥åŠ 2001:db8::/32 æ„å‘³ç€ç¬¬äºŒä¸ªæ®µéœ€è¦æ’é™¤ 0xdb8-0xdbff (ä½†/32å‰ç¼€æ„å‘³ç€åé¢6ä¸ªæ®µéƒ½æ˜¯0)
                # å®é™…ä¸Šä¸ºäº†å®‰å…¨ï¼Œæˆ‘ä»¬æ’é™¤ç¬¬äºŒä¸ªæ®µä¸º 0xdb8-0xdbff çš„æ‰€æœ‰åœ°å€
                second_seg = random.choice([
                    random.randint(0, 0x0f),      # 0-0f (æ’é™¤ 10-1f)
                    random.randint(0x20, 0xdb7),  # 20-db7 (æ’é™¤ db8-dbff)
                    random.randint(0xdc0, 0xffff)  # dc0-ffff
                ])
            else:
                second_seg = random.randint(0, 0xffff)
            parts_hex.append(second_seg)
            
            # å…¶ä½™6ä¸ªæ®µ
            for i in range(6):
                parts_hex.append(random.randint(0, 0xffff))
            
            # è½¬æ¢ä¸ºå­—ç¬¦ä¸²æ ¼å¼
            parts = [f"{val:x}" for val in parts_hex]
            ip_str = ":".join(parts)
            ip = ipaddress.IPv6Address(ip_str)
            
            # æ£€æŸ¥æ˜¯å¦åœ¨ç¦æ­¢èŒƒå›´å†…
            in_forbidden = any(ip in network for network in forbidden_ranges)
            if in_forbidden:
                continue
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯ç¦æ­¢çš„å•ä¸ªIP
            if ip_str.lower() in [f.lower() for f in forbidden_ips]:
                continue
            
            # æ£€æŸ¥æ˜¯å¦ç­‰äº ip_that_could_not_be
            if ip_that_could_not_be_obj is not None and ip == ip_that_could_not_be_obj:
                continue
            
            return ip_str
            
        # å¦‚æœå°è¯•å¤šæ¬¡éƒ½å¤±è´¥ï¼Œè¿”å›ä¸€ä¸ªé»˜è®¤çš„å…¬å…±IPv6
        return "2606:2800:220:1:248:1893:25c8:1946"  # example.com çš„IPv6
        
    else:
        print(f"Warning: Unsupported IP version: {ip_version}")
        return _random_normal_ip("4", ip_that_could_not_be)

def random_field(bits):
    import random
    field_max = 2**bits-1
    field_int = random.randint(0, field_max)
    return field_int

fields_ids = [_str_to_ids(f"<{field}>", None, "qwen3vl", False, False)[0] for field in fields]
payload_ids = _str_to_ids("<payload>", None, "qwen3vl", False, False)[0]
tcp_payload_ids = _str_to_ids("<tcp.payload>", None, "qwen3vl", False, False)[0]
udp_payload_ids = _str_to_ids("<udp.payload>", None, "qwen3vl", False, False)[0]
def _build_table(lines, payloads, flow_type, extract_payloads_from_lines: bool = False, shuffle_columns: bool = False, random_drop_columns: bool = False, biased_avoid:bool = False, token_type: str = "qwen3vl", payload_token_type: str = "bert", payload_flatten: bool = False, payload_flatten_prefix_len: int | None = None):
    """
    æ„å»ºè¡¨æ ¼æ•°æ®
    
    Args:
        token_type: ç”¨äºè¡¨æ ¼å­—æ®µå’Œæ ‡è®°çš„tokenizerç±»å‹ï¼Œé»˜è®¤ä¸º "qwen3vl"
        payload_token_type: ç”¨äºpayloadå†…å®¹çš„tokenizerç±»å‹ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨token_typeï¼Œé»˜è®¤ä¸ºNone
        payload_flatten: æ˜¯å¦å°†payloadå†…å®¹ç›´æ¥å¹³é“ºè¿›è¡¨æ ¼å•å…ƒæ ¼ï¼ˆä¸å†ä½¿ç”¨å ä½ç¬¦ + payload_bert_idsï¼‰
        payload_flatten_prefix_len: payload_flatten å¯ç”¨æ—¶å¯é€‰ï¼Œä»…å–payloadå‰ç¼€æŒ‡å®šé•¿åº¦
    """
    
    # æ ¹æ®token_typeåŠ¨æ€ç”Ÿæˆå­—æ®µIDs
    fields_ids_local = [_str_to_ids(f"<{field}>", None, token_type, False, False)[0] for field in fields]
    payload_ids_local = _str_to_ids("<payload>", None, token_type, False, False)[0]
    tcp_payload_ids_local = _str_to_ids("<tcp.payload>", None, token_type, False, False)[0]
    udp_payload_ids_local = _str_to_ids("<udp.payload>", None, token_type, False, False)[0]
    
    table_columnwise = []
    payload_ids_ = None
    if extract_payloads_from_lines:
        payloads = []
    if flow_type is None:
        value0 = lines[0].split("\t")
        if value0[tcp_payload_index] != "":
            flow_type = "TCP"
        elif value0[udp_payload_index] != "":
            flow_type = "UDP"
        else:
            raise ValueError(f"Unknown protocol type: {flow_type}")
    if flow_type == "TCP":
        payload_ids_ = tcp_payload_ids_local
        valid_indexes = tcp_field_indexes if not biased_avoid else tcp_biased_avoid_field_indexes
        payload_index = tcp_payload_index
        src_port_index = tcp_srcport_index
        dst_port_index = tcp_dstport_index
    elif flow_type == "UDP":
        payload_ids_ = udp_payload_ids_local
        valid_indexes = udp_field_indexes if not biased_avoid else udp_biased_avoid_field_indexes
        payload_index = udp_payload_index
        src_port_index = udp_srcport_index
        dst_port_index = udp_dstport_index
    else:
        raise ValueError(f"Unknown protocol type: {flow_type}")
    if lines is not None:
        table_columnwise = [[fields_ids_local[i]] for i in valid_indexes]
        if biased_avoid:
            ip_dict = {}
            port_dict = {}
        for line in lines:
            values = line.split("\t")
            if biased_avoid:
                ip_src = values[src_index]
                port_src = values[src_port_index]
                ip_dst = values[dst_index]
                port_dst = values[dst_port_index]
                ip_version = values[ip_version_index]
                ip_src_mask = ip_dict.get(ip_src, None)
                ip_dst_mask = ip_dict.get(ip_dst, None)
                port_src_mask = port_dict.get(port_src, None)
                port_dst_mask = port_dict.get(port_dst, None)
                if port_src_mask is None:
                    port_src_mask = random_field(16)
                    port_dict[port_src] = port_src_mask
                if port_dst_mask is None:
                    port_dst_mask = random_field(16)
                    port_dict[port_dst] = port_dst_mask
                if ip_src_mask is None:
                    ip_src_mask = _random_normal_ip(ip_version, ip_dst_mask)
                    ip_dict[ip_src] = ip_src_mask
                if ip_dst_mask is None:
                    ip_dst_mask = _random_normal_ip(ip_version, ip_src_mask)
                    ip_dict[ip_dst] = ip_dst_mask
                values[src_index] = ip_src_mask
                values[dst_index] = ip_dst_mask
                values[src_port_index] = port_src_mask
                values[dst_port_index] = port_dst_mask
            if extract_payloads_from_lines:
                payloads.append(values[payload_index])
            idx = 0
            for i in valid_indexes:
                field_ids = _str_to_ids(f"<{values[i]}>", type=token_type)[0]
                table_columnwise[idx].append(field_ids)
                idx += 1
        if shuffle_columns:
            import random
            random.shuffle(table_columnwise)
        if random_drop_columns:
            import random
            import random
            col_total = len(table_columnwise)
            remain_num = random.randint(col_total // 2, col_total)
            table_columnwise = table_columnwise[:remain_num]
    # print(f"payloads is None: {payloads is None}, payloads: {payloads}")
    payload_bert_ids = []
    if payloads is not None:
        table_columnwise.append([payload_ids_])
        for payload in payloads:
            if len(payload) == 0:
                table_columnwise[-1].append(_str_to_ids(f"<>", type=token_type)[0])
            else:
                if payload_flatten:
                    payload_flatten_text = payload if payload_flatten_prefix_len is None else payload[:payload_flatten_prefix_len]
                    payload_flatten_text = f"<{payload_flatten_text}>"
                    table_columnwise[-1].append(_str_to_ids(payload_flatten_text, type=token_type)[0])
                else:
                    table_columnwise[-1].append(_str_to_ids(f"<<|image_pad|>>", type=token_type)[0])

                    payload_ids, valid_len = _str_to_ids(payload, 1500, payload_token_type, True, True)
                    attention_mask_payload = [1]*valid_len+[0]*(1500-valid_len)
                    global_attention_mask_payload = [0]*1500
                    global_attention_mask_payload[0] = 1
                    payload_bert_ids.append((payload_ids, attention_mask_payload, global_attention_mask_payload))

    return table_columnwise, payload_bert_ids

def _build_kv_flow(lines, payloads, flow_type, extract_payloads_from_lines: bool = False, biased_avoid: bool = False, token_type: str = "qwen3vl", payload_token_type: str = "bert", payload_placeholder: str = "<tool_call>", payload_max_len: int = 1500, 
    packet_separator: str = "<pck>"):
    """æ„å»ºéè¡¨æ ¼æµé‡è¡¨å¾ + payload å‹ç¼©è¡¨å¾ã€‚

    traffic éƒ¨åˆ†ï¼šæ¯ä¸ªpacketç”Ÿæˆ ", " è¿æ¥çš„ "key: value" æ–‡æœ¬ï¼ˆä¸ä½¿ç”¨ <> åŒ…è£¹ï¼‰ã€‚
    payload éƒ¨åˆ†ï¼šåœ¨ ids ä¸­ä½¿ç”¨ payload_placeholderï¼ˆä¸é¢å¤–ç”¨ << >> åŒ…è£¹ï¼‰ï¼Œå¹¶ç”Ÿæˆ bert è¾“å…¥ã€‚
    """
    assert extract_payloads_from_lines == True
    payload_ids_ = None
    if extract_payloads_from_lines:
        payloads = []
    if flow_type is None:
        value0 = lines[0].split("\t")
        if value0[tcp_payload_index] != "":
            flow_type = "TCP"
        elif value0[udp_payload_index] != "":
            flow_type = "UDP"
        else:
            raise ValueError(f"Unknown protocol type: {flow_type}")
    if flow_type == "TCP":
        payload_name_ = _str_to_ids("tcp.payload", None, token_type, False, False)[0]
        valid_indexes = tcp_field_indexes if not biased_avoid else tcp_biased_avoid_field_indexes
        payload_index = tcp_payload_index
        src_port_index = tcp_srcport_index
        dst_port_index = tcp_dstport_index
    elif flow_type == "UDP":
        payload_name_ = _str_to_ids("udp.payload", None, token_type, False, False)[0]
        valid_indexes = udp_field_indexes if not biased_avoid else udp_biased_avoid_field_indexes
        payload_index = udp_payload_index
        src_port_index = udp_srcport_index
        dst_port_index = udp_dstport_index
    else:
        raise ValueError(f"Unknown protocol type: {flow_type}")

    payload_bert_ids: list[tuple[list[int], list[int], list[int]]] = []

    if biased_avoid:
        ip_dict = {}
        port_dict = {}

    for line_idx, line in enumerate(lines):
        values = line.split("\t")
        if biased_avoid:
            ip_src = values[src_index]
            port_src = values[src_port_index]
            ip_dst = values[dst_index]
            port_dst = values[dst_port_index]
            ip_version = values[ip_version_index]
            ip_src_mask = ip_dict.get(ip_src, None)
            ip_dst_mask = ip_dict.get(ip_dst, None)
            port_src_mask = port_dict.get(port_src, None)
            port_dst_mask = port_dict.get(port_dst, None)
            if port_src_mask is None:
                port_src_mask = random_field(16)
                port_dict[port_src] = port_src_mask
            if port_dst_mask is None:
                port_dst_mask = random_field(16)
                port_dict[port_dst] = port_dst_mask
            if ip_src_mask is None:
                ip_src_mask = _random_normal_ip(ip_version, ip_dst_mask)
                ip_dict[ip_src] = ip_src_mask
            if ip_dst_mask is None:
                ip_dst_mask = _random_normal_ip(ip_version, ip_src_mask)
                ip_dict[ip_dst] = ip_dst_mask
            values[src_index] = ip_src_mask
            values[dst_index] = ip_dst_mask
            values[src_port_index] = port_src_mask
            values[dst_port_index] = port_dst_mask

        payload_from_line = values[payload_index]
        if extract_payloads_from_lines:
            payloads.append(payload_from_line)

        parts = []
        for i in valid_indexes:
            key = fields[i]
            val = values[i]
            parts.append(f"{key}: {val}")
        traffic_text = ", ".join(parts)
        if payloads is None:
            payload_text = ""
        else:
            payload_text = payloads[line_idx] if line_idx < len(payloads) else ""

        if len(payload_text) == 0:
            packet_text = f"{traffic_text}, {payload_name_}: ç©º"
        else:
            packet_text = f"{traffic_text}, {payload_name_}: {payload_placeholder}"

            payload_ids, valid_len = _str_to_ids(payload_text, payload_max_len, payload_token_type, True, True)
            attention_mask_payload = [1] * valid_len + [0] * (payload_max_len - valid_len)
            global_attention_mask_payload = [0] * payload_max_len
            global_attention_mask_payload[0] = 1
            payload_bert_ids.append((payload_ids, attention_mask_payload, global_attention_mask_payload))

        packet_texts.append(packet_text)

    flow_text = packet_separator.join(packet_texts)
    flow_ids = _str_to_ids(flow_text, None, token_type, False, False)[0]
    return flow_ids, payload_bert_ids

def _position_ids_flat(seq_len: int):
    import torch
    pos = torch.arange(seq_len).unsqueeze(0).expand(3, -1)
    return pos

def _position_ids(prompt_ids, prompt2_ids, table, label_ids):
    import torch
    prompt_position_ids = torch.arange(len(prompt_ids)).unsqueeze(0).expand(3, -1)
    table_start = prompt_position_ids.max().item()+1 if len(prompt_ids) > 0 else 1
    col_start = table_start
    table_position_ids = []
    h = len(table[0])
    for i, column in enumerate(table):
        seq_len_col = 0
        for cell in column:
            seq_len_col = max(seq_len_col, len(cell))
        pos_0 = torch.full((h, seq_len_col), table_start, dtype=torch.long)
        pos_1 = torch.arange(h).view(-1, 1).expand(-1, seq_len_col)+table_start
        pos_2 = torch.arange(seq_len_col).view(1, -1).expand(h, -1)+col_start
        pos = torch.stack([pos_0, pos_1, pos_2], dim=0).view(3, -1)  # [3, h * seq_len_col]
        to_remove_indexes = []
        for j, cell in enumerate(column):
            len_cell = len(cell)
            if len_cell < seq_len_col:
                to_remove_indexes.extend(list(range(j*seq_len_col+len_cell, (j+1)*seq_len_col)))
        # if i == len(table)-1:
        #     print(f"seq_len_col: {seq_len_col}, to_remove_indexes: {to_remove_indexes}")
        if len(to_remove_indexes) > 0:
            to_remove_indexes = torch.tensor(to_remove_indexes, dtype=torch.long, device=pos.device)
            mask = torch.ones(pos.shape[1], dtype=torch.bool, device=pos.device)
            mask[to_remove_indexes] = False
            pos = pos[:, mask]
        table_position_ids.append(pos)
        col_start += seq_len_col
    labels_start = table_position_ids[-1].max().item()+1
    labels_position_ids = torch.arange(len(prompt2_ids)+len(label_ids)).unsqueeze(0).expand(3, -1) + labels_start
    return torch.cat([prompt_position_ids, torch.cat(table_position_ids, dim=1), labels_position_ids], dim=1)

def _flat_table(table):
    return [item for column in table for cell in column for item in cell]

def _LM_input(lines, payloads, flow_type, label_ids, prompt_ids, prompt2_ids, label = None, extract_payloads_from_lines=False, shuffle_columns=False, random_drop_columns=False, biased_avoid=False,
    _build_table_result=None, token_type: str = "qwen3vl", payload_token_type: str = "bert", payload_flatten: bool = False, payload_flatten_prefix_len: int | None = None):
    """
    ç”Ÿæˆè¯­è¨€æ¨¡å‹è¾“å…¥æ ·æœ¬
    
    Args:
        token_type: ç”¨äºè¡¨æ ¼å­—æ®µå’Œæ ‡è®°çš„tokenizerç±»å‹ï¼Œé»˜è®¤ä¸º "qwen3vl"
        payload_token_type: ç”¨äºpayloadå†…å®¹çš„tokenizerç±»å‹ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨token_typeï¼Œé»˜è®¤ä¸ºNone
    """
    if _build_table_result is None:
        table, payload_bert_ids = _build_table(
            lines,
            payloads,
            flow_type,
            extract_payloads_from_lines,
            shuffle_columns,
            random_drop_columns,
            biased_avoid,
            token_type=token_type,
            payload_token_type=payload_token_type,
            payload_flatten=payload_flatten,
            payload_flatten_prefix_len=payload_flatten_prefix_len,
        )
    else:
        table, payload_bert_ids = _build_table_result
    position_ids = _position_ids(prompt_ids, prompt2_ids, table, label_ids)
    table_ids = _flat_table(table)
    assert len(prompt_ids)+len(table_ids)+len(prompt2_ids)+len(label_ids) == position_ids.shape[1]
    sample = {
        "data": (prompt_ids+table_ids+prompt2_ids, label_ids, payload_bert_ids, position_ids),
        "label": label
    }
    return sample

def _LM_input_kv(lines, payloads, flow_type, label_ids, prompt_ids, prompt2_ids, label=None, extract_payloads_from_lines: bool = True, biased_avoid: bool = False,
    token_type: str = "qwen3vl", payload_token_type: str = "bert", payload_placeholder: str = "<|image_pad|>", payload_max_len: int = 1500):
    """ç”Ÿæˆéè¡¨æ ¼æµé‡è¡¨å¾ + payload å‹ç¼©è¡¨å¾çš„è¯­è¨€æ¨¡å‹è¾“å…¥æ ·æœ¬ã€‚

    - traffic: ", " è¿æ¥çš„ "key: value" æ–‡æœ¬ã€‚
    - payload: ids ä¸­ä½¿ç”¨ payload_placeholderï¼ŒåŒæ—¶ç”Ÿæˆ bert è¾“å…¥ (payload_bert_ids)ã€‚
    """
    flow_ids, payload_bert_ids = _build_kv_flow(
        lines,
        payloads,
        flow_type,
        extract_payloads_from_lines=extract_payloads_from_lines,
        biased_avoid=biased_avoid,
        token_type=token_type,
        payload_token_type=payload_token_type,
        payload_placeholder=payload_placeholder,
        payload_max_len=payload_max_len,
    )
    seq_len = len(prompt_ids) + len(flow_ids) + len(prompt2_ids) + len(label_ids)
    position_ids = _position_ids_flat(seq_len)
    sample = {
        "data": (prompt_ids + flow_ids + prompt2_ids, label_ids, payload_bert_ids, position_ids),
        "label": label,
    }
    return sample

def _lines_simplify(lines: list[str]):
    pass

def generate_id2label(tmp_path: str, dest_path: str):
    import os
    import json
    with open(os.path.join(tmp_path, "label_name_to_id.json"), "r", encoding="utf-8") as f:
        label2id = json.load(f)
    id2label = {str(v): k for k, v in label2id.items()}
    with open(os.path.join(dest_path, "id2label.json"), "w", encoding="utf-8") as f:
        json.dump(id2label, f, ensure_ascii=False, indent=2)

def _cut_stats_list(a: list[tuple]):
    a = sorted(a, key=lambda x: x[0])
    bins = []
    for num, cnt in a[:10]:
        bins.append((num, num, cnt))

    g10_cnt = sum(cnt for _, cnt in a[10:])
    bin_size = g10_cnt // 10
    tmp_cnt = None
    tmp_left = None
    for num, cnt in a[10:]:
        if tmp_cnt is None:
            tmp_left = num
            tmp_cnt = cnt
        else:
            tmp_cnt += cnt
        if tmp_cnt >= bin_size:
            bins.append((tmp_left, num, tmp_cnt))
            tmp_cnt = None
            tmp_left = None
    if tmp_cnt is not None:
        bins.append((tmp_left, a[-1][0], tmp_cnt))
    return bins

def check_tmp(tmp_path: str):
    """
    æ£€æŸ¥ generate_classify_tmp ç”Ÿæˆçš„sqliteä¸­é—´æ–‡ä»¶ï¼Œè¾“å‡ºç»Ÿè®¡ä¿¡æ¯ã€‚
    Args:
        tmp_path: generate_classify_tmp ç”Ÿæˆçš„ä¸´æ—¶æ–‡ä»¶è·¯å¾„
    """
    import os
    import sqlite3

    if not os.path.exists(tmp_path):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {tmp_path}")
        return

    print("=" * 70)
    print("ğŸ“Š generate_classify_tmp sqliteä¸­é—´æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯")
    print("=" * 70)

    from .model import connect_to_dbs, close_dbs, execute_sql_on_dbs
    conns = connect_to_dbs(tmp_path)

    # ç»Ÿè®¡ä¸»è¦è¡¨çš„æ•°é‡
    flow_count = sum(execute_sql_on_dbs(conns, "SELECT COUNT(*) FROM flows", unpack=True))
    burst_count = sum(execute_sql_on_dbs(conns, "SELECT COUNT(*) FROM bursts", unpack=True))
    packet_count = sum(execute_sql_on_dbs(conns, "SELECT COUNT(*) FROM packets", unpack=True))
    payload_count = sum(execute_sql_on_dbs(conns, "SELECT COUNT(*) FROM payloads", unpack=True))
    label_count = len(execute_sql_on_dbs(conns, "SELECT id FROM labels", unpack=True))

    print(f"\n1ï¸âƒ£ flows è¡¨: {flow_count}")
    print(f"2ï¸âƒ£ bursts è¡¨: {burst_count}")
    print(f"3ï¸âƒ£ packets è¡¨: {packet_count}")
    print(f"4ï¸âƒ£ payloads è¡¨: {payload_count}")
    print(f"5ï¸âƒ£ labels è¡¨: {label_count}")

    # flowsè¡¨ label åˆ†å¸ƒ
    print(f"\n   â€¢ flowsæ ‡ç­¾åˆ†å¸ƒï¼ˆå‰10ï¼‰:")
    flows = execute_sql_on_dbs(conns, "SELECT label, COUNT(*) FROM flows GROUP BY label ORDER BY COUNT(*)")
    # INSERT_YOUR_CODE
    # labelç›¸åŒçš„åˆå¹¶ï¼Œå¹¶å–å‰10
    merged_flows = {}
    for lbl, cnt in flows:
        if lbl in merged_flows:
            merged_flows[lbl] += cnt
        else:
            merged_flows[lbl] = cnt
    # æ ¹æ®æ•°é‡é™åºæ’åºå¹¶å–å‰10
    flows = sorted(merged_flows.items(), key=lambda x: x[1], reverse=True)[:10]
    for lbl, cnt in flows:
        print(f"      - {lbl}: {cnt}")
    # flowsçš„payload_ge1_burst_numåˆ†å¸ƒ
    print(f"\n   â€¢ flowsçš„payload_ge1_burst_numåˆ†å¸ƒï¼ˆå‰10ï¼‰:")
    try:
        flows = execute_sql_on_dbs(conns, "SELECT payload_ge1_burst_num, COUNT(*) FROM flows GROUP BY payload_ge1_burst_num ORDER BY COUNT(*)")
        # INSERT_YOUR_CODE
        # payload_ge1_burst_numç›¸åŒçš„åˆå¹¶ï¼Œå¹¶å–å‰10
        merged_flows = {}
        for num, cnt in flows:
            if num in merged_flows:
                merged_flows[num] += cnt
            else:
                merged_flows[num] = cnt
        # æ ¹æ®æ•°é‡é™åºæ’åºå¹¶å–å‰10
        # flows = sorted(merged_flows.items(), key=lambda x: x[1], reverse=True)[:10]
        flows = _cut_stats_list(merged_flows.items())
        for min_num, max_num, cnt_sum in flows:
            print(f"      - payload_ge1_burst_num={min_num} ~ {max_num}: {cnt_sum}")
    except Exception as e:
        print(f"      - è·å–flowsçš„payload_ge1_burst_numåˆ†å¸ƒå¼‚å¸¸: {e}")

    # INSERT_YOUR_CODE
    print(f"\n   â€¢ flowsçš„packetæ•°é‡åˆ†å¸ƒï¼ˆå‰10ï¼‰:")
    try:
        # è·å–æ¯ä¸ªflowçš„packetæ•°é‡
        flow_packet_counts = execute_sql_on_dbs(
            conns,
            """SELECT SUM(packet_count) AS pkt_num
                FROM bursts
                JOIN burst_packet_count ON bursts.id = burst_packet_count.burst_id
                GROUP BY bursts.flow_id
            """)
        # ç»Ÿè®¡å„packetæ•°é‡çš„åˆ†å¸ƒ
        packet_num_distribution = {}
        for pkt_num in flow_packet_counts:
            if pkt_num in packet_num_distribution:
                packet_num_distribution[pkt_num] += 1
            else:
                packet_num_distribution[pkt_num] = 1
        # å–å‰10é«˜é¢‘åˆ†å¸ƒ
        packet_num_distribution_sorted = _cut_stats_list(packet_num_distribution.items())
        for min_num, max_num, cnt_sum in packet_num_distribution_sorted:
            print(f"      - packet_num={min_num} ~ {max_num}: {cnt_sum}")
    except Exception as e:
        print(f"      - è·å–flowsçš„packetæ•°é‡åˆ†å¸ƒå¼‚å¸¸: {e}")

    # bursts è¡¨å«payloadçš„æ•°é‡
    print(f"\n   â€¢ å«æœ‰payloadçš„burstæ•°é‡ï¼ˆpayload_num>0ï¼‰:")
    burst_payload_num = sum(execute_sql_on_dbs(conns, "SELECT COUNT(*) FROM bursts WHERE payload_num > 0", unpack=True))
    if burst_count > 0:
        percent = burst_payload_num / burst_count * 100
        print(f"      - {burst_payload_num} ({percent:.1f}%)")
    else:
        print(f"      - 0 (0%)")
    # å«payload>=2çš„burstæ•°é‡
    print(f"\n   â€¢ å«æœ‰payloadæ•°é‡â‰¥2çš„burstæ•°é‡ (payload_num>=2):")
    burst_payload_ge2_num = sum(execute_sql_on_dbs(conns, "SELECT COUNT(*) FROM bursts WHERE payload_num >= 2", unpack=True))
    if burst_count > 0:
        percent_ge2 = burst_payload_ge2_num / burst_count * 100
        print(f"      - {burst_payload_ge2_num} ({percent_ge2:.1f}%)")
    else:
        print(f"      - 0 (0%)")

    # burstsè¡¨ label åˆ†å¸ƒ
    print(f"\n   â€¢ burstsæ ‡ç­¾åˆ†å¸ƒï¼ˆå‰10ï¼‰:")
    bursts = execute_sql_on_dbs(conns, "SELECT label, COUNT(*) FROM bursts GROUP BY label ORDER BY COUNT(*)")
    # INSERT_YOUR_CODE
    # labelç›¸åŒçš„åˆå¹¶ï¼Œå¹¶å–å‰10
    merged_bursts = {}
    for lbl, cnt in bursts:
        if lbl in merged_bursts:
            merged_bursts[lbl] += cnt
        else:
            merged_bursts[lbl] = cnt
    # æ ¹æ®æ•°é‡é™åºæ’åºå¹¶å–å‰10
    bursts = sorted(merged_bursts.items(), key=lambda x: x[1], reverse=True)[:10]
    for lbl, cnt in bursts:
        print(f"      - {lbl}: {cnt}")

    # INSERT_YOUR_CODE
    print(f"\n   â€¢ burst payload_numåˆ†å¸ƒï¼ˆå‰10ï¼‰:")
    try:
        burst_payloads = execute_sql_on_dbs(conns, "SELECT payload_num, COUNT(*) FROM bursts GROUP BY payload_num ORDER BY COUNT(*)")
        # payload_numç›¸åŒçš„åˆå¹¶ï¼Œå¹¶å–å‰10
        merged_burst_payloads = {}
        for num, cnt in burst_payloads:
            if num in merged_burst_payloads:
                merged_burst_payloads[num] += cnt
            else:
                merged_burst_payloads[num] = cnt
        # æ ¹æ®æ•°é‡é™åºæ’åºå¹¶å–å‰10
        burst_payloads_sorted = _cut_stats_list(merged_burst_payloads.items())
        for min_num, max_num, cnt_sum in burst_payloads_sorted:
            print(f"      - payload_num={min_num} ~ {max_num}: {cnt_sum}")
    except Exception as e:
        print(f"      - è·å–burstsçš„payload_numåˆ†å¸ƒå¼‚å¸¸: {e}")

    # INSERT_YOUR_CODE
    # ç»Ÿè®¡payloadæ•°é‡>=3ã€packetæ•°é‡<=10çš„burstsæ•°é‡
    try:
        burst_payload_ge3_packet_le10_count = sum(execute_sql_on_dbs(
            conns,
            """
            SELECT COUNT(*) 
            FROM bursts JOIN burst_packet_count ON bursts.id = burst_packet_count.burst_id
            WHERE bursts.payload_num >= 3 AND burst_packet_count.packet_count <= 10
            """,
            unpack=True
        ))
        print(f"\n   â€¢ æ»¡è¶³ payload_num>=3 ä¸” packet_count<=10 çš„burstæ•°é‡: {burst_payload_ge3_packet_le10_count}")
    except Exception as e:
        print(f"      - è·å– payload_num>=3 ä¸” packet_count<=10 çš„burstæ•°é‡å¼‚å¸¸: {e}")

    # payloadsè¡¨é•¿åº¦ç»Ÿè®¡
    print(f"\n   â€¢ payloadé•¿åº¦åˆ†å¸ƒ:")
    try:
        lens = execute_sql_on_dbs(conns, "SELECT LENGTH(content) FROM payloads", unpack=True)
        lens_dict = {}
        for len_ in lens:
            if len_ in lens_dict:
                lens_dict[len_] += 1
            else:
                lens_dict[len_] = 1
        lens_sorted = _cut_stats_list(lens_dict.items())
        for min_len, max_len, cnt_sum in lens_sorted:
            print(f"      - len={min_len} ~ {max_len}: {cnt_sum}")
    except Exception as e:
        print(f"      - è·å–payloadé•¿åº¦ç»Ÿè®¡å¼‚å¸¸: {e}")

    # æ¯ä¸ª label çš„ç»Ÿè®¡ä¿¡æ¯
    print(f"\n6ï¸âƒ£ å„æ ‡ç­¾ç»Ÿè®¡ï¼ˆlabelå | flows | packets | å«payload packets | burstsï¼‰:")
    # INSERT_YOUR_CODE
    import os
    import json
    # å‡è®¾å·²å®šä¹‰tmp_pathå˜é‡ï¼ŒæŒ‡å‘tmpç›®å½•
    label_name_to_id_path = os.path.join(tmp_path, "label_name_to_id.json")
    with open(label_name_to_id_path, "r", encoding="utf-8") as f:
        label_name_to_id = json.load(f)
    # æ„å»ºlabels: list of (label_id, label_name)ï¼ŒæŒ‰idå‡åºæ’åˆ—
    labels = [(int(label_id), label_name) for label_name, label_id in label_name_to_id.items()]
    for label_id, label_name in labels:
        # flows
        n_flows = sum(execute_sql_on_dbs(conns, f"SELECT COUNT(*) FROM label_flows WHERE label_id={label_id}", unpack=True))
        # packets
        n_packets = sum(execute_sql_on_dbs(conns, f"SELECT COUNT(*) FROM label_packets WHERE label_id={label_id}", unpack=True))
        # packets_with_payload
        n_packets_with_payload = sum(execute_sql_on_dbs(conns, f"SELECT COUNT(*) FROM label_packets_with_payload WHERE label_id={label_id}", unpack=True))
        # bursts (è¿™é‡Œé€‰labelå­—æ®µ == label_id)
        n_bursts = sum(execute_sql_on_dbs(conns, f"SELECT COUNT(*) FROM bursts WHERE label={label_id}", unpack=True))
        print(f"   - {label_name:20s} | flows: {n_flows:5d} | packets: {n_packets:6d} | packets(pwld): {n_packets_with_payload:6d} | bursts: {n_bursts:5d}")

    close_dbs(conns)

    print("\n" + "=" * 70)
    print("âœ… æ£€æŸ¥å®Œæˆï¼ï¼ˆå·²é‡‡ç”¨ sqlite generate_classify_tmp ç»Ÿè®¡æ–¹æ¡ˆï¼‰")
    print("=" * 70)
def check_dataset(path: str):
    """
    åˆå¹¶æŒ‡å®šç›®å½•ä¸‹æ‰€æœ‰pickleæ–‡ä»¶ä¸­çš„æ•°ç»„ï¼Œå¹¶è¾“å‡ºåˆå¹¶åæ•°ç»„çš„é•¿åº¦ã€‚
    
    Args:
        directory: åŒ…å«pickleæ–‡ä»¶çš„ç›®å½•è·¯å¾„
        
    Returns:
        tuple: (åˆå¹¶åçš„æ•°ç»„, æ•°ç»„é•¿åº¦)
    """
    import os
    import pickle
    from tqdm import tqdm
    import gc
    
    if not os.path.exists(path):
        print(f"ç›®å½•ä¸å­˜åœ¨: {path}")
        return None, 0
    
    # è·å–æ‰€æœ‰pickleæ–‡ä»¶
    pickle_files = [f for f in os.listdir(path) if f.endswith('.pkl')]
    
    if not pickle_files:
        print(f"ç›®å½• {path} ä¸­æ²¡æœ‰æ‰¾åˆ°pickleæ–‡ä»¶")
        return None, 0
    
    print(f"æ‰¾åˆ° {len(pickle_files)} ä¸ªpickleæ–‡ä»¶")
    
    merged_array = []
    
    len_dict = {}
    total_len = 0
    for filename in tqdm(pickle_files, desc="åˆå¹¶pickleæ–‡ä»¶"):
        file_path = os.path.join(path, filename)
        try:
            with open(file_path, "rb") as f:
                data = pickle.load(f)
                if isinstance(data, list):
                    total_len += len(data)
                    len_dict[filename] = total_len
                else:
                    print(f"è­¦å‘Š: {filename} ä¸­çš„æ•°æ®ä¸æ˜¯åˆ—è¡¨ç±»å‹ï¼Œè·³è¿‡")
            del data
            gc.collect()
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
            continue
    
    print(f"åˆå¹¶å®Œæˆï¼Œæ€»é•¿åº¦: {total_len}")
    # å°†len_dictä¿å­˜åˆ°path/len_dict
    import json
    len_dict_path = os.path.join(path, "len_dict.json")
    try:
        with open(len_dict_path, "w", encoding="utf-8") as f:
            json.dump(len_dict, f, ensure_ascii=False, indent=2)
        print(f"å·²å°†len_dictå†™å…¥åˆ° {len_dict_path}")
    except Exception as e:
        print(f"å†™å…¥len_dictæ—¶å‡ºé”™: {e}")

def check_preprocess(preprocess_path: str):
    """
    ç»Ÿè®¡ç»™å®šè·¯å¾„ä¸‹å„labelï¼ˆå­ç›®å½•ï¼‰åŠå…¶å„è‡ªçš„æ ·æœ¬æ•°é‡ï¼ˆæ–‡ä»¶æ•°é‡ï¼‰ã€‚
    """
    import os

    if not os.path.exists(preprocess_path):
        print(f"æŒ‡å®šè·¯å¾„ä¸å­˜åœ¨: {preprocess_path}")
        return

    label_names = [name for name in os.listdir(preprocess_path)
                   if os.path.isdir(os.path.join(preprocess_path, name))]
    if not label_names:
        print(f"è·¯å¾„ {preprocess_path} ä¸‹æœªæ‰¾åˆ°ä»»ä½•labelå­ç›®å½•")
        return

    print("labelsï¼ˆå­ç›®å½•åç§°ï¼‰åŠå…¶æ ·æœ¬æ•°é‡å¦‚ä¸‹ï¼š")
    print("=" * 60)
    total_samples = 0
    for label in sorted(label_names):
        label_dir = os.path.join(preprocess_path, label)
        file_list = [f for f in os.listdir(label_dir) if os.path.isfile(os.path.join(label_dir, f))]
        num_samples = len(file_list)
        print(f"{label:<30} : {num_samples}")
        total_samples += num_samples
    print("=" * 60)
    print(f"å…± {len(label_names)} ä¸ªlabelï¼Œæ€»æ ·æœ¬æ•°: {total_samples}")

def check_LM_samples(src_path: str, type="qwen-vl"):
    """
    æ£€æŸ¥LMæ ·æœ¬å†…å®¹ï¼šå¦‚æœsrc_pathæ˜¯ç›®å½•ï¼Œåˆ™éå†å…¶ä¸­æ‰€æœ‰.pklæ–‡ä»¶ï¼›å¦‚æœsrc_pathæ˜¯æ–‡ä»¶ï¼Œåˆ™åªè¯»å–è¯¥æ–‡ä»¶ã€‚
    """
    import os
    import pickle
    IMAGE_PAD_ID = 151655

    if not os.path.exists(src_path):
        print(f"âŒ è·¯å¾„ä¸å­˜åœ¨: {src_path}")
        return

    from transformers import AutoTokenizer
    if type == "qwen-vl":
        model_name_or_path = "./Qwen3-VL-8B-Instruct"
    elif type == "qwen-vl-emb":
        model_name_or_path = "./Qwen3-VL-Embedding-2B"
    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)

    if os.path.isfile(src_path):
        files = [src_path]
    else:
        files = [os.path.join(src_path, f) for f in os.listdir(src_path) if f.endswith(".pkl")]

    for fpath in sorted(files[:1]):
        try:
            with open(fpath, "rb") as f:
                samples = pickle.load(f)
            for sample in samples[:50]:
                x_ids, y_ids, payloads, position_ids = sample["data"]
                print(len(x_ids), len(y_ids), len(payloads), position_ids.shape)
                assert sum(1 if id_ == IMAGE_PAD_ID else 0 for id_ in x_ids) == len(payloads)
                assert len(x_ids)+len(y_ids) == position_ids.shape[1]
                input_ids =  x_ids+y_ids
                text = tokenizer.decode(input_ids, skip_special_tokens=False)
                print(text)
                position_ids = position_ids.tolist()
                for i in range(3):
                    for j in range(len(position_ids[i])):
                        print(f"{position_ids[i][j]:03d}", end=" ")
                    print()
                print("\n")
        except Exception as e:
            print(f"{fpath}: è¯»å–å¤±è´¥: {e}")

if __name__ == "__main__":
    from fire import Fire
    Fire()