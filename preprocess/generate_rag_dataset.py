"""
RAG ç³»ç»Ÿçš„æ•°æ®é›†ç”Ÿæˆæ¨¡å—

åŒ…å«ä¸‰ä¸ªä¸»è¦åŠŸèƒ½ï¼š
1. generate_rag_catalog: ç”Ÿæˆ catalog æ–‡ä»¶ï¼Œä¸åŒºåˆ† label æˆ– test/train/val
2. generate_embedding_dataset: ç”Ÿæˆæµé‡åµŒå…¥çš„æ•°æ®é›†
3. generate_corpus_dataset: ç”Ÿæˆè¯­æ–™çš„æ•°æ®é›†
"""

import os
import random
import gc
from typing import List, Dict
from tqdm import tqdm


def generate_rag_catalog(preprocess_path: str, dest_path: str, k: int = 500):
    """
    ç”Ÿæˆ RAG ç³»ç»Ÿçš„ catalog æ–‡ä»¶ï¼Œä¸åŒºåˆ† label æˆ– test/train/valã€‚
    
    ä» preprocess_path ä¸­è¯»å–æ‰€æœ‰ label çš„æ–‡ä»¶ï¼Œæ‰“ä¹±åä»ä¸­æŠ½å– k ä¸ªæ–‡ä»¶ï¼Œ
    å°†æ‰€æœ‰æœ‰æ•ˆçš„ pcap æ–‡ä»¶åä¿å­˜åˆ°ä¸€ä¸ªç»Ÿä¸€çš„ catalog.txt æ–‡ä»¶ä¸­ã€‚
    æ¯è¡Œæ ¼å¼ï¼šlabel\tpcap_name
    
    Args:
        preprocess_path: é¢„å¤„ç†æ–‡ä»¶çš„æ ¹ç›®å½•ï¼Œç›®å½•ç»“æ„: preprocess_path/label_name/*.txt
        dest_path: ä¿å­˜ catalog çš„ç›®çš„åœ°ç›®å½•
        k: æ€»å…±é‡‡é›†çš„æ–‡ä»¶æ•°é‡
    """
    import sys
    
    os.makedirs(dest_path, exist_ok=True)
    
    # è·å–æ‰€æœ‰ label å­ç›®å½•
    label_names = [name for name in os.listdir(preprocess_path)
                   if os.path.isdir(os.path.join(preprocess_path, name))]
    
    # æ”¶é›†æ‰€æœ‰æ ‡ç­¾ä¸‹çš„txtæ–‡ä»¶å
    all_files = []  # [(label, filename), ...]
    for label in label_names:
        label_dir = os.path.join(preprocess_path, label)
        if not os.path.isdir(label_dir):
            continue
        
        file_list = [f for f in os.listdir(label_dir) if f.endswith('.txt')]
        for filename in file_list:
            all_files.append((label, filename))
    
    print(f"ğŸ“ å…±æ”¶é›†åˆ° {len(all_files)} ä¸ªæ–‡ä»¶")
    
    # æ‰“ä¹±åæŠ½å– k ä¸ª
    random.shuffle(all_files)
    catalog_entries = []
    
    for label, filename in tqdm(all_files, desc="å¤„ç†æ–‡ä»¶", file=sys.stdout):
        if len(catalog_entries) >= k:
            break
            
        label_dir = os.path.join(preprocess_path, label)
        
        # æ£€æŸ¥æ–‡ä»¶æœ‰æ•ˆæ€§
        try:
            lines = open(os.path.join(label_dir, filename), "r", encoding="utf-8").readlines()
            if len(lines) < 3:
                continue
        except:
            continue
        
        # ä¿å­˜ label å’Œ pcap åç§°
        catalog_entries.append(f"{label}\t{filename}")
    
    # ä¿å­˜ç»Ÿä¸€çš„ catalog æ–‡ä»¶
    catalog_path = os.path.join(dest_path, "catalog.txt")
    with open(catalog_path, "w", encoding="utf-8") as f:
        for entry in catalog_entries:
            f.write(entry + "\n")
    
    print(f"\nâœ… Catalog ç”Ÿæˆå®Œæˆï¼")
    print(f"   - æ€»æ–‡ä»¶æ•°: {len(catalog_entries)}")
    print(f"   - ä¿å­˜è·¯å¾„: {catalog_path}")
    
    # æ¸…ç†å†…å­˜
    del catalog_entries
    del all_files
    gc.collect()


def generate_embedding_dataset(
    preprocess_path: str,
    catalog_path: str,
    dest_path: str,
    packet_num_in_flow: int = 5
):
    """
    æ ¹æ® catalog ç”Ÿæˆæµé‡åµŒå…¥çš„æ•°æ®é›†ã€‚
    
    Prompt è®¾ç½®ä¸ generate_contrastive_dataset_2 ä¸€è‡´ï¼Œå³å¯¹æ¯”å­¦ä¹ çš„æ ¼å¼ã€‚
    ä¸åŒºåˆ† label æˆ– test/train/valï¼Œæ‰€æœ‰æ•°æ®ä¿å­˜åœ¨åŒä¸€ä¸ªç›®å½•ä¸‹ã€‚
    æ¯ä¸ªæ ·æœ¬ä¿ç•™ pcap ä¿¡æ¯ä½œä¸ºæ ‡è¯†ã€‚
    
    Args:
        preprocess_path: é¢„å¤„ç†æ–‡ä»¶çš„æ ¹ç›®å½•
        catalog_path: catalog æ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼ŒåŒ…å« catalog.txt
        dest_path: ä¿å­˜æ•°æ®é›†çš„ç›®çš„åœ°ç›®å½•
        packet_num_in_flow: æ¯ä¸ªæµåŒ…å«çš„åŒ…æ•°é‡
    """
    from .utils import _LM_input, _str_to_ids, _dump_in_chunks
    import pickle
    import sys
    
    os.makedirs(dest_path, exist_ok=True)
    
    # è¯»å– catalog
    catalog_file = os.path.join(catalog_path, "catalog.txt")
    if not os.path.exists(catalog_file):
        raise FileNotFoundError(f"Catalog æ–‡ä»¶ä¸å­˜åœ¨: {catalog_file}")
    
    catalog_entries = []
    with open(catalog_file, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            parts = line.split('\t')
            if len(parts) == 2:
                label, txt_filename = parts
                catalog_entries.append((label, txt_filename)) 
    print(f"ğŸ“– å·²åŠ è½½ catalogï¼Œå…± {len(catalog_entries)} ä¸ªæ¡ç›®")
    
    # å‡†å¤‡ promptï¼ˆä¸ generate_contrastive_dataset_2 ä¸€è‡´ï¼‰
    system_prompt = """<|im_start|>system
Represent the user's input.<|im_end|> """
    prompt = system_prompt + f"""
<|im_start|>user
<è¡¨æ ¼å¼€å§‹>"""
    prompt_ids = _str_to_ids(prompt, type="qwen3vl-emb")[0]
    prompt2 = """<è¡¨æ ¼ç»“æŸ><|im_end|>
<|im_start|>assistant
"""
    prompt2_ids = _str_to_ids(prompt2, type="qwen3vl-emb")[0]
    
    # ç”Ÿæˆæ•°æ®é›†
    samples = []
    
    # å®šä¹‰æ ·æœ¬ç”Ÿæˆå‡½æ•°ï¼ŒåŒ…å«é•¿åº¦æ§åˆ¶é€»è¾‘
    def generate_sample(lines, lines_used, label):
        """ç”Ÿæˆæ ·æœ¬å¹¶æ§åˆ¶é•¿åº¦ä¸è¶…è¿‡ 4096"""
        lines_used_here = lines_used
        sample = _LM_input(lines[:lines_used_here], None, None, [], prompt_ids, prompt2_ids, label=label, extract_payloads_from_lines=True, biased_avoid=True, token_type="qwen3vl-emb")
        # å¦‚æœæ ·æœ¬é•¿åº¦è¶…è¿‡ 4096ï¼Œé€æ­¥å‡å°‘ä½¿ç”¨çš„è¡Œæ•°
        while sample["data"][-1].shape[1] > 4096 and lines_used_here > 0:
            lines_used_here -= 2
            sample = _LM_input(lines[:lines_used_here], None, None, [], prompt_ids, prompt2_ids, label=label, extract_payloads_from_lines=True, biased_avoid=True, token_type="qwen3vl-emb")
        if sample["data"][-1].shape[1] > 4096:
            raise Exception(f"æ ·æœ¬é•¿åº¦å§‹ç»ˆå¤§äº4096ï¼Œå³ä½¿åªä½¿ç”¨æœ€å°‘çš„è¡Œæ•°")
        return sample
    
    for label, txt_filename in tqdm(catalog_entries, desc="ç”ŸæˆåµŒå…¥æ•°æ®é›†"):
        # æ„é€ æ–‡ä»¶è·¯å¾„
        txt_path = os.path.join(preprocess_path, label, txt_filename)
        
        if not os.path.exists(txt_path):
            continue
        
        try:
            lines = open(txt_path, "r", encoding="utf-8").readlines()
            assert len(lines) >= 3, f"æ–‡ä»¶è¡Œæ•°ä¸è¶³: {txt_path}"
            
            # ä½¿ç”¨é•¿åº¦æ§åˆ¶é€»è¾‘ç”Ÿæˆæ ·æœ¬
            sample = generate_sample(lines, packet_num_in_flow, f'{label}/{txt_filename}')
            samples.append(sample)
            
        except Exception as e:
            print(f"å¤„ç† {txt_path} æ—¶å‡ºé”™: {e}")
            continue
    
    # ä¿å­˜æ•°æ®é›†
    print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜æ•°æ®é›†...")
    _dump_in_chunks(samples, dest_path, chunk_size=1000, name="embedding")
    
    print(f"\nâœ… åµŒå…¥æ•°æ®é›†ç”Ÿæˆå®Œæˆï¼")
    print(f"   - æ ·æœ¬æ€»æ•°: {len(samples)}")
    print(f"   - ä¿å­˜è·¯å¾„: {dest_path}")
    
    del samples
    gc.collect()

def generate_corpus_dataset(
    preprocess_path: str,
    catalog_path: str,
    dest_path: str,
    packet_num_in_flow: int = 5,
    understanding_prompts: List[str] = None,
    common_prompt: bool = True
):
    """
    æ ¹æ® catalog ç”Ÿæˆè¯­æ–™çš„æ•°æ®é›†ã€‚
    
    Prompt æ˜¯ç¡¬ç¼–ç çš„æµé‡ç†è§£é—®é¢˜ï¼Œæµé‡è¡¨å¾ä½¿ç”¨ _LM_input çš„é€»è¾‘ã€‚
    ä¸åŒºåˆ† label æˆ– test/train/valï¼Œæ‰€æœ‰æ•°æ®ä¿å­˜åœ¨åŒä¸€ä¸ªç›®å½•ä¸‹ã€‚
    æ¯ä¸ªæ ·æœ¬ä¿ç•™ pcap ä¿¡æ¯ä½œä¸ºæ ‡è¯†ã€‚
    
    Args:
        preprocess_path: é¢„å¤„ç†æ–‡ä»¶çš„æ ¹ç›®å½•
        catalog_path: catalog æ–‡ä»¶æ‰€åœ¨ç›®å½•ï¼ŒåŒ…å« catalog.txt
        dest_path: ä¿å­˜æ•°æ®é›†çš„ç›®çš„åœ°ç›®å½•
        packet_num_in_flow: æ¯ä¸ªæµåŒ…å«çš„åŒ…æ•°é‡
        understanding_prompts: æµé‡ç†è§£é—®é¢˜åˆ—è¡¨ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨é»˜è®¤é—®é¢˜
    """
    from .utils import _LM_input, _str_to_ids, _dump_in_chunks
    import pickle
    import sys
    
    os.makedirs(dest_path, exist_ok=True)
    
    # è¯»å– catalog
    catalog_file = os.path.join(catalog_path, "catalog.txt")
    if not os.path.exists(catalog_file):
        raise FileNotFoundError(f"Catalog æ–‡ä»¶ä¸å­˜åœ¨: {catalog_file}")
    
    catalog_entries = []
    with open(catalog_file, "r", encoding="utf-8") as f:
        for line in f:
            line = line.strip()
            if not line:
                continue
            parts = line.split('\t')
            if len(parts) == 2:
                label, txt_filename = parts
                catalog_entries.append((label, txt_filename))
    
    print(f"ğŸ“– å·²åŠ è½½ catalogï¼Œå…± {len(catalog_entries)} ä¸ªæ¡ç›®")
    
    # é»˜è®¤çš„æµé‡ç†è§£é—®é¢˜ï¼ˆç¡¬ç¼–ç ï¼Œç›®å‰ç•™ç©ºå¾…å®Œå–„ï¼‰
    if common_prompt:
        understanding_prompts = [
            "è¯·åˆ†æåŠ å¯†æµé‡ä¸­ä½“ç°å‡ºçš„é€šä¿¡è¡Œä¸ºæ¨¡å¼ï¼Œå¹¶æè¿°è¯¥è¡Œä¸ºå¯èƒ½å¯¹åº”å“ªç§ç±»å‹çš„ç½‘ç»œæ´»åŠ¨ã€‚è¯·è¯´æ˜åˆ¤æ–­çš„ä¾æ®ã€‚",
            "ä»è¿™æ®µåŠ å¯†æµé‡ä¸­ï¼Œä½ èƒ½æ¨æ–­å‡ºé€šä¿¡åŒæ–¹çš„äº¤äº’æ„å›¾å—ï¼Ÿè¯·æè¿°å…¶å¯èƒ½çš„äº¤äº’é€»è¾‘ï¼Œå¹¶è¯´æ˜åˆ¤æ–­çš„ä¾æ®ã€‚",
            "è¿™æ®µæµé‡æ˜¯å¦è¡¨ç°å‡ºæ­£å¸¸ç½‘ç»œé€šä¿¡çš„ç‰¹å¾ï¼Ÿè¯·è¯´æ˜åˆ¤æ–­çš„ä¾æ®ã€‚",
            "è¯·æè¿°è¿™æ®µåŠ å¯†æµé‡æ‰€å±•ç°çš„ä¼šè¯ç»“æ„ï¼Œå¹¶è§£é‡Šå…¶ç»“æ„ç‰¹ç‚¹å¯èƒ½åæ˜ äº†ä»€ä¹ˆã€‚è¯·è¯´æ˜åˆ¤æ–­çš„ä¾æ®ã€‚",
            "åœ¨è¿™æ®µæµé‡ä¸­ï¼Œèƒ½å¦åˆ¤æ–­å“ªä¸€ç«¯æ›´å¯èƒ½æ˜¯å®¢æˆ·ç«¯ï¼Œå“ªä¸€ç«¯æ›´å¯èƒ½æ˜¯æœåŠ¡ç«¯ï¼Ÿè¯·è¯´æ˜åˆ¤æ–­çš„ä¾æ®ã€‚"
        ]
    else:
        understanding_prompts = [
            "æ¥ä¸‹æ¥ä¼šç»™å‡ºä¸€ä¸ªæµé‡è¡¨æ ¼ï¼ŒåŒ…å«è‹¥å¹²ä¸ªåŒ…çš„å¤´éƒ¨ç‰¹å¾å’Œç»Ÿè®¡ç‰¹å¾ï¼Œä»¥åŠåœ¨æœ€åä¸€åˆ—çš„payloadã€‚è¯·è¾“å‡ºå¯¹åº”çš„ç±»åˆ«ã€‚ç±»åˆ«åŒ…å«:sendAudio, sendImage, sendText, shareLocationOnce, transferFileã€‚"
        ]
    
    # å‡†å¤‡ prompt
    system_prompt = """<|im_start|>system
ä½ æ˜¯ä¸€ä¸ªAIåŠ©æ‰‹ï¼Œæ“…é•¿é˜…è¯»è¡¨æ ¼å½¢å¼çš„ç½‘ç»œæµé‡å¹¶å¯¹å…¶è¿›è¡Œæ€è€ƒå’Œç†è§£ï¼Œå¹¶èƒ½å¤Ÿå®Œæˆå„ç§é’ˆå¯¹ç½‘ç»œæµé‡çš„é—®é¢˜ã€‚<|im_end|>
<|im_start|>user
"""
    
    # ç”Ÿæˆæ•°æ®é›†
    samples = []
    
    # å®šä¹‰æ ·æœ¬ç”Ÿæˆå‡½æ•°ï¼ŒåŒ…å«é•¿åº¦æ§åˆ¶é€»è¾‘
    def generate_sample(lines, lines_used, label, prompt_ids, prompt2_ids, answer_ids):
        """ç”Ÿæˆæ ·æœ¬å¹¶æ§åˆ¶é•¿åº¦ä¸è¶…è¿‡ 4096"""
        lines_used_here = lines_used
        sample = _LM_input(lines[:lines_used_here], None, None, answer_ids, prompt_ids, prompt2_ids, label=label, extract_payloads_from_lines=True, biased_avoid=True, token_type="qwen3vl")
        # å¦‚æœæ ·æœ¬é•¿åº¦è¶…è¿‡ 4096ï¼Œé€æ­¥å‡å°‘ä½¿ç”¨çš„è¡Œæ•°
        while sample["data"][-1].shape[1] > 4096 and lines_used_here > 0:
            lines_used_here -= 2
            sample = _LM_input(lines[:lines_used_here], None, None, answer_ids, prompt_ids, prompt2_ids, label=label, extract_payloads_from_lines=True, biased_avoid=True, token_type="qwen3vl")
        if sample["data"][-1].shape[1] > 4096:
            raise Exception(f"æ ·æœ¬é•¿åº¦å§‹ç»ˆå¤§äº4096ï¼Œå³ä½¿åªä½¿ç”¨æœ€å°‘çš„è¡Œæ•°")
        return sample
    
    for label, txt_filename in tqdm(catalog_entries, desc="ç”Ÿæˆè¯­æ–™æ•°æ®é›†"):
        # æ„é€ æ–‡ä»¶è·¯å¾„
        txt_path = os.path.join(preprocess_path, label, txt_filename)
        
        if not os.path.exists(txt_path):
            continue
        
        try:
            lines = open(txt_path, "r", encoding="utf-8").readlines()
            assert len(lines) >= 3, f"æ–‡ä»¶è¡Œæ•°ä¸è¶³: {txt_path}"
            
            # éšæœºé€‰æ‹©ä¸€ä¸ªç†è§£é—®é¢˜
            question_idx = random.randint(0, len(understanding_prompts) - 1)
            question = understanding_prompts[question_idx]
            
            # æ„é€ å®Œæ•´ prompt
            prompt = system_prompt + f"""æ¥ä¸‹æ¥ä¼šç»™å‡ºä¸€ä¸ªæµé‡è¡¨æ ¼ï¼ŒåŒ…å«è‹¥å¹²ä¸ªåŒ…çš„å¤´éƒ¨ç‰¹å¾å’Œç»Ÿè®¡ç‰¹å¾ï¼Œä»¥åŠåœ¨æœ€åä¸€åˆ—çš„payloadã€‚
è¯·å°±æµé‡è¡¨æ ¼æ¥å›ç­”ä»¥ä¸‹é—®é¢˜ï¼š
{question}
å›ç­”çš„å­—æ•°åº”åœ¨300åˆ°500ä¹‹é—´ã€‚
æ¥ä¸‹æ¥æ˜¯æµé‡è¡¨æ ¼ï¼š<è¡¨æ ¼å¼€å§‹>"""
            prompt_ids = _str_to_ids(prompt, type="qwen3vl")[0]
            prompt2 = """<è¡¨æ ¼ç»“æŸ><|im_end|>
<|im_start|>assistant
"""
            prompt2_ids = _str_to_ids(prompt2, type="qwen3vl")[0]
            
            # ä½¿ç”¨é•¿åº¦æ§åˆ¶é€»è¾‘ç”Ÿæˆæ ·æœ¬
            sample = generate_sample(lines, packet_num_in_flow, f'{label}/{txt_filename}', prompt_ids, prompt2_ids, [])
            samples.append(sample)
            
        except Exception as e:
            print(f"å¤„ç† {txt_path} æ—¶å‡ºé”™: {e}")
            continue
    
    # ä¿å­˜æ•°æ®é›†
    print(f"\nğŸ’¾ æ­£åœ¨ä¿å­˜æ•°æ®é›†...")
    _dump_in_chunks(samples, dest_path, chunk_size=1000, name="corpus")
    
    print(f"\nâœ… è¯­æ–™æ•°æ®é›†ç”Ÿæˆå®Œæˆï¼")
    print(f"   - æ ·æœ¬æ€»æ•°: {len(samples)}")
    print(f"   - ä¿å­˜è·¯å¾„: {dest_path}")
    
    del samples
    gc.collect()


def main():
    """ä¸»å‡½æ•° - æ¼”ç¤ºæ•°æ®é›†ç”Ÿæˆæµç¨‹"""
    print("=" * 60)
    print("RAG æ•°æ®é›†ç”Ÿæˆç³»ç»Ÿ")
    print("=" * 60)
    
    # ç¤ºä¾‹é…ç½®
    preprocess_path = "path/to/preprocess"
    catalog_path = "path/to/catalog"
    embedding_dest = "path/to/embedding_dataset"
    corpus_dest = "path/to/corpus_dataset"
    
    # æ­¥éª¤ 1: ç”Ÿæˆ catalog
    print("\nã€æ­¥éª¤ 1/3ã€‘ç”Ÿæˆ Catalog")
    print("-" * 60)
    # generate_rag_catalog(preprocess_path, catalog_path, k=500)
    
    # æ­¥éª¤ 2: ç”ŸæˆåµŒå…¥æ•°æ®é›†
    print("\nã€æ­¥éª¤ 2/3ã€‘ç”ŸæˆåµŒå…¥æ•°æ®é›†")
    print("-" * 60)
    # generate_embedding_dataset(preprocess_path, catalog_path, embedding_dest)
    
    # æ­¥éª¤ 3: ç”Ÿæˆè¯­æ–™æ•°æ®é›†
    print("\nã€æ­¥éª¤ 3/3ã€‘ç”Ÿæˆè¯­æ–™æ•°æ®é›†")
    print("-" * 60)
    # generate_corpus_dataset(preprocess_path, catalog_path, corpus_dest)
    
    print("\n" + "=" * 60)
    print("âœ… æ•°æ®é›†ç”Ÿæˆç³»ç»Ÿæ¼”ç¤ºå®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    from fire import Fire
    Fire()
