import os

fields = ["frame.encap_type", "frame.time", "frame.offset_shift", "frame.time_epoch", "frame.time_delta",
                "frame.time_relative", "frame.number", "frame.len", "frame.marked", "frame.protocols", "eth.dst",
                "eth.dst_resolved", "eth.src", "eth.src_resolved", "eth.type",
                "ip.version", "ip.hdr_len", "ip.dsfield", "ip.dsfield.dscp", "ip.dsfield.ecn", "ip.len", "ip.id",
                "ip.flags", "ip.flags.rb", "ip.flags.df", "ip.flags.mf", "ip.frag_offset", "ip.ttl", "ip.proto",
                "ip.checksum", "ip.checksum.status", "ip.src", "ip.dst", "tcp.srcport", "tcp.dstport", "tcp.stream",
                "tcp.len", "tcp.seq", "tcp.nxtseq", "tcp.ack", "tcp.hdr_len", "tcp.flags",
                "tcp.flags.res", "tcp.flags.cwr", "tcp.flags.urg", "tcp.flags.ack",
                "tcp.flags.push", "tcp.flags.reset", "tcp.flags.syn", "tcp.flags.fin", "tcp.flags.str",
                "tcp.window_size", "tcp.window_size_scalefactor", "tcp.checksum", "tcp.checksum.status", "tcp.urgent_pointer",
                "tcp.time_relative", "tcp.time_delta", "tcp.analysis.bytes_in_flight", "tcp.analysis.push_bytes_sent", "tcp.segment",
                "tcp.segment.count", "tcp.reassembled.length", "tcp.payload", "udp.srcport", "udp.dstport", "udp.length",
                "udp.checksum", "udp.checksum.status", "udp.stream", "udp.payload", "data.len"]

def _get_field_index(target):
    for i, field in enumerate(fields):
        if field == target:
            return i
src_index = _get_field_index("ip.src")
tcp_payload_index = _get_field_index("tcp.payload")
udp_payload_index = _get_field_index("udp.payload")


def process_pcap(pcap_path: str, tmp_path: str):

    extract_str = " -e " + " -e ".join(fields) + " "
    cmd = "tshark -r " + pcap_path + extract_str + "-T fields -Y 'tcp or udp' > " + tmp_path
    os.system(cmd)

def process_flow_dataset(src_path: str, dest_path: str):
    """
    å¤„ç†æ•°æ®é›†ï¼Œå°†src_pathä¸‹çš„pcapæ–‡ä»¶è½¬æ¢ä¸ºå¤„ç†åçš„æ–‡ä»¶
    
    Args:
        src_path: æºè·¯å¾„ï¼ŒåŒ…å«ä¸€çº§æ–‡ä»¶å¤¹ï¼Œä¸€çº§æ–‡ä»¶å¤¹ä¸‹åŒ…å«pcapæ–‡ä»¶
        dest_path: ç›®æ ‡è·¯å¾„ï¼Œå°†åˆ›å»ºç›¸åŒçš„ç›®å½•ç»“æ„å¹¶è¾“å‡ºå¤„ç†åçš„æ–‡ä»¶
    """
    import os
    import shutil
    from tqdm import tqdm
    
    # åˆ›å»ºç›®æ ‡è·¯å¾„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    os.makedirs(dest_path, exist_ok=True)
    
    # æ”¶é›†æ‰€æœ‰éœ€è¦å¤„ç†çš„pcapæ–‡ä»¶
    pcap_files = []
    for item in os.listdir(src_path):
        item_path = os.path.join(src_path, item)
        if os.path.isdir(item_path):
            dest_item_path = os.path.join(dest_path, item)
            os.makedirs(dest_item_path, exist_ok=True)
            
            for file in os.listdir(item_path):
                file_path = os.path.join(item_path, file)
                if os.path.isfile(file_path) and file.lower().endswith('.pcap'):
                    base_name = os.path.splitext(file)[0]
                    output_file = base_name + '.txt'
                    output_path = os.path.join(dest_item_path, output_file)
                    pcap_files.append((file_path, output_path))
    
    # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
    for file_path, output_path in tqdm(pcap_files, desc="å¤„ç†PCAPæ–‡ä»¶"):
        process_pcap(file_path, output_path)

def _cut_bursts(in_path):
    bursts = []
    with open(in_path, "r", encoding="utf-8") as fin:
        lines = fin.readlines()
        src_ip = lines[0][:-1].split("\t")[src_index]
        current_burst = []
        payloads_in_current_burst = []
        payload_num = 0
        for line in lines:
            values = line[:-1].split("\t")
            ip = values[src_index]
            if src_ip != ip:
                bursts.append({"packets": current_burst, "payload_num": payload_num, "payloads": payloads_in_current_burst})
                current_burst = []
                payload_num = 0
                payloads_in_current_burst = []
                src_ip = ip
            current_burst.append(line)
            payload = values[tcp_payload_index]
            if payload == "":
                payload = values[udp_payload_index]
            if payload != "":
                payload_num += 1
                payloads_in_current_burst.append(payload)
        if len(current_burst) > 0:
                bursts.append({"packets": current_burst, "payload_num": payload_num, "payloads": payloads_in_current_burst})
    return bursts

def _dump_in_chunks(items, out_dir, chunk_size):
    import os
    import pickle
    os.makedirs(out_dir, exist_ok=True)
    idx = 0
    if chunk_size == -1:
        chunk_size = len(items)
    for start in range(0, len(items), chunk_size):
        chunk = items[start:start + chunk_size]
        file_name = f"part_{idx:05d}.pkl"
        file_path = os.path.join(out_dir, file_name)
        with open(file_path, "wb") as fout:
            pickle.dump(chunk, fout)
        idx += 1

def generate_packet_dataset(src_path: str, dest_path: str, k: int = 10000):
    # ä»flowä¸­é—´æ•°æ®ç”Ÿæˆpacketæ•°æ®é›†
    import os
    import shutil
    from tqdm import tqdm
    
    # åˆ›å»ºç›®æ ‡è·¯å¾„ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    os.makedirs(dest_path, exist_ok=True)
    
    all_packets = []
    # æ”¶é›†æ‰€æœ‰txtæ–‡ä»¶
    txt_files = []
    for item in os.listdir(src_path):
        item_path = os.path.join(src_path, item)
        if os.path.isdir(item_path):
            for file in os.listdir(item_path):
                file_path = os.path.join(item_path, file)
                if os.path.isfile(file_path) and file.lower().endswith('.txt'):
                    txt_files.append(file_path)
    
    # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
    for file_path in tqdm(txt_files, desc="å¤„ç†æ–‡æœ¬æ–‡ä»¶"):
        with open(file_path, "r", encoding="utf-8") as fin:
            lines = fin.readlines()
            for line in lines:
                # æå–payloadå†…å®¹
                values = line[:-1].split("\t")
                payload = values[tcp_payload_index]
                if payload == "":
                    payload = values[udp_payload_index]
                all_packets.append(payload)
    
    import random
    random.shuffle(all_packets)

    # æŒ‰ 9:1 åˆ’åˆ†è®­ç»ƒé›†ä¸æµ‹è¯•é›†
    total_count = len(all_packets)
    train_count = int(total_count * 0.9)
    train_packets = all_packets[:train_count]
    test_packets = all_packets[train_count:]

    # åˆ›å»º train/test ç›®å½•
    train_dir = os.path.join(dest_path, "train")
    test_dir = os.path.join(dest_path, "test")
    os.makedirs(train_dir, exist_ok=True)
    os.makedirs(test_dir, exist_ok=True)

    # ä¿å­˜åˆ†ç‰‡pickle
    print("ä¿å­˜è®­ç»ƒé›†...")
    _dump_in_chunks(train_packets, train_dir, k)
    print("ä¿å­˜æµ‹è¯•é›†...")
    _dump_in_chunks(test_packets, test_dir, k)


def generate_flow_dataset(src_path: str, dest_path: str, k: int = 1000):
    """
    ç”Ÿæˆæµçº§åˆ«ã€å¸¦æ ‡ç­¾çš„æ•°æ®é›†ã€‚

    - æºç›®å½•ç»“æ„ï¼šsrc_path/<label>/*.txtï¼ˆæ¯ä¸ª txt ä¸ºä¸€ä¸ªæµçš„ä¸­é—´ç»“æœï¼‰
    - ç›®æ ‡ç›®å½•ç»“æ„ï¼š
        dest_path/train/<label>/part_XXXXX.pkl
        dest_path/test/<label>/part_XXXXX.pkl
    - æ¯ä¸ªæ ·æœ¬ä¸ºä¸€ä¸ª"åµŒå¥—åˆ—è¡¨"ï¼ˆå³è¯¥ txt æ–‡ä»¶çš„æ‰€æœ‰è¡Œæ„æˆçš„åˆ—è¡¨ï¼‰
    - æ¯ä¸ª pkl ä¸­å­˜æ”¾æœ€å¤š k ä¸ªæ ·æœ¬ï¼ˆé»˜è®¤ 10000ï¼‰
    - æ¯ä¸ªæ ‡ç­¾å†…ç‹¬ç«‹æŒ‰ 9:1 åˆ’åˆ†è®­ç»ƒ/æµ‹è¯•
    """
    import os
    import pickle
    import random
    from tqdm import tqdm

    # åˆ›å»ºç›®æ ‡æ ¹è·¯å¾„
    os.makedirs(dest_path, exist_ok=True)

    # ç›®æ ‡ä¸‹çš„ train/test ç›®å½•
    train_root = os.path.join(dest_path, "train")
    test_root = os.path.join(dest_path, "test")
    os.makedirs(train_root, exist_ok=True)
    os.makedirs(test_root, exist_ok=True)

    # è·å–æ‰€æœ‰æ ‡ç­¾
    labels = [item for item in os.listdir(src_path) 
              if os.path.isdir(os.path.join(src_path, item))]

    # éå†æ ‡ç­¾ï¼ˆsrc_path çš„ä¸€çº§ç›®å½•ï¼‰
    for label in tqdm(labels, desc="å¤„ç†æ ‡ç­¾"):
        label_src_dir = os.path.join(src_path, label)
        if not os.path.isdir(label_src_dir):
            continue

        # æ”¶é›†è¯¥æ ‡ç­¾ä¸‹çš„æ‰€æœ‰æ ·æœ¬ï¼ˆæ¯ä¸ªæ ·æœ¬ä¸ºä¸€ä¸ª txt çš„è¡Œåˆ—è¡¨ï¼‰
        samples = []
        txt_files = [f for f in os.listdir(label_src_dir) 
                     if f.lower().endswith('.txt')]
        
        for fname in tqdm(txt_files, desc=f"å¤„ç†æ ‡ç­¾ {label}", leave=False):
            fpath = os.path.join(label_src_dir, fname)
            if os.path.isfile(fpath):
                with open(fpath, "r", encoding="utf-8") as fin:
                    lines = fin.readlines()
                    samples.append(lines)

        if not samples:
            continue

        # æ¯ä¸ªæ ‡ç­¾å†…éšæœºæ‰“ä¹±å¹¶ 9:1 åˆ’åˆ†
        random.shuffle(samples)
        total = len(samples)
        train_n = int(total * 0.9)
        train_samples = samples[:train_n]
        test_samples = samples[train_n:]

        # è¯¥æ ‡ç­¾åœ¨ train/test ä¸‹çš„ç›®å½•
        train_label_dir = os.path.join(train_root, label)
        test_label_dir = os.path.join(test_root, label)
        os.makedirs(train_label_dir, exist_ok=True)
        os.makedirs(test_label_dir, exist_ok=True)

        # å†™å…¥åˆ†ç‰‡ pickleï¼ˆåµŒå¥—åˆ—è¡¨ï¼šæ¯ä¸ªå…ƒç´ ä¸ºä¸€ä¸ª txt çš„è¡Œåˆ—è¡¨ï¼‰
        print(f"ä¿å­˜æ ‡ç­¾ {label} çš„è®­ç»ƒé›†...")
        _dump_in_chunks(train_samples, train_label_dir, k)
        print(f"ä¿å­˜æ ‡ç­¾ {label} çš„æµ‹è¯•é›†...")
        _dump_in_chunks(test_samples, test_label_dir, k)


def generate_classify_tmp(src_path: str, tmp_path: str):
    """
    å¯¹æ‰€æœ‰æµï¼ˆä¸åˆ†æ ‡ç­¾ï¼‰è¿›è¡Œç»Ÿè®¡å¹¶åœ¨å†…å­˜ä¸­ä¿å­˜ç›¸å…³æ•°æ®ç»“æ„ï¼š
    1) ç»Ÿè®¡æ‰€æœ‰"è‡³å°‘åŒ…å«ä¸¤ä¸ª payload çš„ burst"ï¼ˆå…¨å±€ï¼‰ï¼Œå¹¶ä¿å­˜è¿™äº› burstï¼›
    2) ç»Ÿè®¡æ¯ä¸ªæµä¸­"è‡³å°‘åŒ…å«ä¸€ä¸ª payload çš„ burst"çš„æ•°é‡ï¼›
    3) ç»Ÿè®¡"è‡³å°‘æœ‰ä¸¤ä¸ªç±»å‹2ï¼ˆpayload>=1ï¼‰burst"çš„æµé›†åˆï¼›
    4) ç»Ÿè®¡"è‡³å°‘æœ‰ä¸€ä¸ªç±»å‹2ï¼ˆpayload>=1ï¼‰burst"çš„æµé›†åˆã€‚
    5) æ–°å¢ï¼šç»Ÿè®¡æ‰€æœ‰payloadå†…å®¹ï¼Œæ ¼å¼ä¸º{label: [payload, ...]}ï¼Œå¹¶ä¿å­˜

    è¿”å›åŒ…å«ä¸Šè¿°ç»Ÿè®¡ä¸æ•°æ®ç»“æ„çš„å­—å…¸ã€‚
    """
    import os
    from tqdm import tqdm

    bursts_payload_ge2 = []  # å…¨å±€ï¼šæ»¡è¶³ payload_num>=2 çš„ burst
    flows_bursts_payload_ge1_count = {}  # flow_path -> æ»¡è¶³ payload_num>=1 çš„ burst æ•°é‡

    # æ–°å¢ï¼šç»Ÿè®¡ï¼Œæ¯ä¸ªlabelå¯¹åº”ä¸€ä¸ªpayloadåˆ—è¡¨
    payload_label_dict = {}

    # æ”¶é›†æ‰€æœ‰æµæ–‡ä»¶
    flow_files = []
    for label in os.listdir(src_path):
        label_dir = os.path.join(src_path, label)
        if os.path.isdir(label_dir):
            for fname in os.listdir(label_dir):
                if fname.lower().endswith('.txt'):
                    flow_path = os.path.join(label_dir, fname)
                    if os.path.isfile(flow_path):
                        flow_files.append((flow_path, label))

    # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
    for flow_path, label in tqdm(flow_files, desc="åˆ†ææµæ–‡ä»¶"):
        if label not in payload_label_dict:
            payload_label_dict[label] = []
        bursts = _cut_bursts(flow_path)
        ge1_count = 0
        bursts_payload_ge1 = []
        for b in bursts:
            # æ–°å¢ï¼šæŠŠpayloadè®°å½•åˆ°payload_label_dictä¸­
            if "payloads" in b and isinstance(b["payloads"], list):
                for payload in b["payloads"]:
                    if payload.strip():  # åªç»Ÿè®¡éç©ºpayload
                        payload_label_dict[label].append(payload)

            # ç»Ÿè®¡burstæ•°é‡
            if b["payload_num"] >= 2:
                bursts_payload_ge2.append(b)
            if b["payload_num"] >= 1:
                ge1_count += 1
                bursts_payload_ge1.append(b)
        flows_bursts_payload_ge1_count[flow_path] = {"ge1_count": ge1_count, "bursts_payload_ge1": bursts_payload_ge1}

    # å°†ç»“æœä¿å­˜åˆ° tmp_path
    import pickle
    os.makedirs(tmp_path, exist_ok=True)

    print("ä¿å­˜ç»Ÿè®¡ç»“æœ...")
    with open(os.path.join(tmp_path, "bursts_payload_ge2.pkl"), "wb") as f:
        pickle.dump(bursts_payload_ge2, f)
    with open(os.path.join(tmp_path, "flows_bursts_payload_ge1_count.pkl"), "wb") as f:
        pickle.dump(flows_bursts_payload_ge1_count, f)
    # ä¿å­˜payloadä¸labelçš„ç»Ÿè®¡å­—å…¸
    with open(os.path.join(tmp_path, "payload_label_dict.pkl"), "wb") as f:
        pickle.dump(payload_label_dict, f)
def generate_classify1_dataset(tmp_path: str, dest_path: str, k: int = 10000):
    """
    ç”Ÿæˆæœ‰æ ‡ç­¾çš„æ•°æ®é›†ï¼š
    1) æŒ‡å®š k ä½œä¸ºæ¯ç±»æ•°æ®çš„æ ·æœ¬æ•°é‡ï¼›
    2) ä»"è‡³å°‘æœ‰ä¸¤ä¸ª payload çš„ burst"ä¸­éšæœºæŒ‘é€‰ k ä¸ªï¼Œæ¯ä¸ª burst å–ä¸¤ä¸ªå¸¦ payload çš„è¡Œï¼š
       - é¡ºåºç»„åˆä¸ºä¸€ä¸ªæ ·æœ¬ï¼Œlabel=1ï¼›
       - é€†åºç»„åˆä¸ºä¸€ä¸ªæ ·æœ¬ï¼Œlabel=2ï¼›
    3) ä»"è‡³å°‘æœ‰ä¸¤ä¸ªç±»å‹2 burstï¼ˆpayload>=1ï¼‰"çš„æµé›†åˆä¸­ï¼ŒéšæœºæŒ‘é€‰ k æ¬¡ï¼š
       - æ¯æ¬¡éšæœºé€‰ä¸€ä¸ªæµï¼Œå†éšæœºé€‰ä¸¤ä¸ª burstï¼›
       - å„è‡ªéšæœºå–ä¸€ä¸ªå¸¦ payload çš„è¡Œï¼›
       - æŒ‰ burst åœ¨æµä¸­çš„å…ˆåé¡ºåºç»„åˆä¸ºä¸€ä¸ªæ ·æœ¬ï¼Œlabel=3ï¼›é€†åºä¸ºä¸€ä¸ªæ ·æœ¬ï¼Œlabel=4ï¼›
    4) ä»"è‡³å°‘æœ‰ä¸€ä¸ªç±»å‹2 burst"çš„æµä¸­ï¼ŒéšæœºæŒ‘é€‰ k æ¬¡ï¼š
       - æ¯æ¬¡éšæœºé€‰ä¸¤ä¸ªä¸åŒçš„æµï¼Œåˆ†åˆ«åœ¨å„è‡ªæµä¸­éšæœºé€‰ä¸€ä¸ªç±»å‹2 burstï¼Œéšæœºå–ä¸€ä¸ªå¸¦ payload çš„è¡Œï¼›
       - ä¸¤è¡Œç»„æˆä¸€ä¸ªæ ·æœ¬ï¼ˆé¡ºåºä»»æ„ï¼‰ï¼Œlabel=5ï¼›
    5) å°†æ‰€æœ‰æ ·æœ¬ä½¿ç”¨ _dump_in_chunks å­˜åˆ° dest_path/classify1 ä¸‹ã€‚
    """
    import os
    import pickle
    import random
    from tqdm import tqdm

    os.makedirs(dest_path, exist_ok=True)
    out_dir = os.path.join(dest_path, "classify1")
    os.makedirs(out_dir, exist_ok=True)

    # è¯»å–ä¸´æ—¶ç»Ÿè®¡ç»“æœ
    print("åŠ è½½ç»Ÿè®¡ç»“æœ...")
    with open(os.path.join(tmp_path, "bursts_payload_ge2.pkl"), "rb") as f:
        bursts_payload_ge2 = pickle.load(f)
    with open(os.path.join(tmp_path, "flows_bursts_payload_ge1_count.pkl"), "rb") as f:
        flows_bursts_payload_ge1_count = pickle.load(f)

    # ä» flows_bursts_payload_ge1_count åŠ¨æ€å¾—åˆ°ä¸¤ä¸ªæµé›†åˆï¼ˆé”®ä¸º flow_pathï¼Œä¿è¯å¯ç´¢å¼•ï¼‰
    flows_with_ge2 = [
        flow for flow, detail in flows_bursts_payload_ge1_count.items() if detail["ge1_count"] >= 2
    ]
    flows_with_ge1 = [
        flow for flow, detail in flows_bursts_payload_ge1_count.items() if detail["ge1_count"] >= 1
    ]

    samples = []  # æ¯ä¸ªæ ·æœ¬ä¸º {"lines": [str, str], "label": int}

    # ç±»åˆ« 1/2ï¼šä» bursts_payload_ge2 ä¸­é‡‡æ ·
    if bursts_payload_ge2:
        print("ç”Ÿæˆç±»åˆ« 1/2 æ ·æœ¬...")
        for _ in tqdm(range(k), desc="ç±»åˆ«1/2", leave=False):
            burst = random.choice(bursts_payload_ge2)
            payload_lines = burst["payloads"]
            if len(payload_lines) < 2:
                continue
            i, j = random.sample(range(len(payload_lines)), 2)
            a = payload_lines[i]
            b = payload_lines[j]
            samples.append({"lines": [a, b], "label": 1})
            samples.append({"lines": [b, a], "label": 2})

    # ç±»åˆ« 3/4ï¼šä»æ‹¥æœ‰ >=2 ä¸ªç±»å‹2 burst çš„æµä¸­é‡‡æ ·
    if flows_with_ge2:
        print("ç”Ÿæˆç±»åˆ« 3/4 æ ·æœ¬...")
        for _ in tqdm(range(k), desc="ç±»åˆ«3/4", leave=False):
            flow = random.choice(flows_with_ge2)
            bursts = flows_bursts_payload_ge1_count[flow]["bursts_payload_ge1"]
            if len(bursts) < 2:
                continue
            i, j = random.sample(range(len(bursts)), 2)
            i, j = (i, j) if i < j else (j, i)
            b1, b2 = bursts[i], bursts[j]
            pl1 = b1["payloads"]
            pl2 = b2["payloads"]
            if not pl1 or not pl2:
                continue
            l1 = random.choice(pl1)
            l2 = random.choice(pl2)
            samples.append({"lines": [l1, l2], "label": 3})
            samples.append({"lines": [l2, l1], "label": 4})

    # ç±»åˆ« 5ï¼šä»æ‹¥æœ‰ >=1 ä¸ªç±»å‹2 burst çš„æµä¸­é‡‡æ ·ï¼Œæˆå¯¹ä¸åŒæµ
    if len(flows_with_ge1) >= 2:
        print("ç”Ÿæˆç±»åˆ« 5 æ ·æœ¬...")
        for _ in tqdm(range(k), desc="ç±»åˆ«5", leave=False):
            flow_a, flow_b = random.sample(flows_with_ge1, 2)
            bursts_a = flows_bursts_payload_ge1_count[flow_a]["bursts_payload_ge1"]
            bursts_b = flows_bursts_payload_ge1_count[flow_b]["bursts_payload_ge1"]
            if not bursts_a or not bursts_b:
                continue
            ba = random.choice(bursts_a)
            bb = random.choice(bursts_b)
            pla = ba["payloads"]
            plb = bb["payloads"]
            if not pla or not plb:
                continue
            la = random.choice(pla)
            lb = random.choice(plb)
            samples.append({"lines": [la, lb], "label": 5})

    # ä¿å­˜æ‰€æœ‰æ ·æœ¬
    print("ä¿å­˜æ ·æœ¬...")
    _dump_in_chunks(samples, out_dir, k)

def _bigram_generation(packet_datagram, packet_len=64):
    def cut(obj, sec):
        sec = sec % 4 + sec
        return [obj[i: i + sec] for i in range(0, len(obj), sec)]

    result = ""
    generated_datagram = cut(packet_datagram, 1)
    token_count = 0
    for sub_string_index in range(len(generated_datagram)):
        if sub_string_index != (len(generated_datagram) - 1):
            token_count += 1
            if token_count > packet_len:
                break
            else:
                merge_word_bigram = (
                    generated_datagram[sub_string_index]
                    + generated_datagram[sub_string_index + 1]
                )
        else:
            break
        result += merge_word_bigram
        result += " "

    return result

def _str_to_ids(text: str, seq_length: int, tokenizer):
    from uer.uer.utils import CLS_TOKEN, PAD_TOKEN
    tokens = tokenizer.tokenize(text)
    token_len = len(tokens)+1
    assert len(tokens) <= seq_length
    tokens = [CLS_TOKEN] + tokens + [PAD_TOKEN] * (seq_length - len(tokens))
    return tokenizer.convert_tokens_to_ids(tokens), token_len

def generate_contrastive_dataset(tmp_path: str, dest_path: str, k: int = 10000, k2: int = 1000):
    """
    ç”Ÿæˆå¯¹æ¯”å­¦ä¹ æ•°æ®é›†ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œåªæœ‰ä¸¤ç±»æ ‡ç­¾ï¼‰ï¼Œå¹¶æŒ‰9:1åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚
    é¢å¤–ï¼šåŠ è½½payload_label_dictï¼Œä»æ¯ä¸ªlabelä¸­éšæœºé€‰å–k2ä¸ªpayloadï¼Œå°†<payload, label>å¯¹æ‰“ä¹±å¹¶ä¿å­˜åœ¨dest_path/testè·¯å¾„ä¸‹ï¼Œç”¨_dump_in_chunksåˆ†ç‰‡ä¿å­˜ã€‚
    
    Args:
        tmp_path: generate_classify1_tmp ç”Ÿæˆçš„ä¸´æ—¶æ–‡ä»¶è·¯å¾„
        dest_path: è¾“å‡ºæ•°æ®é›†è·¯å¾„
        k: æ¯ç±»æ ·æœ¬æ•°é‡ï¼ˆå¯¹æ¯”è®­ç»ƒé›†ï¼‰
        k2: æ¯ç±»æµ‹è¯•é›†payloadæ•°é‡
    """
    import os
    import pickle
    import random
    from tqdm import tqdm

    out_dir = dest_path
    os.makedirs(out_dir, exist_ok=True)

    import argparse
    parser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    from uer.uer.opts import tokenizer_opts
    tokenizer_opts(parser)
    args = parser.parse_args([])  # ä½¿ç”¨ç©ºåˆ—è¡¨ï¼Œé‡‡ç”¨é»˜è®¤å‚æ•°è€Œéå‘½ä»¤è¡Œ
    args.vocab_path = "config/encryptd_vocab.txt"
    from uer.uer.utils import str2tokenizer
    args.tokenizer = str2tokenizer[args.tokenizer](args)
    args.vocab = args.tokenizer.vocab
    SEQ_LENGTH = 1500

    # è¯»å–ä¸´æ—¶ç»Ÿè®¡ç»“æœ
    print("åŠ è½½ç»Ÿè®¡ç»“æœ...")
    with open(os.path.join(tmp_path, "bursts_payload_ge2.pkl"), "rb") as f:
        bursts_payload_ge2 = pickle.load(f)
    with open(os.path.join(tmp_path, "flows_bursts_payload_ge1_count.pkl"), "rb") as f:
        flows_bursts_payload_ge1_count = pickle.load(f)

    # ä» flows_bursts_payload_ge1_count åŠ¨æ€å¾—åˆ°æµé›†åˆ
    flows_with_ge2 = [
        flow for flow, detail in flows_bursts_payload_ge1_count.items() if detail["ge1_count"] >= 2
    ]

    samples_label1 = []  # ç±»åˆ«1çš„æ ·æœ¬
    samples_label2 = []  # ç±»åˆ«2çš„æ ·æœ¬

    # ç±»åˆ« 1ï¼šä» bursts_payload_ge2 ä¸­é‡‡æ ·ï¼ˆåŒä¸€ burst å†…çš„æ­£æ ·æœ¬å¯¹ï¼‰
    if bursts_payload_ge2:
        print("ç”Ÿæˆç±»åˆ« 1 æ ·æœ¬ï¼ˆåŒ burst å†…ï¼‰...")
        for _ in tqdm(range(k), desc="ç±»åˆ«1", leave=False):
            burst = random.choice(bursts_payload_ge2)
            payload_lines = burst["payloads"]
            assert len(payload_lines) >= 2
            i, j = random.sample(range(len(payload_lines)), 2)
            a = _str_to_ids(_bigram_generation(payload_lines[i]), SEQ_LENGTH, args.tokenizer)
            b = _str_to_ids(_bigram_generation(payload_lines[j]), SEQ_LENGTH, args.tokenizer)
            # éšæœºé€‰æ‹©é¡ºåº
            if random.random() < 0.5:
                samples_label1.append({"data": [a, b], "label": 1})
            else:
                samples_label1.append({"data": [b, a], "label": 1})

    # ç±»åˆ« 2ï¼šä»æ‹¥æœ‰ >=2 ä¸ªç±»å‹2 burst çš„æµä¸­é‡‡æ ·ï¼ˆåŒæµå†…ä¸åŒ burstï¼‰
    if flows_with_ge2:
        print("ç”Ÿæˆç±»åˆ« 2 æ ·æœ¬ï¼ˆåŒæµå†…ä¸åŒ burstï¼‰...")
        for _ in tqdm(range(k), desc="ç±»åˆ«2", leave=False):
            flow = random.choice(flows_with_ge2)
            bursts = flows_bursts_payload_ge1_count[flow]["bursts_payload_ge1"]
            assert len(bursts) >= 2
            i, j = random.sample(range(len(bursts)), 2)
            b1, b2 = bursts[i], bursts[j]
            pl1 = b1["payloads"]
            pl2 = b2["payloads"]
            assert len(pl1) >= 1 and len(pl2) >= 1
            l1 = _str_to_ids(_bigram_generation(random.choice(pl1)), SEQ_LENGTH, args.tokenizer)
            l2 = _str_to_ids(_bigram_generation(random.choice(pl2)), SEQ_LENGTH, args.tokenizer)
            # éšæœºé€‰æ‹©é¡ºåº
            if random.random() < 0.5:
                samples_label2.append({"data": [l1, l2], "label": 2})
            else:
                samples_label2.append({"data": [l2, l1], "label": 2})

    # å¯¹æ¯ä¸ªç±»åˆ«è¿›è¡Œéšæœºæ‰“ä¹±
    random.shuffle(samples_label1)
    random.shuffle(samples_label2)

    # æ‰€æœ‰æ ·æœ¬å½’ä¸ºè®­ç»ƒé›†
    train_samples_1 = samples_label1
    train_samples_2 = samples_label2
    train_samples = train_samples_1 + train_samples_2

    # æ‰“ä¹±è®­ç»ƒæ ·æœ¬
    random.shuffle(train_samples)

    # åˆ›å»ºè¾“å‡ºç›®å½•ï¼ˆtrainï¼‰
    train_dir = os.path.join(out_dir, "train")
    os.makedirs(train_dir, exist_ok=True)

    # ä¿å­˜è®­ç»ƒé›†
    print(f"ä¿å­˜è®­ç»ƒé›†ï¼ˆ{len(train_samples)} ä¸ªæ ·æœ¬ï¼‰...")
    _dump_in_chunks(train_samples, train_dir, k)
    print(f"å¯¹æ¯”å­¦ä¹ æ•°æ®é›†ç”Ÿæˆå®Œæˆï¼")
    print(f"  ç±»åˆ«1: è®­ç»ƒé›† {len(train_samples_1)} ä¸ª")
    print(f"  ç±»åˆ«2: è®­ç»ƒé›† {len(train_samples_2)} ä¸ª")
    print(f"  æ€»è®¡: è®­ç»ƒé›† {len(train_samples)} ä¸ª")

    # ----- æ–°å¢é€»è¾‘: ç”Ÿæˆå•æ¡payloadæµ‹è¯•é›†ï¼Œå¹¶åˆ†ç‰‡ä¿å­˜ -----
    # åŠ è½½payload_label_dict
    payload_label_path = os.path.join(tmp_path, "payload_label_dict.pkl")
    if os.path.exists(payload_label_path):
        with open(payload_label_path, "rb") as f:
            payload_label_dict = pickle.load(f)
        print(f"å·²åŠ è½½ payload_label_dictï¼ˆæ ‡ç­¾æ•°: {len(payload_label_dict)}ï¼‰")
    else:
        print(f"æœªæ‰¾åˆ°payload_label_dict.pklï¼Œè·³è¿‡æµ‹è¯•é›†ç”Ÿæˆ")
        return

    # å°†å­—ç¬¦ä¸²labelæ˜ å°„ä¸ºintï¼Œå¹¶ç”Ÿæˆæ˜ å°„è¡¨
    test_samples = []
    label2id = {label: idx for idx, label in enumerate(sorted(payload_label_dict.keys()))}
    id2label = {idx: label for label, idx in label2id.items()}
    for label, payloads in payload_label_dict.items():
        if not payloads:
            continue
        if len(payloads) <= k2:
            chosen_payloads = payloads[:]  # å…¨éƒ¨
        else:
            chosen_payloads = random.sample(payloads, k2)
        label_int = label2id[label]
        for payload in chosen_payloads:
            test_samples.append({"data": _str_to_ids(_bigram_generation(payload), SEQ_LENGTH, args.tokenizer), "label": label_int})

    # æ‰“ä¹±
    random.shuffle(test_samples)

    # ä¿å­˜åˆ° dest_path/testï¼Œåˆ†ç‰‡
    test_dir = os.path.join(out_dir, "test")
    os.makedirs(test_dir, exist_ok=True)
    print(f"ä¿å­˜å•æ¡payloadæµ‹è¯•é›†ï¼ˆ{len(test_samples)} ä¸ªæ ·æœ¬ï¼‰åˆ°: {test_dir}")
    _dump_in_chunks(test_samples, test_dir, k2)

    # ä¿å­˜labelæ˜ å°„è¡¨
    label2id_path = os.path.join(test_dir, "label2id.json")
    id2label_path = os.path.join(test_dir, "id2label.json")
    with open(label2id_path, "w", encoding="utf-8") as f:
        import json
        json.dump(label2id, f, ensure_ascii=False, indent=2)
    with open(id2label_path, "w", encoding="utf-8") as f:
        import json
        json.dump(id2label, f, ensure_ascii=False, indent=2)

def check_tmp(tmp_path: str):
    """
    æ£€æŸ¥ generate_classify1_tmp ç”Ÿæˆçš„ä¸­é—´æ–‡ä»¶ï¼Œè¾“å‡ºç»Ÿè®¡ä¿¡æ¯ã€‚
    
    Args:
        tmp_path: generate_classify1_tmp ç”Ÿæˆçš„ä¸´æ—¶æ–‡ä»¶è·¯å¾„
    """
    import os
    import pickle
    
    if not os.path.exists(tmp_path):
        print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {tmp_path}")
        return
    
    print("=" * 70)
    print("ğŸ“Š ä¸­é—´æ–‡ä»¶ç»Ÿè®¡ä¿¡æ¯")
    print("=" * 70)
    
    # æ£€æŸ¥ bursts_payload_ge2.pkl
    bursts_file = os.path.join(tmp_path, "bursts_payload_ge2.pkl")
    if os.path.exists(bursts_file):
        with open(bursts_file, "rb") as f:
            bursts_payload_ge2 = pickle.load(f)
        
        print(f"\n1ï¸âƒ£  bursts_payload_ge2.pkl")
        print(f"   â€¢ æ€» burst æ•°é‡: {len(bursts_payload_ge2)}")
        
        if bursts_payload_ge2:
            # ç»Ÿè®¡ payload æ•°é‡åˆ†å¸ƒ
            payload_counts = [b["payload_num"] for b in bursts_payload_ge2]
            print(f"   â€¢ Payload æ•°é‡èŒƒå›´: {min(payload_counts)} ~ {max(payload_counts)}")
            print(f"   â€¢ å¹³å‡ Payload æ•°é‡: {sum(payload_counts) / len(payload_counts):.2f}")
            
            # ç»Ÿè®¡ä¸åŒ payload æ•°é‡çš„ burst åˆ†å¸ƒ
            from collections import Counter
            count_dist = Counter(payload_counts)
            print(f"   â€¢ Payload æ•°é‡åˆ†å¸ƒï¼ˆå‰5ï¼‰:")
            for count, freq in sorted(count_dist.items(), key=lambda x: x[1], reverse=True)[:5]:
                print(f"      - {count} ä¸ª payload: {freq} ä¸ª burst")
            
            # ç¤ºä¾‹æ•°æ®
            sample = bursts_payload_ge2[0]
            print(f"   â€¢ ç¤ºä¾‹ burst ç»“æ„:")
            print(f"      - packets æ•°é‡: {len(sample['packets'])}")
            print(f"      - payloads æ•°é‡: {len(sample['payloads'])}")
    else:
        print(f"\nâŒ æ–‡ä»¶ä¸å­˜åœ¨: bursts_payload_ge2.pkl")
    
    # æ£€æŸ¥ flows_bursts_payload_ge1_count.pkl
    flows_file = os.path.join(tmp_path, "flows_bursts_payload_ge1_count.pkl")
    if os.path.exists(flows_file):
        with open(flows_file, "rb") as f:
            flows_bursts_payload_ge1_count = pickle.load(f)
        
        print(f"\n2ï¸âƒ£  flows_bursts_payload_ge1_count.pkl")
        print(f"   â€¢ æ€»æµæ•°é‡: {len(flows_bursts_payload_ge1_count)}")
        
        if flows_bursts_payload_ge1_count:
            # ç»Ÿè®¡æ¯ä¸ªæµçš„ burst æ•°é‡
            ge1_counts = [detail["ge1_count"] for detail in flows_bursts_payload_ge1_count.values()]
            print(f"   â€¢ ç±»å‹2 burst æ•°é‡èŒƒå›´: {min(ge1_counts)} ~ {max(ge1_counts)}")
            print(f"   â€¢ å¹³å‡ç±»å‹2 burst æ•°é‡: {sum(ge1_counts) / len(ge1_counts):.2f}")
            
            # ç»Ÿè®¡æ»¡è¶³æ¡ä»¶çš„æµ
            flows_with_ge2 = sum(1 for count in ge1_counts if count >= 2)
            flows_with_ge1 = sum(1 for count in ge1_counts if count >= 1)
            
            print(f"   â€¢ è‡³å°‘æœ‰ 1 ä¸ªç±»å‹2 burst çš„æµ: {flows_with_ge1}")
            print(f"   â€¢ è‡³å°‘æœ‰ 2 ä¸ªç±»å‹2 burst çš„æµ: {flows_with_ge2}")
            
            # ç±»å‹2 burst æ•°é‡åˆ†å¸ƒ
            from collections import Counter
            count_dist = Counter(ge1_counts)
            print(f"   â€¢ ç±»å‹2 burst æ•°é‡åˆ†å¸ƒï¼ˆå‰5ï¼‰:")
            for count, freq in sorted(count_dist.items(), key=lambda x: x[1], reverse=True)[:5]:
                print(f"      - {count} ä¸ªç±»å‹2 burst: {freq} ä¸ªæµ")
            
            # ç¤ºä¾‹æ•°æ®
            sample_flow = list(flows_bursts_payload_ge1_count.keys())[0]
            sample_detail = flows_bursts_payload_ge1_count[sample_flow]
            print(f"   â€¢ ç¤ºä¾‹æµç»“æ„:")
            print(f"      - ge1_count: {sample_detail['ge1_count']}")
            print(f"      - bursts_payload_ge1 æ•°é‡: {len(sample_detail['bursts_payload_ge1'])}")
    else:
        print(f"\nâŒ æ–‡ä»¶ä¸å­˜åœ¨: flows_bursts_payload_ge1_count.pkl")
    
    # INSERT_YOUR_CODE
    # æ£€æŸ¥ payload_dict.pkl
    payload_dict_file = os.path.join(tmp_path, "payload_label_dict.pkl")
    if os.path.exists(payload_dict_file):
        with open(payload_dict_file, "rb") as f:
            payload_dict = pickle.load(f)

        print(f"\n3ï¸âƒ£  payload_label_dict.pkl")
        print(f"   â€¢ é”®å€¼å¯¹æ•°é‡: {len(payload_dict)}")

        if payload_dict:
            # è·å–æ‰€æœ‰payloadé•¿åº¦
            payload_lengths = []
            for v in payload_dict.values():
                payload_lengths.extend(len(p) for p in v)

            if payload_lengths:
                print(f"   â€¢ payloadé•¿åº¦èŒƒå›´: {min(payload_lengths)} ~ {max(payload_lengths)}")
                from collections import Counter
                payload_len_dist = Counter(payload_lengths)
                print(f"   â€¢ payloadé•¿åº¦åˆ†å¸ƒï¼ˆå‰5ï¼‰:")
                for l, freq in payload_len_dist.most_common(5):
                    print(f"      - é•¿åº¦ {l}: {freq} ä¸ª")
            else:
                print(f"   â€¢ æ²¡æœ‰å¯ç»Ÿè®¡é•¿åº¦çš„payload")
    else:
        print(f"\nâŒ æ–‡ä»¶ä¸å­˜åœ¨: payload_label_dict.pkl")
    
    print("\n" + "=" * 70)
    print("âœ… æ£€æŸ¥å®Œæˆï¼")
    print("=" * 70)


def check_dataset(path: str):
    """
    åˆå¹¶æŒ‡å®šç›®å½•ä¸‹æ‰€æœ‰pickleæ–‡ä»¶ä¸­çš„æ•°ç»„ï¼Œå¹¶è¾“å‡ºåˆå¹¶åæ•°ç»„çš„é•¿åº¦ã€‚
    
    Args:
        directory: åŒ…å«pickleæ–‡ä»¶çš„ç›®å½•è·¯å¾„
        
    Returns:
        tuple: (åˆå¹¶åçš„æ•°ç»„, æ•°ç»„é•¿åº¦)
    """
    import os
    import pickle
    from tqdm import tqdm
    
    if not os.path.exists(path):
        print(f"ç›®å½•ä¸å­˜åœ¨: {path}")
        return None, 0
    
    # è·å–æ‰€æœ‰pickleæ–‡ä»¶
    pickle_files = [f for f in os.listdir(path) if f.endswith('.pkl')]
    
    if not pickle_files:
        print(f"ç›®å½• {path} ä¸­æ²¡æœ‰æ‰¾åˆ°pickleæ–‡ä»¶")
        return None, 0
    
    print(f"æ‰¾åˆ° {len(pickle_files)} ä¸ªpickleæ–‡ä»¶")
    
    merged_array = []
    
    # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
    for filename in tqdm(pickle_files, desc="åˆå¹¶pickleæ–‡ä»¶"):
        file_path = os.path.join(path, filename)
        try:
            with open(file_path, "rb") as f:
                data = pickle.load(f)
                if isinstance(data, list):
                    merged_array.extend(data)
                else:
                    print(f"è­¦å‘Š: {filename} ä¸­çš„æ•°æ®ä¸æ˜¯åˆ—è¡¨ç±»å‹ï¼Œè·³è¿‡")
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
            continue
    
    array_length = len(merged_array)
    print(f"åˆå¹¶å®Œæˆï¼Œæ€»é•¿åº¦: {array_length}")
    # print(merged_array[:5])
    
    # return merged_array, array_length


def test(arg: str):
    print(f'test: {arg}')

def main():
    """
    ä½¿ç”¨ Fire åº“ç®¡ç†å‘½ä»¤è¡Œå‚æ•°çš„ä¸»å‡½æ•°ã€‚
    
    æ”¯æŒçš„å‡½æ•°è°ƒç”¨ï¼š
    - process_pcap(pcap_path, tmp_path)
    - process_flow_dataset(src_path, dest_path)
    - generate_packet_dataset(src_path, dest_path, k=10000)
    - generate_flow_dataset(src_path, dest_path, k=1000)
    - generate_classify1_tmp(src_path, tmp_path)
    - generate_classify1_dataset(tmp_path, dest_path, k=10000)
    - generate_contrastive_dataset(tmp_path, dest_path, k=10000)
    - check_tmp(tmp_path)
    - check_dataset(path)
    
    ä½¿ç”¨ç¤ºä¾‹ï¼š
    python preprocess.py process_pcap --pcap_path="input.pcap" --tmp_path="output.txt"
    python preprocess.py generate_packet_dataset --src_path="flows" --dest_path="packets" --k=5000
    python preprocess.py generate_classify1_tmp --src_path="flows" --tmp_path="tmp"
    python preprocess.py check_tmp --tmp_path="tmp"
    python preprocess.py generate_contrastive_dataset --tmp_path="tmp" --dest_path="output" --k=10000
    python preprocess.py check_dataset --path="path/to/pickle/files"
    """
    import fire
    fire.Fire()


if __name__ == "__main__":
    main()
